---
title: "CITS4009 - Project 2"
author: 
- "Tong LAN (24056082)"
- "Hanyu XUE (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

Task distributionï¼š
Tong LAN (24056082) (50%): all tasks
Hanyu XUE (24070974) (50%): all tasks

# Introduction
The dataset used in this project is obtained from [*kaggle.com*](https://www.kaggle.com/), collected by *PAOLO MAZZA*. [[1]](https://www.kaggle.com/datasets/meraxes10/fantasy-premier-league-dataset-2022-2023) 
This dataset serves as a fundamental collection of statistics derived from the official Fantasy Premier League website [2](FPL), which allows users to construct virtual soccer teams comprising actual players and accumulate points according to their performance or perceived on-field contributions. Based on the exploration in  Project 1, we will further explore the dataset and try to modelling and visualising to predict the player's position this project. 


## Load Libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ROCit)
library(pander)
library(xgboost)
library(lime)
library(factoextra)
library(fpc)
library(grDevices)
```

# Data Preprocessing
According to the analysis in project 1, we found that the position is highly correlated with the performance variables, such as goals_scored, assists, clean_sheets, etc. Therefore, we decided to group the 4 position into 2 labels, offensive and defensive as our binary types. The offensive position includes forward and midfielder, and the defensive position includes defender and goalkeeper. Then, we will use the performance variables to predict the position.

## Initial Transform
Before we start to explore the dataset, we need to do some initial transformations to make the dataset more readable and easier to use. First of all, we drop some columns that not meaningful for our analysis. Then, we deal with the missing values. Finally, we convert the label column.

```{r preprocess_init_transform}
preprocess_init_transform <- function(data) {
  data %>%
    # drop some columns, which are not useful for our analysis
    # for example, names, teams, points, news and variable related to cost
    select(
      -c(
        "id",
        "team",
        "name",
        "now_cost",
        "transfers_out",
        "value_form",
        "value_season",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "event_points",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "points_per_game",
        "total_points",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in",
        "selected_by_percent",
        "bps",
        "bonus"
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type"))
}
```

## Handle Missing Values


```{r preprocess_init_handle_NAs}
preprocess_init_handle_NAs <- function(data) {
  # check missing values
  sum(is.na(data))
  # check which columns have missing values
  missing_values_sum <- colSums(is.na(data) > 0)
  missing_values_sum[missing_values_sum > 0]
  # convert missing values columns to categorical variables
  data <- data %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
  # check missing values again
  sum(is.na(data))
  
  return(data)
}
```
After checking missing values, we decided to convert these columns to logical values, True represents not missing and False represents missing. Then, we check the missing values again, and there are no missing values in the dataset.

## Further Transform
Before we convert the label column, we drop Position Columns. Because it is highly correlated with the label column, and we just used it to create the label column. Then we covert the label column to numerical values, 1 present offensive, 0 present defensive.

```{r preprocess_further_transform, results='hide'}
preprocess_further_transform <- function(data) {
  data %>%
    # drop highly correlated columns
    #(move to the content)if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
    select(
      -c(
        "clean_sheets",
        "expected_assists",
        "starts",
        "expected_goals_conceded",
        "saves",
        "expected_goal_involvements",
        "expected_goals"
      )
    ) %>%
    # transform other performance variable to per 90 values
    mutate(minutes_per_90 = minutes / 90, .after = minutes) %>%
    mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
    mutate(goals_per_90 = goals_scored / minutes * 90,
           .after = goals_scored) %>%
    mutate(goals_conceded_per_90 = goals_conceded / minutes * 90,
           .after = goals_conceded) %>%
    mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
    mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
    mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
    mutate(creativity_per_90 = creativity / minutes * 90,
           .after = creativity) %>%
    mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
    mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
           .after = yellow_cards) %>%
    # drop the origin columns
    select(
      -c(
        "minutes",
        "assists",
        "goals_scored",
        "goals_conceded",
        "red_cards",
        "threat",
        "influence",
        "creativity",
        "own_goals",
        "yellow_cards"
      )
    )
}
```

### Features that were removed

With the above transformation, we removed the some features that not useful for our analysis. For example, we removed the columns that are highly correlated with the label column. 

### Features that were included

list of features that were included

```{r preprocess_further_handle_NAs}
preprocess_further_handle_NAs <- function(data) {
  # count missing values
  missing_values <- apply(is.na(data), 2, sum)
  which(missing_values > 0)
  # fill missing values with 0
  data <- data %>%
    mutate(
      minutes_per_90 = ifelse(is.na(minutes_per_90), 0, minutes_per_90),
      assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
      goals_per_90 = ifelse(is.na(goals_per_90), 0, goals_per_90),
      goals_conceded_per_90 = ifelse(is.na(goals_conceded_per_90), 0, goals_conceded_per_90),
      red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
      threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
      influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
      creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
      own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
      yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
    )
  # look at the missing values again, they have been cleaned
  missing_values <- apply(is.na(data), 2, sum)
  missing_values
  
  return(data)
}
```

```{r convert categorical to numerical}
convert_categorial <- function(data) {
  if (any(grepl("penalties_order", colnames(data)))) {
    data <-
      data %>% mutate(penalties_order = ifelse(penalties_order == "Taker", 1, 0))
  }
  if (any(grepl("direct_freekicks_order", colnames(data)))) {
    data <-
      data %>% mutate(direct_freekicks_order = ifelse(direct_freekicks_order == "Taker", 1, 0))
  }
  if (any(grepl("corners_and_indirect_freekicks_order", colnames(data)))) {
    data <-
      data %>% mutate(
        corners_and_indirect_freekicks_order = ifelse(corners_and_indirect_freekicks_order == "Taker", 1, 0)
      )
  }
  return(data)
}
```


## Target Value
From the original data set we can see there are 4 positions we take "GKP" and "DEF" as a defensive players, and take "MID" and "FWD" as offensive players. So "Offensive" and "Defensive" are our classes. Then we create a new column called "player_type_value" to represent the player type with numerical values, 1 represents the offensive player, and 0 represents the defensive player.

```{r preprocess_target_value}
preprocess_target_value <- function(data) {
  data %>%
    # create target column, 1 represents offensive player, 2 represents defensive player
    mutate(
      player_type = ifelse(position == 'GKP' |
                             position == 'DEF', "Defensive", "Offensive"),
      .before = position
    ) %>%
    select(-position)
}
```

```{r preprocess_target_barplot}
preprocess_target_barplot <- function(data, target) {
  data %>%
    ggplot(aes(x = target, fill = target)) +
    geom_bar(alpha = 0.8, width = 0.8) +
    geom_label(stat = "count",
               aes(label = after_stat(count)),
               show.legend = F) +
    theme_ipsum() +
    scale_fill_brewer(palette = "Set1")
}
```
To easily see the quantity and distribution of the binary value, we define a plot function to show it in the further sections.
```{r preprocess_target_donut}
preprocess_target_donut <- function(data) {
  data %>%
    mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
    group_by(player_type) %>%
    summarise(count = n()) %>%
    mutate(
      percentage = count / sum(count),
      ymax = cumsum(percentage),
      ymin = c(0, head(ymax, n = -1)),
      labelPosition = (ymax + ymin) / 2,
      label = paste(
        ifelse(player_type == 0, 'Defensive' , 'Offensive'),
        "\n",
        round(percentage * 100, 2),
        "%",
        sep = ""
      )
    ) %>%
    ggplot(aes(
      ymax = ymax,
      ymin = ymin,
      xmax = 4,
      xmin = 3,
      fill = as.factor(player_type)
    )) +
    geom_rect() +
    coord_polar(theta = "y") +
    geom_label(x = 3.5,
               aes(y = labelPosition, label = label),
               size = 4) +
    scale_fill_brewer(palette = 4) +
    scale_color_brewer(palette = 3) +
    xlim(c(2, 4)) +
    theme_void() +
    theme(legend.position = "none")
}
```

## Read Data
- Import and plot data
```{r read_data}
# read data and preprocess
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")
classification_data <- fpl_raw_data %>%
  preprocess_init_transform() %>%
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()
# target value plot
classification_data %>% preprocess_target_barplot(classification_data$player_type)
classification_data %>% preprocess_target_donut()
# remove raw data
rm(fpl_raw_data)
```
From these 2 plots, we can see that the data is nearly balanced, because the difference between the offensive players and defensive players is not that much. Next, we do some sampling for the data.

## Split Data

> When working with a small dataset, it is generally recommended to prioritize maximizing the amount of data available for training the model. Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance.

> In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# split to training and testing set
split_ratio <- 0.9
set.seed(500)
fortrain <- runif(nrow(classification_data)) < split_ratio
train_data <- classification_data[fortrain, ]
test_data <- classification_data[!fortrain, ]
# prepare the data for modelling
outCol <- names(train_data)[-c(1, 2)]
# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type'))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
# view data
cat(paste("train data dim:"), dim(train_data), "\n")
kable(train_data[1:5,], caption =  "First 5 rows of classification train data")

cat(paste("train data dim:"), dim(test_data), "\n")
kable(test_data[1:5,], caption =  "First 5 rows of classification test data")
```
# Classification

## Binary Classification Problem
As mentioned before, offensive and defensive are our binary problem. So we define labels and threshold for the problem.

```{r binary target variable}
target <- 'player_type'
pos.label <- 'Offensive'
neg.label <- 'Defensive'
threhold <- 0.5
```

## Model Comparing Metrics
According to the multiple usage, we define some functions so that we can reuse them to do multiple performance calculations. These functions include several main methods, such as confusion matrix, AUC, deviance and log-likelihood. It will be used in the following sections and increase the efficiency of the code. At the same time, it is convenient to compare the results of each model.
Firstly, we define log-likelihood and deviance functions to compare models and combine them into a table.
```{r model_comparing_metrics}
# extract column
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- colunm_name
  if (length(result) > 0) {
    result <- paste(colunm_name, collapse = ", ")
  }
  return(result)
}

# log likelihood
calc_log_likelihood <- function(ypred, ytrue, epsilon = 1e-6) {
  log_likelihood <-
    sum(ifelse(ytrue, log(ypred), log(1 - ypred - epsilon)), na.rm = T)
  return(log_likelihood)
}

calc_null_log_likelihood <- function(data, epsilon = 1e-6) {
  null_log_likelihood <-
    calc_log_likelihood(sum(data[[target]] == pos.label) / nrow(data),
                        data[[target]] == pos.label)
  return(null_log_likelihood)
}

# reduction of deviance
calc_drop_deviance <- function(ypred, ytrue, data, epsilon = 1e-6) {
  null_log_likelihood <- calc_null_log_likelihood(data)
  model_log_likelihood <- calc_log_likelihood(ypred, ytrue, epsilon)
  drop_deviance <- 2 * (model_log_likelihood - null_log_likelihood)
  return(drop_deviance)
}

null_model_comparison <- function(data, epsilon = 1e-6) {
  null_model <- data.frame(
    Model = 'Null Model',
    Log_Likelihood = calc_null_log_likelihood(data),
    Reduction_Deviance = 0
  )
  row.names(null_model) <- NULL
  return(null_model)
}

model_comparison <-
  function(ypred,
           ytrue,
           data,
           model = 'Model',
           epsilon = 1e-6) {
    log_likelihood <- calc_log_likelihood(ypred, ytrue, epsilon)
    drop_deviance <- calc_drop_deviance(ypred, ytrue, data, epsilon)
    compare_df <- data.frame(
      Model =  model,
      Log_Likelihood = log_likelihood,
      Reduction_Deviance = drop_deviance
    )
    row.names(compare_df) <- NULL
    return(compare_df)
  }
```


## Model Evaluation Metrics
Here we define some functions to evaluate models and compare with the null model.
```{r single_evaluation_metrics}
# AUC calculation
calc_auc <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}

# evaluation result
single_evaluation <- function(vars) {
  varResult <-
    tribble( ~ Type,
             ~ Pred_Variable,
             ~ Log_Likelihood,
             ~ Drop_Deviance,
             ~ Train_AUC,
             ~ Test_AUC)
  for (v in vars) {
    type <- ''
    if (v %in% catVars) {
      type <- 'categorical'
    } else if (v %in% numVars) {
      type <- 'numerical'
    }
    pred_variable <- paste('pred_', v, sep = '')
    print(pred_variable)
    log_likelihood <-
       calc_log_likelihood(train_data[[pred_variable]], train_data[[target]] == pos.label)
     drop_deviance <-
       calc_drop_deviance(train_data[[pred_variable]], train_data[[target]] == pos.label, train_data)
    train_auc <-
      calc_auc(train_data[, pred_variable], train_data[[target]])
    test_auc <-
      calc_auc(test_data[, pred_variable], test_data[[target]])
    
    varResult <-
      add_row(
        varResult,
        Type = type,
        Pred_Variable = pred_variable,
        Log_Likelihood = log_likelihood,
        Drop_Deviance = drop_deviance,
        Train_AUC = train_auc,
        Test_AUC = test_auc
      )
  }
  
  varResult <- arrange(varResult, type, desc(Log_Likelihood))
  
  # add null model values
  varResult <- add_row(
    varResult,
    Type = 'Null Model',
    Pred_Variable = 'Null Model',
    Log_Likelihood = calc_null_log_likelihood(train_data),
    Drop_Deviance = 0,
    Train_AUC = 0.5,
    Test_AUC = 0.5,
    .before = 1
  )
  
  return (varResult)
}
```

Then we define a function to plot the AUC and ROC curve for single variable model.
```{r single_evaluation_plot}
# AUC plot
plot_auc <- function(data, target, feature) {
  data %>%
    ggplot(aes(x = data[[feature]], color = as.factor(player_type))) +
    geom_density() +
    xlab(feature)
}

# ROC plot - single variable model
plot_roc_sa <- function(data,
                        features,
                        title) {
  colors = c("salmon",
             "lightblue",
             "navyblue",
             "seagreen",
             "orchid",
             "lightpink")
  for (i in 1:length(features)) {
    par(new = T)
    feature = features[i]
    pred_feature = paste0("pred_", feature)
    ROCit_obj <-
      rocit(score = data[[pred_feature]], class = data[, target] == pos.label)
    plot(
      ROCit_obj,
      col = c(colors[i], 1),
      legend = FALSE,
      YIndex = FALSE,
      values = FALSE
    )
    title(title)
  }
  legend("bottomright",
         legend = features,
         col = colors,
         cex = 0.6,
         lty = 1)
}
```

We define confusion matrix functions and combine other metrics together. Moreover, create a table to store and directly show the related performance for each model.
```{r evaluation_metrics}
model_evaluation <-
  function(pred,
           truth,
           name = "model",
           threshold = 0.5) {
    # data preparation
    if (str_detect(name, "XGBoost")) {
      pred_class <- ifelse(pred > threshold, pos.label, neg.label)
      truth_class <-
        ifelse(truth > threshold, pos.label, neg.label)
    }
    factor_pred <- pred
    if (class(pred) != "factor") {
      if (str_detect(name, "XGBoost")) {
        factor_pred <- as.factor(pred_class)
      } else{
        factor_pred <- as.factor(pred)
      }
    }
    factor_truth <- truth
    if (class(truth) != "factor") {
      factor_truth <- as.factor(truth)
    }
    # metrics calculation
    cm <- confusionMatrix(factor_pred, factor_truth)
    accuracy <- cm$overall['Accuracy']
    precision <- cm$byClass['Pos Pred Value']
    recall <- cm$byClass['Sensitivity']
    f1 <- cm$byClass['F1']
    
    if (str_detect(name, "Decision Tree")) {
      pred <- as.numeric(pred)
    }
    AUC <- calc_auc(pred, truth)
    # store the result
    result <- data.frame(
      Model = name,
      Accuracy = accuracy,
      Precision = precision,
      Recall = recall,
      F1 = f1,
      AUC = AUC
    )
    row.names(result) <- NULL
    return (result)
  }

model_performance <-
  function(train_pred,
           test_pred,
           train_truth,
           test_truth,
           name = "model") {
    train_performance <-
      model_evaluation(train_pred, train_truth, paste(name, "- Train"))
    test_performance <-
      model_evaluation(test_pred, test_truth, paste(name, "- Test"))
    return(rbind(train_performance, test_performance))
  }

multiple_performance <- function(train_preds,
                                 test_preds,
                                 train_truths,
                                 test_truths,
                                 names) {
  result <- data.frame()
  for (i in 1:length(names)) {
    train_pred <- train_preds[[i]]
    test_pred <- test_preds[[i]]
    train_truth <- train_truths[[i]]
    test_truth <- test_truths[[i]]
    name <- names[[i]]
    performance <-
      model_performance(train_pred, test_pred, train_truth, test_truth, name)
    result <- rbind(result, performance)
  }
}
```

Finally, define a function to plot the ROC curve for multiple variable models. And a function to visualize a tree model.
```{r evaluation_plot}
# ROC curve - multiple variable model
plot_model_roc <-
  function(train_pred,
           train_truth,
           test_pred,
           test_truth,
           title) {
    roc_train <-
      rocit(score = train_pred, class = train_truth == pos.label)
    roc_test <-
      rocit(score = test_pred, class = test_truth == pos.label)
    
    plot(
      roc_train,
      col = c("lightblue", "forestgreen"),
      lwd = 3,
      legend = FALSE,
      YIndex = FALSE,
      values = TRUE,
      asp = 1
    )
    lines(
      roc_test$TPR ~ roc_test$FPR,
      lwd = 3,
      col = c("salmon", "forestgreen"),
      asp = 1
    )
    legend(
      "bottomright",
      col = c("lightblue", "salmon", "forestgreen"),
      c("Model on Train", "Model on Test", "Null Model"),
      cex = 0.8,
      lwd = 2
    )
    title(title)
  }

# tree plot
plot_tree <- function(model) {
  rpart.plot(model)
}
```

## Null Model Performance

```{r null_model_performance}
log_null_model_comparison <- null_model_comparison(train_data)
kable(log_null_model_comparison, caption = "Null Model")

null_TP <- 0
null_TN <- sum(train_data[[target]] == neg.label)
null_FP <- 0
null_FN <- sum(train_data[[target]] == pos.label)

null_accuracy <- (null_TP + null_TN) / nrow(train_data)
null_precision <- null_TP / (null_TP + null_FP)
null_recall <- null_TP / (null_TP + null_FN)
null_Npos <- sum(train_data[[target]] == pos.label)
null_F1 <-
  2 * (null_precision * null_recall) / (null_precision + null_recall)
pred.Null <- null_Npos / nrow(train_data)
pred.Null <- rep(pred.Null, nrow(train_data))
null_AUC <- calc_auc(pred.Null, train_data[[target]])

null_performance <- tribble(
  ~ Model,
  ~ Accuracy,
  ~ Precision,
  ~ Recall,
  ~ F1,
  ~ AUC,
  "Null Model",
  null_accuracy,
  null_precision,
  null_recall,
  null_F1,
  null_AUC
)
kable(null_performance, caption = "Null Model Evaluation")
```
From the result, we can see the log-likelihood of the Null model is -486.8746. And the accuracy (0.4477401), Auc(0.5). We will use it to compare with other models later.

## Single Variable Models
With the previous split data set, we built single-variable models to see the performance of each variable.

### Categorial Variables
- Build and predict model 
```{r categorical_predictions}
# categorical variables prediction
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  return(pred)
}

# predict all the categorical variables
predCatVars <- c()
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  predCatVars <- c(predCatVars, pi)
  train_data[, pi] <-
    mkPredC(train_data[[target]], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[[target]], train_data[, v], test_data[, v])
}

# view categorical predictions
kable(train_data[1:5, which(names(train_data) %in% predCatVars)], caption =  "First 5 rows of categorical predictions")
```


### Numerical Variables
- Build and predict model
```{r numerical_predictions}
# numerical variables prediction
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# predict all the numerical variables
predNumVars <- c()
for (v in numVars) {
  pi <- paste('pred_', v, sep = '')
  predNumVars <- c(predNumVars, pi)
  train_data[, pi] <-
    mkPredN(train_data[[target]], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[[target]], train_data[, v], test_data[, v])
}

# view numerical predictions
kable(train_data[1:5, which(names(train_data) %in% predNumVars)], caption =  "First 5 rows of categorical predictions")
```

### Variables Evaluation

```{r categorical_evaluation}
# evaluate categorical variables
cat_evaluation <- single_evaluation(catVars)
kable(cat_evaluation, caption = "Categorical Variables Evaluation")
# evaluate numerical variables
num_evaluation <- single_evaluation(numVars)
kable(num_evaluation, caption = "Numerical Variables Evaluation")
```
As we can see, the most of features are better than the Null model. The AUC value of each feature has little difference between the training set and the test set which means the models are fairly accurate. The best categorical AUC for training and testing data are 0.55 and 0.61 from "penalties_order". The best numerical AUC is 0.76/0.80 from "pred_expected_goal_involvements_per_90" which is good. The highest Deviance is 199.19 still from "pred_expected_goal_involvements_per_90". And the following numerical features still have good performance. But the "pred_red_cards_per_90" shows lowest performance which metrics are close to Null model.

### Top-performance Variables
We will select the top-performance variables from the previous evaluation result.

```{r top_performance_variables}
cat_min_drop <- 30 # can adjust this value
select_cat_result <- cat_evaluation[-1] %>% 
  filter(Drop_Deviance > cat_min_drop) %>% 
  mutate(Variable = sub("pred_", "", Pred_Variable)) %>% 
  select(Variable)
num_min_drop <- 15 # can adjust this value
select_num_result <- num_evaluation[-1] %>% 
  filter(Drop_Deviance > num_min_drop) %>% 
  mutate(Variable = sub("pred_", "", Pred_Variable)) %>% 
  select(Variable)
```

## Visualise Top-performance Variables

```{r top-performance_variables_auc_plot}
# AUC
p1 <-
  plot_auc(test_data, test_data$player_type, as.character(select_cat_result[1, ]))
p2 <-
  plot_auc(test_data, test_data$player_type, as.character(select_num_result[1, ]))
p3 <-
  plot_auc(test_data, test_data$player_type, as.character(select_num_result[2, ]))
p4 <-
  plot_auc(test_data, test_data$player_type, as.character(select_num_result[3, ]))
p5 <-
  plot_auc(test_data, test_data$player_type, as.character(select_num_result[4, ]))
p6 <-
  plot_auc(test_data, test_data$player_type, as.character(select_num_result[5, ]))
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

# ROC
plot_roc_sa(
  test_data,
  features = c(as.character(select_cat_result[1, ]),
               as.character(select_num_result[1, ]),
               as.character(select_num_result[2, ]),
               as.character(select_num_result[3, ]),
               as.character(select_num_result[4, ]),
               as.character(select_num_result[5, ])),
  title = "ROC for Top-Performance Single Variables"
)
``` 
Through the AUC density charts above, we can check the relationship between predicted probabilities and the variables. The higher the predicted probabilities, the more likely the player is offensive.From the ROC, we can see "threat_per_90"(pred) and "expected_goal_involvements_per_90"(pred) are very close to the top left corner. It means these two variables have good performance in predicting the target variable. 


## Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

Since our dataset contains both categorical and numerical features, we cannot use simple filter methods such as Pearson Correlation or Chi-Square Test to select features. Because they work only in categorical variables and numerical variables respectively. Instead, we will use the following methods to select features.

> A key part of building many variable models is selecting what variables to use. Each
variable we use represents a chance of explaining more of the outcome variation (a
chance of building a better model), but also represents a possible source of noise and
overfitting. To control this effect, we often preselect which subset of variables weâ€™ll use
to fit.

> Practical Data Science With R

### Concatenation Top-performance Feature

From the previous section, we have identified the following features that have good performance in predicting the target variable. We will use 2 methods to select features for further analysis.

```{r concatenation_feature}
concatenation_features <- c(select_cat_result$Variable, select_num_result$Variable)
concatenation_features
```

### Recursive Feature Elimination
Next we will use the RFE method to select features. RFE is a greedy optimization algorithm that aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.
- Wrapped Methods

```{r RFE}
rfe_train_data <- train_data %>%
  select(all_of(c(target, catVars, numVars))) %>%
  mutate(player_type = as.factor(player_type))

result <-
  rfe(
    rfe_train_data[, -which(names(rfe_train_data) == target)],
    rfe_train_data[, target],
    sizes = c(1:4),
    rfeControl = rfeControl(functions = rfFuncs),
    method = "repeatedcv",
    verbose = FALSE
  )
optVariables <- result$optVariables
rfe_features <- optVariables[1:10]
rfe_features
```

## Multiple Variables Models

### Decision Tree

the performance on the test set is slightly better than on the training set,

#### Build Model

```{r decision_tree_model}
# Decision Tree 1 - Concatenation
dt_train_data1 <-
  train_data %>%
  select(all_of(c(target, concatenation_features)))
dt_test_data1 <-
  test_data %>%
  select(all_of(c(target, concatenation_features)))
dt_model1 <-
  rpart(player_type ~ ., data = dt_train_data1, method = 'class')

# Decision Tree 2 - RFE
dt_train_data2 <-
  train_data %>%
  select(all_of(c(target, rfe_features)))
dt_test_data2 <-
  test_data %>%
  select(all_of(c(target, rfe_features)))
dt_model2 <-
  rpart(player_type ~ ., data = dt_train_data2, method = 'class')
```

#### Model Prediction

```{r decision_tree_prediction}
# Decision Tree 1 - Concatenation
dt_train_pred_class1 <-
  predict(dt_model1, newdata = dt_train_data1, type = "class")
dt_test_pred_class1 <-
  predict(dt_model1, newdata = dt_test_data1, type = "class")

# Decision Tree 2 - RFE
dt_train_pred_class2 <-
  predict(dt_model2, newdata = dt_train_data2, type = "class")
dt_test_pred_class2 <-
  predict(dt_model2, newdata = dt_test_data2, type = "class")
```

#### Model Comparision

```{r decision_tree_comparision}
dt_comparision1 <-
  model_comparison(
    as.numeric(dt_train_pred_class1),
    ifelse(dt_train_data1[[target]] == pos.label, 1, 2),
    dt_train_data1,
    "Decision Tree (Concatenation)"
  )

dt_comparision2 <-
  model_comparison(
    as.numeric(dt_train_pred_class2),
    ifelse(dt_train_data2[[target]] == pos.label, 1, 2),
    dt_train_data2,
    "Decision Tree (RFE)"
  )

dt_all_comparision <-
  rbind(log_null_model_comparison, dt_comparision1, dt_comparision2)
kable(dt_all_comparision)
```
From the first table, log-likelihood and reduction deviance are both much more than the Null model, which means the 2 models perform fit. Next, we keeping compare another measure result.

#### Model Performance

```{r tree_model_performance}
dt_performance1 <-
  model_performance(
    dt_train_pred_class1,
    dt_test_pred_class1,
    dt_train_data1$player_type,
    dt_test_data1$player_type,
    "Decision Tree (Concatenation)"
  )

dt_performance2 <-
  model_performance(
    dt_train_pred_class2,
    dt_test_pred_class2,
    dt_train_data2$player_type,
    dt_test_data2$player_type,
    "Decision Tree (RFE)"
  )

dt_model_performance <- rbind(dt_performance1, dt_performance2)
kable(dt_model_performance)
```
The second table indicates that the difference of 2 models scores between train and test are small, which around 0.02, it means 2 models are fit. We also can see that all Accuracy are more than 0.76, which means both models can successfully accurate classification to predict the player type. Precision are more than 0.68 that seems fair enough. Recall have really highest score in all metrics, The high recall rate indicates that these tree models successfully captures most real positive samples.Tree models achieved a good balance between accuracy and recall with high F1 score(each > 0.76). AUC still good enough.
Overall, these two tree models are good enough to predict the player type.

#### Visualisation

```{r tree_model_performance_plot}
dt_pred_train_roc1 <- predict(dt_model1, newdata = dt_train_data1)
dt_pred_test_roc1 <- predict(dt_model1, newdata = dt_test_data1)

dt_roc_1 <- plot_model_roc(
  dt_pred_train_roc1[, 2],
  train_data$player_type,
  dt_pred_test_roc1[, 2],
  test_data$player_type,
  title = "ROC for Decision Tree (Concatenation)"
)

dt_pred_train_roc2 <- predict(dt_model2, newdata = dt_train_data2)
dt_pred_test_roc2 <- predict(dt_model2, newdata = dt_test_data2)

tree2_roc <- plot_model_roc(
  dt_pred_train_roc2[, 2],
  train_data$player_type,
  dt_pred_test_roc2[, 2],
  test_data$player_type,
  title = "ROC for Decision Tree (RFE)"
)

# visualise the decision tree
rpart.plot(dt_model1, main = "Decision Tree (Concatenation)")
rpart.plot(dt_model2, main = "Decision Tree (RFE)")
```
For both tree models, the ROC curves are not very well but AUC are 0.78/0.80(Concatenation: train/test) and 0.77/0.80 (RFE: train/test) which are good. The ROC curves are not very close to the left . However, there is little difference between the training data and the test data curve for each model, which shows that the training data model has high accuracy.
According to the decision tree plot, we can directly see the one in highest level is "expected_goal_inbolvement_per_90".


### XGBoost

#### Build Model

```{r xgboost_model}
# hyperparameter
xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  lambda = 1,
  alpha = 0.1,
  max_depth = 3,
  min_child_weight = 5,
  eta = 0.1
)

# XGBoost - Concatenation
xgb_train_data1 <- train_data %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_test_data1 <- test_data %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_input1 <- as.matrix(xgb_train_data1[-c(1, 2)])
xgb_test_input1 <- as.matrix(xgb_test_data1[-c(1, 2)])

xgb_model1 <- xgboost(
  data = xgb_input1,
  label = xgb_train_data1$type,
  params = xgb_params,
  nrounds = 10,
  early_stopping_rounds = 10,
  verbose = 0
)

# XGBoost - RFE
xgb_train_data2 <- train_data %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_test_data2 <- test_data %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_input2 <- as.matrix(xgb_train_data2[-c(1, 2)])
xgb_test_input2 <- as.matrix(xgb_test_data2[-c(1, 2)])

xgb_model2 <- xgboost(
  data = xgb_input2,
  label = xgb_train_data2$type,
  params = xgb_params,
  nrounds = 10,
  early_stopping_rounds = 10,
  verbose = 0
)
```

#### Model Prediction

```{r xgboost_prediction}
# XGBoost - Concatenation
xgb_train_pred1 <- predict(xgb_model1, xgb_input1)
xgb_train_pred_class1 <-
  ifelse(xgb_train_pred1 > threhold, pos.label, neg.label)
xgb_test_pred1 <- predict(xgb_model1, xgb_test_input1)
xgb_test_pred_class1 <-
  ifelse(xgb_test_pred1 > threhold, pos.label, neg.label)

# XGBoost - RFE
xgb_train_pred2 <- predict(xgb_model2, xgb_input2)
xgb_train_pred_class2 <-
  ifelse(xgb_train_pred2 > threhold, pos.label, neg.label)
xgb_test_pred2 <- predict(xgb_model2, xgb_test_input2)
xgb_test_pred_class2 <-
  ifelse(xgb_test_pred2 > threhold, pos.label, neg.label)
```

#### Model Comparision

```{r xgboost_comparision}
xgb_comparision1 <-
  model_comparison(
    ifelse(xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(xgb_train_data1[[target]]  == pos.label, 1, 2),
    xgb_train_data1,
    "XGBoost (Concatenation)"
  )

xgb_comparision2 <-
  model_comparison(
    ifelse(xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(xgb_train_data2[[target]] == pos.label, 1, 2),
    xgb_train_data2,
    "XGBoost (RFE)"
  )

xgb_all_comparision <-
  rbind(log_null_model_comparison, xgb_comparision1, xgb_comparision2)
kable(xgb_all_comparision)
```
For 2 XGBoost models, log-likelihood and reduction deviance are much better than NUll model(-486.8746). XGBoost 2 (RFE) is slightly better than XGBoost 1 (Concatenation).

#### Model Performance

```{r xgboost_model_performance}
xgb_performance1 <-
  model_performance(
    xgb_train_pred1,
    xgb_test_pred1,
    xgb_train_data1$player_type,
    xgb_test_data1$player_type,
    "XGBoost (Concatenation)"
  )

xgb_performance2 <-
  model_performance(
    xgb_train_pred2,
    xgb_test_pred2,
    xgb_train_data2$player_type,
    xgb_test_data2$player_type,
    "XGBoost (RFE)"
  )

xgb_preformance <- rbind(xgb_performance1, xgb_performance2)
kable(xgb_preformance)
```
The above table shows the performance of the two XGBoost models. Among all metrics, the disparity between their scores is minimal. Both models perform the highest recall and AUC, which are remarkably similar above 0.87 and close to 1, the best recall score is 0.93. It suggests that the 2 models have excellent performance in this binary classification, but their adaptability and performance in other measures should be further assessed. The other remaining metrics (Accuracy, Precision, F1 score), all above 0.67, mostly about 0.76, which also shows that 2 models are effective.  
Then, We will use ROC to further evaluate the model.

#### Visualisation

```{r xgboost_model_performance_plot}
xgb_pred_train_roc1 <- predict(xgb_model1, newdata = xgb_input1)
xgb_pred_test_roc1 <- predict(xgb_model1, newdata = xgb_test_input1)

plot_model_roc(
  xgb_pred_train_roc1,
  train_data$player_type,
  xgb_pred_test_roc1,
  test_data$player_type,
  title = "ROC for XGBoost (Concatenation)"
)

xgb_pred_train_roc2 <- predict(xgb_model2, newdata = xgb_input2)
xgb_pred_test_roc2 <- predict(xgb_model2, newdata = xgb_test_input2)

plot_model_roc(
  xgb_pred_train_roc2,
  train_data$player_type,
  xgb_pred_test_roc2,
  test_data$player_type,
  title = "ROC for XGBoost (RFE)"
)
```
From this plot, we can see that both ROC curves are extremely close to the Y axis and the top left corner, which means that the 2 models have a high true positive rate and a low false positive rate. With the high AUC (both above 0.87), it shows that the 2 models have excellent fit performance.  

### Explaining Models Using LIME
In this section, we employ LIME to facilitate automated sanity checks on a few typical and extreme cases, helping us in model improvement.

```{r LIME}
cases <- c(2, 14, 26, 38)

# combined 1 - concatenation feature
sample1 <-
  as.data.frame(xgb_test_data1[cases, concatenation_features])
explainer1 <-
  lime(xgb_train_data1[, concatenation_features],
       model = xgb_model1,
       bin_continuous = FALSE)
true_label1 <- xgb_test_data1[cases, 2]
explanation1 <-
  explain(sample1,
          explainer1,
          n_labels = 1,
          n_features = 6)
print(explanation1)

plot_features(explanation1)
plot_explanations(explanation1)

# combined2 - RFE
sample2 <- as.data.frame(xgb_test_data2[cases, rfe_features])
explainer2 <-
  lime(xgb_train_data2[, rfe_features],
       model = xgb_model2,
       bin_continuous = FALSE)
true_label2 <- xgb_test_data2[cases, 2]
explanation2 <-
  explain(sample2,
          explainer2,
          n_labels = 1,
          n_features = 6)
print(explanation2)

plot_features(explanation2)
plot_explanations(explanation2)
```
We can see from the multiple plots and table that ç»“è®º.
The feature "expected_goals_involvements_per_90" shows strong evidence for player type classification in many case.

## Improved Models
improve the models performances by implementing such measures:
- increase the training data size
- cross validation

### Improving Measures

#### Increasing Data

```{r preprocess_join_data}
# read new data
fpl_raw_data1 <- read.csv("./FPL_Dataset_2022-2023.csv")
fpl_raw_data2 <- read.csv("./FPL_Dataset_2023-2024.csv")

# merge
diff_columns <- setdiff(names(fpl_raw_data2), names(fpl_raw_data1))
fpl_raw_data2 <-
  fpl_raw_data2[, -which(names(fpl_raw_data2) %in% diff_columns)]
fpl_raw_data <- union(fpl_raw_data1, fpl_raw_data2)

join_classification_data <- fpl_raw_data %>%
  preprocess_init_transform() %>%
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()

# target value plot
join_classification_data %>% preprocess_target_barplot(join_classification_data$player_type)
join_classification_data %>% preprocess_target_donut()

# remove raw data
rm(fpl_raw_data1)
rm(fpl_raw_data2)

kable(join_classification_data[1:5,], caption = "first 5 rows of join data")
```
```{r split_join_data}
set.seed(1414)
# split data into train and test sets
index <-
  createDataPartition(join_classification_data$player_type,
                      p = split_ratio,
                      list = FALSE)
improved_train_data <-
  join_classification_data[index, ] %>% select(all_of(c(target, rfe_features)))
improved_test_data <-
  join_classification_data[-index, ] %>% select(all_of(c(target, rfe_features)))
```

#### Cross-validation

```{r cross_validation_models}
# decision tree
cv_decision_tree <-
  function(fold_train,
           fold_validation,
           dt_metrics) {
    # fold model
    fold_tree_model <-
      rpart(player_type ~ ., data = fold_train, method = 'class', control = rpart.control(maxdepth = 5))
    fold_tree_model <- prune(fold_tree_model, cp = 0.01)

    # evaluation each fold
    fold_predictions <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    
    fold_train_prediction <-
      predict(fold_tree_model, newdata = fold_train, type = "class")
    fold_train_auc <-
      calc_auc(as.numeric(fold_train_prediction), fold_train[, target])
    
    fold_validation_prediction <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    fold_validation_auc <-
      calc_auc(as.numeric(fold_validation_prediction), fold_validation[, target])
    
    dt_metrics <-
      c(dt_metrics, fold_train_auc, fold_validation_auc)
    
    return(dt_metrics)
  }

final_decision_tree <-
  function(improved_train_data,
           improved_test_data,
           dt_metrics) {
    mean_train_auc <-
      mean(dt_metrics[seq(1, length(dt_metrics), 2)])
    mean_validation_auc <-
      mean(dt_metrics[seq(2, length(dt_metrics), 2)])
    sd_train_auc <-
      sd(dt_metrics[seq(1, length(dt_metrics), 2)])
    sd_validation_auc <-
      sd(dt_metrics[seq(2, length(dt_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final decision tree model
    final_tree_model <-
      rpart(player_type ~ ., data = improved_train_data, method = "class", control = rpart.control(maxdepth = 5))
    final_tree_model <- prune(final_tree_model, cp = 0.01)
    
    return(final_tree_model)
  }

# XGBoost

# hyperparameter
improved_xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  lambda = 0.1,
  alpha = 0.1,
  max_depth = 3,
  min_child_weight = 5,
  eta = 0.1
)

cv_xgb <-
  function(fold_train,
           fold_validation,
           xgb_params,
           xgb_metrics) {
    # build model
    xgb_fold_train <- fold_train %>%
      mutate(type = as.numeric(player_type == pos.label),
             .after = player_type)
    xgb_fold_validation <- fold_validation %>%
      mutate(type = as.numeric(player_type == pos.label),
             .after = player_type)
    fold_train_input <- as.matrix(xgb_fold_train[-c(1, 2)])
    fold_validation_input <-
      as.matrix(xgb_fold_validation[-c(1, 2)])
    
    fold_xgbmodel <- xgboost(
      data = fold_train_input,
      label = xgb_fold_train$type,
      params = improved_xgb_params,
      nrounds = 10,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    # evaluation each fold
    fold_train_pred <- predict(fold_xgbmodel, fold_train_input)
    fold_train_auc <-
      calc_auc(fold_train_pred, xgb_fold_train[, target])
    fold_validation_pred <-
      predict(fold_xgbmodel, fold_validation_input)
    fold_validation_auc <-
      calc_auc(fold_validation_pred, fold_validation[, target])
    xgb_metrics <-
      c(xgb_metrics, fold_train_auc, fold_validation_auc)
    
    return(xgb_metrics)
  }

final_xgb <-
  function(improved_train_data,
           improved_test_data,
           xgb_params,
           xgb_metrics) {
    mean_train_auc <-
      mean(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    mean_validation_auc <-
      mean(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    sd_train_auc <-
      sd(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    sd_validation_auc <-
      sd(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final XGBoost model
    final_train_data <- improved_train_data %>%
      mutate(type = as.numeric(player_type == 'Offensive'),
             .after = player_type)
    final_test_data <- improved_test_data %>%
      mutate(type = as.numeric(player_type == 'Offensive'),
             .after = player_type)
    final_train_input <- as.matrix(final_train_data[-c(1, 2)])
    final_test_input <- as.matrix(final_test_data[-c(1, 2)])
    
    final_xgbmodel <- xgboost(
      data = final_train_input,
      label = final_train_data$type,
      params = improved_xgb_params,
      nrounds = 10,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    return(final_xgbmodel)
  }
```

### Build Improved Model

```{r k-fold cv}
# 100-fold cross-validation
k <- 100
folds <-
  createFolds(
    improved_train_data$player_type,
    k = k,
    list = TRUE,
    returnTrain = TRUE
  )
```

#### Decision Tree

```{r improved_decision_tree_model}
# Decision Tree - Concatenation
improved_dt_train_data1 <-
  join_classification_data[index,] %>%
  select(all_of(c(target, concatenation_features)))
improved_dt_test_data1 <-
  join_classification_data[-index,] %>%
  select(all_of(c(target, concatenation_features)))

dt_metrics1 <- c()
for (i in 1:k) {
  fold_train <- improved_dt_train_data1[folds[[i]], ]
  fold_validation <- improved_dt_train_data1[-folds[[i]], ]
  dt_metrics1 <-
    cv_decision_tree(fold_train, fold_validation, dt_metrics1)
}
improved_dt_model1 <-
  final_decision_tree(improved_dt_train_data1,
                      improved_dt_test_data1,
                      dt_metrics1)

# Decision Tree - RFE
improved_dt_train_data2 <-
  join_classification_data[index,] %>%
  select(all_of(c(target, rfe_features)))
improved_dt_test_data2 <-
  join_classification_data[-index,] %>%
  select(all_of(c(target, rfe_features)))

dt_metrics2 <- c()
for (i in 1:k) {
  fold_train <- improved_dt_train_data2[folds[[i]], ]
  fold_validation <- improved_dt_train_data2[-folds[[i]], ]
  dt_metrics2 <-
    cv_decision_tree(fold_train, fold_validation, dt_metrics2)
}
improved_dt_model2 <-
  final_decision_tree(improved_dt_train_data2,
                      improved_dt_test_data2,
                      dt_metrics2)
```

#### XGBoost

```{r improved_xgboost_model}
# XGBoost - Concatenation
improved_xgb_train_data1 <-
  join_classification_data[index,] %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>% 
   mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_test_data1 <-
  join_classification_data[-index,] %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>% 
   mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_input1 <- as.matrix(improved_xgb_train_data1[-c(1, 2)])
improved_xgb_test_input1 <-
  as.matrix(improved_xgb_test_data1[-c(1, 2)])

xgb_metrics1 <- c()
for (i in 1:k) {
  fold_train <- improved_xgb_train_data1[folds[[i]], ]
  fold_validation <- improved_xgb_train_data1[-folds[[i]], ]
  xgb_metrics1 <-
    cv_xgb(fold_train, fold_validation, xgb_params, xgb_metrics1)
}
improved_xgb_model1 <-
  final_xgb(improved_xgb_train_data1,
            improved_xgb_test_data1,
            xgb_params,
            xgb_metrics1)

# XGBoost - RFE
improved_xgb_train_data2 <-
  join_classification_data[index,] %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>% 
   mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_test_data2 <-
  join_classification_data[-index,] %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>% 
   mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_input2 <- as.matrix(improved_xgb_train_data2[-c(1, 2)])
improved_xgb_test_input2 <-
  as.matrix(improved_xgb_test_data2[-c(1, 2)])

xgb_metrics2 <- c()
for (i in 1:k) {
  fold_train <- improved_xgb_train_data2[folds[[i]], ]
  fold_validation <- improved_xgb_train_data2[-folds[[i]], ]
  xgb_metrics2 <-
    cv_xgb(fold_train, fold_validation, xgb_params, xgb_metrics2)
}
improved_xgb_model2 <-
  final_xgb(improved_xgb_train_data2,
            improved_xgb_test_data2,
            xgb_params,
            xgb_metrics2)
```

### Improved Model Prediction

#### Decision Tree

```{r improved_decision_tree_prediction}
# Decision Tree On New Data - Concatenation
new_dt_train_pred_class1 <-
  predict(dt_model1, newdata = improved_dt_train_data1, type = "class")
new_dt_test_pred_class1 <-
  predict(dt_model1, newdata = improved_dt_test_data1, type = "class")

# Decision Tree On New Data - RFE
new_dt_train_pred_class2 <-
  predict(dt_model2, newdata = improved_dt_train_data2, type = "class")
new_dt_test_pred_class2 <-
  predict(dt_model2, newdata = improved_dt_test_data2, type = "class")

# Improved Decision Tree - Concatenation
improved_dt_train_pred_class1 <-
  predict(improved_dt_model1, newdata = improved_dt_train_data1, type = "class")
improved_dt_test_pred_class1 <-
  predict(improved_dt_model1, newdata = improved_dt_test_data1, type = "class")

# Improved Decision Tree - RFE
improved_dt_train_pred_class2 <-
  predict(improved_dt_model2, newdata = improved_dt_train_data2, type = "class")
improved_dt_test_pred_class2 <-
  predict(improved_dt_model2, newdata = improved_dt_test_data2, type = "class")
```

#### XGBoost

```{r improved_xgboost_prediction}
# XGBoost - Concatenation
new_xgb_train_pred1 <- predict(xgb_model1, improved_xgb_input1)
new_xgb_train_pred_class1 <-
  ifelse(new_xgb_train_pred1 > threhold, pos.label, neg.label)
new_xgb_test_pred1 <- predict(xgb_model1, improved_xgb_test_input1)
new_xgb_test_pred_class1 <-
  ifelse(new_xgb_test_pred1 > threhold, pos.label, neg.label)

# XGBoost - RFE
new_xgb_train_pred2 <- predict(xgb_model2, improved_xgb_input2)
new_xgb_train_pred_class2 <-
  ifelse(new_xgb_train_pred2 > threhold, pos.label, neg.label)
new_xgb_test_pred2 <- predict(xgb_model2, improved_xgb_test_input2)
new_xgb_test_pred_class2 <-
  ifelse(new_xgb_test_pred2 > threhold, pos.label, neg.label)

# Improved XGBoost - Concatenation
improved_xgb_train_pred1 <- predict(improved_xgb_model1, improved_xgb_input1)
improved_xgb_train_pred_class1 <-
  ifelse(improved_xgb_train_pred1 > threhold, pos.label, neg.label)
improved_xgb_test_pred1 <- predict(improved_xgb_model1, improved_xgb_test_input1)
improved_xgb_test_pred_class1 <-
  ifelse(improved_xgb_test_pred1 > threhold, pos.label, neg.label)

# Improved XGBoost - RFE
improved_xgb_train_pred2 <- predict(improved_xgb_model2, improved_xgb_input2)
improved_xgb_train_pred_class2 <-
  ifelse(improved_xgb_train_pred2 > threhold, pos.label, neg.label)
improved_xgb_test_pred2 <- predict(improved_xgb_model2, improved_xgb_test_input2)
improved_xgb_test_pred_class2 <-
  ifelse(improved_xgb_test_pred2 > threhold, pos.label, neg.label)
```

### Improved Model Comparision

#### Decision Tree

```{r improved_decision_tree_comparision}
new_dt_comparision1 <-
  model_comparison(
    as.numeric(new_dt_train_pred_class1),
    ifelse(improved_dt_train_data1[[target]] == pos.label, 1, 2),
    improved_dt_train_data1,
    "Decision Tree (Concatenation)"
  )

new_dt_comparision2 <-
  model_comparison(
    as.numeric(new_dt_train_pred_class2),
    ifelse(improved_dt_train_data2[[target]] == pos.label, 1, 2),
    improved_dt_train_data2,
    "Decision Tree (RFE)"
  )

improved_dt_comparision1 <-
  model_comparison(
    as.numeric(improved_dt_train_pred_class1),
    ifelse(improved_dt_train_data1[[target]] == pos.label, 1, 2),
    improved_dt_train_data1,
    "Improved Decision Tree (Concatenation)"
  )

improved_dt_comparision2 <-
  model_comparison(
    as.numeric(improved_dt_train_pred_class2),
    ifelse(improved_dt_train_data2[[target]] == pos.label, 1, 2),
    improved_dt_train_data2,
    "Improved Decision Tree (RFE)"
  )

improved_dt_all_comparision <-
  rbind(log_null_model_comparison,
        new_dt_comparision1,
        new_dt_comparision2,
        improved_dt_comparision1,
        improved_dt_comparision2)
kable(improved_dt_all_comparision)
```

#### XGBoost

```{r improved_xgboost_comparision}
new_xgb_comparision1 <-
  model_comparison(
    ifelse(new_xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data1[[target]]  == pos.label, 1, 2),
    improved_xgb_train_data1,
    "XGBoost (Concatenation)"
  )

new_xgb_comparision2 <-
  model_comparison(
    ifelse(new_xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data2[[target]] == pos.label, 1, 2),
    improved_xgb_train_data2,
    "XGBoost (RFE)"
  )

improved_xgb_comparision1 <-
  model_comparison(
    ifelse(improved_xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data1[[target]]  == pos.label, 1, 2),
    improved_xgb_train_data1,
    "Improved XGBoost (Concatenation)"
  )

improved_xgb_comparision2 <-
  model_comparison(
    ifelse(improved_xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data2[[target]] == pos.label, 1, 2),
    improved_xgb_train_data2,
    "Improved XGBoost (RFE)"
  )

improved_xgb_all_comparision <-
  rbind(log_null_model_comparison,
        new_xgb_comparision1,
        new_xgb_comparision2,
        improved_xgb_comparision1,
        improved_xgb_comparision2)
kable(improved_xgb_all_comparision)
```

### Improved Models Performance

#### Decision Tree

```{r improved_decision_tree_model_performance}
new_dt_performance1 <-
  model_performance(
    new_dt_train_pred_class1,
    new_dt_test_pred_class1,
    improved_dt_train_data1$player_type,
    improved_dt_test_data1$player_type,
    "Decision Tree (Concatenation)"
  )

new_dt_performance2 <-
  model_performance(
    new_dt_train_pred_class2,
    new_dt_test_pred_class2,
    improved_dt_train_data2$player_type,
    improved_dt_test_data2$player_type,
    "Decision Tree (RFE)"
  )

improved_dt_performance1 <-
  model_performance(
    improved_dt_train_pred_class1,
    improved_dt_test_pred_class1,
    improved_dt_train_data1$player_type,
    improved_dt_test_data1$player_type,
    "Improved Decision Tree (Concatenation)"
  )

improved_dt_performance2 <-
  model_performance(
    improved_dt_train_pred_class2,
    improved_dt_test_pred_class2,
    improved_dt_train_data2$player_type,
    improved_dt_test_data2$player_type,
    "Improved Decision Tree (RFE)"
  )

improved_dt_model_performance <-
  rbind(
    new_dt_performance1,
    new_dt_performance2,
    improved_dt_performance1,
    improved_dt_performance2
  )
kable(improved_dt_model_performance)
```

#### XGBoost

```{r improved_xgboost_model_performance}
new_xgb_performance1 <-
  model_performance(
    new_xgb_train_pred1,
    new_xgb_test_pred1,
    improved_xgb_train_data1$player_type,
    improved_xgb_test_data1$player_type,
    "XGBoost (Concatenation)"
  )

new_xgb_performance2 <-
  model_performance(
    new_xgb_train_pred2,
    new_xgb_test_pred2,
    improved_xgb_train_data2$player_type,
    improved_xgb_test_data2$player_type,
    "XGBoost (RFE)"
  )

improved_xgb_performance1 <-
  model_performance(
    improved_xgb_train_pred1,
    improved_xgb_test_pred1,
    improved_xgb_train_data1$player_type,
    improved_xgb_test_data1$player_type,
    "Improved XGBoost (Concatenation)"
  )

improved_xgb_performance2 <-
  model_performance(
    improved_xgb_train_pred2,
    improved_xgb_test_pred2,
    improved_xgb_train_data2$player_type,
    improved_xgb_test_data2$player_type,
    "Improved XGBoost (RFE)"
  )

improved_xgb_preformance <-
  rbind(
    new_xgb_performance1,
    new_xgb_performance2,
    improved_xgb_performance1,
    improved_xgb_performance2
  )
kable(improved_xgb_preformance)
```

### Improved Models Visualisation

#### Decision Tree

```{r improved_tree_model_performance_plot}
improved_dt_pred_train_roc1 <- predict(improved_dt_model1, newdata = improved_dt_train_data1)
improved_dt_pred_test_roc1 <- predict(improved_dt_model1, newdata = improved_dt_test_data1)

improved_dt_roc_1 <- plot_model_roc(
  improved_dt_pred_train_roc1[, 2],
  improved_train_data$player_type,
  improved_dt_pred_test_roc1[, 2],
  improved_test_data$player_type,
  title = "ROC for Improved Decision Tree (Concatenation)"
)

improved_dt_pred_train_roc2 <- predict(improved_dt_model2, newdata = improved_dt_train_data2)
improved_dt_pred_test_roc2 <- predict(improved_dt_model2, newdata = improved_dt_test_data2)

improved_tree2_roc <- plot_model_roc(
  improved_dt_pred_train_roc2[, 2],
  improved_train_data$player_type,
  improved_dt_pred_test_roc2[, 2],
  improved_test_data$player_type,
  title = "ROC for Improved Decision Tree (RFE)"
)

# visualise the decision tree
rpart.plot(improved_dt_model1, main = "Improved Decision Tree (Concatenation)")
rpart.plot(improved_dt_model2, main = "Improved Decision Tree (RFE)")
```
#### XGBoost

```{r improved_xgboost_model_performance_plot}
improved_xgb_pred_train_roc1 <- predict(improved_xgb_model1, newdata = improved_xgb_input1)
improved_xgb_pred_test_roc1 <- predict(improved_xgb_model1, newdata = improved_xgb_test_input1)

plot_model_roc(
  improved_xgb_pred_train_roc1,
  improved_train_data$player_type,
  improved_xgb_pred_test_roc1,
  improved_test_data$player_type,
  title = "ROC for Improved XGBoost (Concatenation)"
)

improved_xgb_pred_train_roc2 <- predict(improved_xgb_model2, newdata = improved_xgb_input2)
improved_xgb_pred_test_roc2 <- predict(improved_xgb_model2, newdata = improved_xgb_test_input2)

plot_model_roc(
  improved_xgb_pred_train_roc2,
  improved_train_data$player_type,
  improved_xgb_pred_test_roc2,
  improved_test_data$player_type,
  title = "ROC for Improved XGBoost (RFE)"
)
```

## Comparing All Classfication Models

```{r classification_models_comparision}
all_comparison <- rbind(
  dt_comparision1,
  dt_comparision2,
  xgb_comparision1,
  xgb_comparision2,
  improved_dt_comparision1,
  improved_dt_comparision2,
  improved_xgb_comparision1,
  improved_xgb_comparision2
)
kable(all_comparison)

all_performance <-
  rbind(
    dt_performance1,
    dt_performance2,
    xgb_performance1,
    xgb_performance2,
    improved_dt_performance1,
    improved_dt_performance2,
    improved_xgb_performance1,
    improved_xgb_performance2
  )
kable(all_performance)
```

## Discussion

# Cluster

## Clustering problem
clustering different types of players based on their performance in the game

## Data Preprocessing

### Data Transform and cleaning
```{r clustering_transform}
clustering_transform <- function(data) {
  data %>%
    # remove irrelevant and noisy features
    select(
      -c(
        "id",
        "team",
        "name",
        "transfers_out",
        "value_form",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in"
      )
    ) %>%
    # drop highly related columns which can introduce redundancy
    select(
      -c(
        "value_season",
        "points_per_game",
        "clean_sheets_per_90",
        "expected_goals_per_90",
        "expected_assists_per_90",
        "starts_per_90",
        "expected_goals_conceded_per_90",
        "saves_per_90",
        "expected_goal_involvements_per_90",
        "expected_goals_per_90",
        "goals_conceded_per_90",
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type")) %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
}
```

### One Hot Encoding
```{r clustering_OHE}
clustering_OHE <- function(data) {
  # find all categorical cols
  colname <- names(data)
  cat_col_char <-
    str_split(colname[sapply(data[, colname], class) %in% c('factor', 'character')], " ")
  cat_col <- c()
  for (i in 1:length(cat_col_char)) {
    value = cat_col_char[[i]]
    cat_col <- c(cat_col, value)
  }
  # one hot encoding
  encoded_data <-
    dummyVars(paste("~", paste(cat_col, collapse = "+")), data = data) %>%
    predict(newdata = data)
  return_data <-
    cbind(encoded_data, data[, -which(names(data) %in% cat_col)])
  return(return_data)
}
```

## kMeans Clustering

### Read Data
```{r clustering_read_data}
# prepare data
clustering_data <- fpl_raw_data %>%
  clustering_transform() %>%
  clustering_OHE()
# scale
scale_clustering_data <- scale(clustering_data)
```

### Picking Best Number of Clusters

```{r clustering_methods}
sqr_euDist <- function(x, y) {
  sum((x - y) ** 2)
}

wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply(
    clustermat,
    1,
    FUN = function(row) {
      sqr_euDist(row, c0)
    }
  ))
}

wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

tss <- function(scaled_df) {
  wss(scaled_df)
}

CH_index <- function(scaled_df, kmax, method = "kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
    stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
    # kmeans
    for (k in 2:kmax) {
      clustering <- kmeans(scaled_df, k, nstart = 10, iter.max = 100)
      wss.value[k] <- clustering$tot.withinss
    }
  } else {
    # hclust
    d <- dist(scaled_df, method = "euclidean")
    pfit <- hclust(d, method = "ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k = k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax - 1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax,
             CH_index = B / W,
             WSS = wss.value)
}
```

```{r best_number_of_clusters}
kmClustering.ch <- kmeansruns(scale_clustering_data, krange=1:10, criterion="ch")
kmClustering.ch$bestk
kmClustering.asw <- kmeansruns(scale_clustering_data, krange=1:10, criterion="asw")
kmClustering.asw$bestk
# Compare the CH values for kmeans() and hclust().
print("CH index from kmeans for k=1 to 10:")
print(kmClustering.ch$crit)
print("CH index from hclust for k=1 to 10:")
hclusting <- CH_index(scale_clustering_data, 10, method="hclust")
print(hclusting$CH_index)

kmCritframe <- data.frame(k = 1:10,
                          ch = kmClustering.ch$crit,
                          asw = kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x = k, y = ch)) +
  geom_point() + geom_line(colour = "salmon") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "CH index") + theme(text = element_text(size = 20))
fig2 <- ggplot(kmCritframe, aes(x = k, y = asw)) +
  geom_point() + geom_line(colour = "dodgerblue") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "ASW") + theme(text = element_text(size = 20))
grid.arrange(fig1, fig2, nrow = 1)

# calculate the optimal number of clusters
fviz_nbclust(scale_clustering_data, kmeans, method = "wss")
fviz_nbclust(scale_clustering_data, kmeans, method = "silhouette")
```

Applied K-Means Clustering in R
https://www.youtube.com/watch?v=NKQpVU1LTm8

```{r kmeans_clustering}
# clustering
kmClusters <-
  kmeans(
    scale_clustering_data,
    centers = 3,
    iter.max = 100,
    trace = F
  )
kmClusters$size
cat("Total of cluster sizes =", sum(kmClusters$size))
groups <- kmClusters$cluster
# visualize
clusters <- kmClusters$cluster
fviz_cluster(kmClusters, geom = "point", data = scale_clustering_data)

```

## Analyse Clustering Result


```{r convex_hull}
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,
          lapply(
            unique(groups),
            FUN = function(c) {
              f <- subset(proj2Ddf, cluster == c)
              f[chull(f), ]
            }
          ))
}

fig <- c()
kvalues <- seq(2, 5)
nComp <- 2
princ <- prcomp(scale_clustering_data)
project2D <-
  as.data.frame(predict(princ, newdata = scale_clustering_data)[, 1:nComp])
for (k in kvalues) {
  groups <-
    kmeans(scale_clustering_data,
           k,
           nstart = 100,
           iter.max = 100)$cluster
  kmclust.project2D <- cbind(project2D,
                             cluster = as.factor(groups),
                             position = fpl_raw_data$position)
  kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
  assign(
    paste0("fig", k),
    ggplot(kmclust.project2D, aes(x = PC1, y = PC2)) +
      geom_point(aes(shape = cluster, color = cluster)) +
      geom_polygon(
        data = kmclust.hull,
        aes(group = cluster, fill = cluster),
        alpha = 0.4,
        linetype = 0
      ) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position = "none", text = element_text(size = 20))
  )
}
grid.arrange(fig2, fig3, fig4, fig5, nrow = 2)
```

## Discussion

# Conclusion
