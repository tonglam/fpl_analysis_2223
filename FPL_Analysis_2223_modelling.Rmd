---
title: "FPL_Analysis_2223_modelling"
author: 
- "Tong LAN (24056082)"
- "Hanyu XUE (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

## Load libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ROCit)
library(pander)
library(xgboost)
library(lime)
```
# Data Preprocessing

## Initial Transform

```{r preprocess_init_transform}
preprocess_init_transform <- function(data) {
  data %>%
    # drop some columns, which are not useful for our analysis
    # for example, names, teams, points, news and variable related to cost
    select(
      -c(
        "id",
        "team",
        "name",
        "now_cost",
        "transfers_out",
        "value_form",
        "value_season",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "event_points",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "points_per_game",
        "total_points",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in",
        "selected_by_percent",
        "bps",
        "bonus"
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type"))
}
```

## Handle Missing Values

```{r preprocess_init_handle_NAs}
preprocess_init_handle_NAs <- function(data) {
  # check missing values
  sum(is.na(data))
  # check which columns have missing values
  missing_values_sum <- colSums(is.na(data) > 0)
  missing_values_sum[missing_values_sum > 0]
  # convert missing values columns to categorical variables, True represent not missing and False represent missing
  data <- data %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
  # check missing values again
  sum(is.na(data))
  
  return(data)
}
```

## Further Transform

drop Position Column because it is highly correlated with the label column, like we just used it to create the label column.

Then, we should covert the label column to numerical values, 1 present offensive, 0 present defensive.

```{r preprocess_further_transform, results='hide'}
preprocess_further_transform <- function(data) {
  data %>%
    # drop highly correlated columns, if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
    select(
      -c(
        "clean_sheets",
        "expected_assists",
        "starts",
        "expected_goals_conceded",
        "saves",
        "expected_goal_involvements",
        "expected_goals"
      )
    ) %>%
    # transform other performance variable to per 90 values
    mutate(minutes_per_90 = minutes / 90, .after = minutes) %>%
    mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
    mutate(goals_per_90 = goals_scored / minutes * 90,
           .after = goals_scored) %>%
    mutate(goals_conceded_per_90 = goals_conceded / minutes * 90,
           .after = goals_conceded) %>%
    mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
    mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
    mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
    mutate(creativity_per_90 = creativity / minutes * 90,
           .after = creativity) %>%
    mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
    mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
           .after = yellow_cards) %>%
    # drop the origin columns
    select(
      -c(
        "minutes",
        "assists",
        "goals_scored",
        "goals_conceded",
        "red_cards",
        "threat",
        "influence",
        "creativity",
        "own_goals",
        "yellow_cards"
      )
    )
}
```

### Features that were removed

### Features that were included

```{r preprocess_further_handle_NAs}
preprocess_further_handle_NAs <- function(data) {
  # count missing values
  missing_values <- apply(is.na(data), 2, sum)
  which(missing_values > 0)
  # if the actual value and expected value are both 0, then the actual value of 90 and expected value of 90 will result in NaN
  # if the expected 90 values are NaN, it means that the player did not play a single minute
  data <- data %>%
    mutate(
      minutes_per_90 = ifelse(is.na(minutes_per_90), 0, minutes_per_90),
      assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
      goals_per_90 = ifelse(is.na(goals_per_90), 0, goals_per_90),
      goals_conceded_per_90 = ifelse(is.na(goals_conceded_per_90), 0, goals_conceded_per_90),
      red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
      threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
      influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
      creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
      own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
      yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
    )
  
  # look at the missing values again, they have been cleaned
  missing_values <- apply(is.na(data), 2, sum)
  missing_values
  
  return(data)
}
```

## Add Target Value

add a target column

```{r preprocess_target_value}
preprocess_target_value <- function(data) {
  data %>%
    # create target column, 1 represents offensive player, 0 represents defensive player
    mutate(
      player_type = ifelse(position == 'GKP' |
                             position == 'DEF', "Defensive", "Offensive"),
      .before = position
    ) %>%
    mutate(player_type_value = ifelse(player_type == "Defensive", 2, 1),
           .after = player_type) %>%
    select(-position)
}
```

### plot target value

```{r preprocess_target_barplot}
preprocess_target_barplot <- function(data, target) {
  data %>%
    ggplot(aes(x = target, fill = target)) +
    geom_bar(alpha = 0.8, width = 0.8) +
    geom_label(stat = "count",
               aes(label = ..count..),
               show.legend = F) +
    theme_ipsum() +
    scale_fill_brewer(palette = "Set1")
}
```

```{r preprocess_target_donut}
preprocess_target_donut <- function(data) {
  data %>%
    mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
    group_by(player_type) %>%
    summarise(count = n()) %>%
    mutate(
      percentage = count / sum(count),
      ymax = cumsum(percentage),
      ymin = c(0, head(ymax, n = -1)),
      labelPosition = (ymax + ymin) / 2,
      label = paste(ifelse(player_type == 0, 'Defensive' , 'Offensive'),
                    "\n",
                    round(percentage * 100, 2),
                    "%",
                    sep = "")
    ) %>%
    ggplot(aes(
      ymax = ymax,
      ymin = ymin,
      xmax = 4,
      xmin = 3,
      fill = as.factor(player_type)
    )) +
    geom_rect() +
    coord_polar(theta = "y") +
    geom_label(x = 3.5,
               aes(y = labelPosition, label = label),
               size = 4) +
    scale_fill_brewer(palette = 4) +
    scale_color_brewer(palette = 3) +
    xlim(c(2, 4)) +
    theme_void() +
    theme(legend.position = "none")
}
```


## Read Data

```{r read_data, message=FALSE}
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")

model_data <- fpl_raw_data %>% 
  preprocess_init_transform() %>% 
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()

# target value plot
model_data %>% preprocess_target_barplot(model_data$player_type)
model_data %>% preprocess_target_donut()


# remove raw data
rm(fpl_raw_data)

# str(model_data)
# summary(model_data)

# knitr::kable()
```


## Split data into 2 sets

> When working with a small dataset, it is generally recommended to prioritize maximizing the amount of data available for training the model. Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance.

> In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# do a 90/10 split to form the training and test sets.
set.seed(500)
fortrain <- runif(nrow(model_data)) < 0.9
train_data <- model_data[fortrain, ]
test_data <- model_data[!fortrain, ]

# prepare the data for modelling
outCol <- names(train_data)[-c(1, 2)]
target <- 'player_type'
pos.label <- 'Offensive'

# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type', "player_type_value"))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numericVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
```

# Classification

## Binary Classification Problem

offensive and defensive

## Single Variable

## Null Model and Single Variable Model Evaluation

#### LogLikelihood

```{r calculate_log_likelihood}
# define compute function - likelihood
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)), na.rm = T)
}
```

- Evaluate the performance by log likelihood

```{r null_log_likelihood}
# Compute the likelihood of the Null model on the test data
logNull <-
  logLikelihood(sum(test_data[, target] == pos.label) / nrow(test_data),
                test_data[, target] == pos.label)

cat("The log likelihood of the Null model is:", logNull)
## The log likelihood of the Null model is: -47.80357
```

## Single Variable Model Build

### Categorical
- Create a predict function for categorical

```{r categorical_predictions}
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

```{r call_categorical_prediction}
# call the predict function for the candidate columns
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], test_data[, v])
}
```

- Evaluate the categorical performance by AUC

```{r calculate_AUC}
# define compute function - AUC
calcAUC <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}

# define plot function - AUC
plotAUC <- function(data, target, feature) {
 data %>%
  ggplot(aes(x = data[[feature]], color = as.factor(target))) + 
  geom_density() +
  xlab(feature)
}
```


```{r call_categorical_AUC}
catResult <- tribble(~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  aucTrain <- calcAUC(train_data[, pi], train_data[, "player_type"])
  aucTest <- calcAUC(test_data[, pi], test_data[, "player_type"])
  result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                          pi, aucTrain, aucTest)
  catResult <-
    add_row(catResult,
            feature = pi,
            pred = result,
            trainAUC = aucTrain,
            testAUC = aucTest)
}

# sort by train AUC desc, then test AUC desc
catResult <- arrange(catResult, desc(trainAUC), desc(testAUC))
catResult
```

- further explore the AUC values of the above categorical columns

```{r  density plot for AUC values (categorical)}
catAUC1 <- plotAUC(test_data, test_data$player_type, "pred_penalties_order")
catAUC2 <- plotAUC(test_data, test_data$player_type, "pred_corners_and_indirect_freekicks_order")
catAUC3 <- plotAUC(test_data, test_data$player_type, "pred_direct_freekicks_order")
grid.arrange(catAUC1, catAUC2, catAUC3, ncol = 2)
```

The result shows 3 of the features have similar score by AUC. "penalties_order" is the highest AUC score.

### Numerical
- Create a predict function for numerical

```{r numerical_predictions}
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

- Evaluate the performance by AUC

```{r call_numerical_AUC}
# create a list to store the result
numResult <- tribble( ~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in numericVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], test_data[, v])
  aucTrain <- calcAUC(train_data[, pi], train_data[, 'player_type'])
  
  if (aucTrain >= 0.4) {
    aucTest <- calcAUC(test_data[, pi], test_data[, 'player_type'])
    result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                      pi, aucTrain, aucTest)
    numResult <-
      add_row(
        numResult,
        feature = pi,
        pred = result,
        trainAUC = aucTrain,
        testAUC = aucTest
      )
  }
}

# sort by train AUC desc, then test AUC desc
numResult <- arrange(numResult, desc(trainAUC), desc(testAUC))
numResult['pred']
```

- further explore the AUC values of the above numerical columns
```{r  density plot for AUC values (numerical)}
# define a function - get the column name
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}

# call the functions to plot the density plot
Nplotlist <- list()
Ncol <- str_split(extract_column(numResult, "feature"), ", ")[[1]]
for (col in Ncol[1:6]) {
  plot <- plotAUC(test_data, test_data$player_type, col)
  Nplotlist <- c(Nplotlist, list(plot))

}

grid.arrange(grobs = Nplotlist, ncol = 2)
```

### likelihood ratio test

```{r calculate_categorical_likelihood}
# store the top performing categorical variables.
select_cat_result <- tribble(~ feature,  ~ pred, ~ deviance)

minDrop <- 10  # may need to adjust this number
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    select_cat_result <-
      add_row(
        select_cat_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
  }
}

# sort by deviance desc
select_cat_result <- arrange(select_cat_result, desc(deviance))
select_cat_result
```

```{r calculate_numerical_likelihood}
# separate the expected and actual numerical columns
expectedVars <- numericVars[str_detect(numericVars, 'expected')]
actualVars <- str_replace(expectedVars, "expected_", "")
observeVars <- c(expectedVars, actualVars)

# store the top performing categorical variables.
sel_num_result <- tribble( ~ feature, ~ pred, ~ deviance)
observe_selNumResult <- tribble( ~ feature, ~ pred, ~ deviance)

minDrop <- 3  # may need to adjust this number
for (v in numericVars) {
  v
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    sel_num_result <-
      add_row(
        sel_num_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    if (v %in% observeVars) {
      observe_selNumResult <- add_row(
        observe_selNumResult,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    }
  }
}

# sort by deviance desc
sel_num_result <- arrange(sel_num_result, desc(deviance))
observe_selNumResult <-
  arrange(observe_selNumResult, desc(deviance))

# drop actual variable
sel_num_result <-
  sel_num_result %>% filter(!feature %in% actualVars)
sel_num_result$pred
numericVars <- numericVars[!numericVars %in% actualVars]
```

## Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

### Feature Selection Methods

Since our dataset contains both categorical and numerical features, we cannot use simple filter methods such as Pearson Correlation or Chi-Square Test to select features. Because they work only in categorical variables and numerical variables respectively. Instead, we will use the following methods to select features.

> A key part of building many variable models is selecting what variables to use. Each
variable we use represents a chance of explaining more of the outcome variation (a
chance of building a better model), but also represents a possible source of noise and
overfitting. To control this effect, we often preselect which subset of variables we’ll use
to fit.

> Practical Data Science With R

### Concatenation Top-performance Features

From the previous section, we have identified the following features that have good performance in predicting the target variable.

```{r feature_concatenation}
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- colunm_name
  if (length(result) > 0) {
    result <- paste(colunm_name, collapse = ", ")
  }
  return(result)
}

combined_features <-
  c(extract_column(select_cat_result, feature),
    strsplit(extract_column(sel_num_result, feature), ", ")[[1]])

combined_features
```

#### Recursive Feature Elimination - Wrapped Methods

```{r RFE}
rfe_train_data <- train_data %>% 
  select(all_of(c(target, catVars, numericVars))) %>% 
  mutate(player_type = as.factor(player_type))

result <- rfe(rfe_train_data[, -which(names(rfe_train_data) == target)],
              rfe_train_data[, target],
              sizes = c(1:4),
              rfeControl = rfeControl(functions = rfFuncs),
              method = "repeatedcv",
              verbose = FALSE)
rfe_features <- result$optVariables[1:12]
rfe_features
```


## Multiple Variables

## Methods for evaluating models
According to the multiple usage, we define a function so that we can do multiple performance calculations. This function includes several main methods. It will be used in the following sections and increase the efficiency of the code.

```{r All performance_measures function}
# Define functions
#AUC - convert the target variable to numeric
# predict <- as.numeric(predict(model, newdata = data, type = "class"))
# print(calcAUC(predict, train_data[, target]))

# ROC curve
plot_roc <- function(predcol,outcol,colour_id = 2,overlaid = F) {
  #Calculate the performance metrics of an ROC: TPR, FPR
  ROCit_obj <- rocit(score = predcol, class = outcol == pos.label)
  par(new = overlaid)
  plot(
    ROCit_obj,
    col = c(colour_id, 1),
    legend = FALSE,
    YIndex = FALSE,
    values = FALSE
  )
}

# plot_roc(test_data$pred_expected_goal_involvements_per_90,
#          test_data[, target])
# plot_roc(
#   test_data$pred_direct_freekicks_order,
#   test_data[, target],
#   colour_id = 4,
#   overlaid = T
# )

#Confusion matrix - single
cm_value <- function(pred, truth, name = "model") {
   ctable <- table(truth = truth, pred = (pred < 2))
   accuracy <- sum(diag(ctable)) / sum(ctable)
   precision <- ctable[2, 2] / sum(ctable[, 2])
   Recall <- ctable[2, 2] / sum(ctable[2, ])
   F1 <- 2 * precision * Recall / (precision + Recall)
   data.frame(Model = name, Precision = precision,
              Recall = Recall,
              F1 = F1, Accuracy = accuracy)}
   
#Confusion matrix - combined for training data and test data
CM_metrics <- function(pred1, pred2,truth1,truth2, name="model", name1="model1"){
  pred1_performance <- cm_value(pred1, truth1, name1)
  pred2_performance <- cm_value(pred2, truth2, name)
  result <- rbind(pred1_performance, pred2_performance)
  print(result)
}


```

```{r All performance_measures function }
#Define a performance measure function including AUC, AOC, Likelihood Ratio, Confusion Matrix
main_performance <- function(pred_train,pred_test,truth_train,truth_test, pos=pos.label, name="model") {
  auc_train <- calcAUC(pred_train, truth_train)
  auc_test <- calcAUC(pred_test, truth_test)
  cat("AUC for training data is", auc_train, "\n","AUC for test data is", auc_test, "\n")
  auc_plot_train <- plotAUC(pred_train,truth_train)
  auc_plot_test <- plotAUC(pred_test,truth_test)
  auc_plot_train
  auc_plot_test
  # roc_plot <- plot_roc(pred_train, truth_train)
  # print(roc_plot)
  # likelihood_ratio <- logLikelihood(pred_train, truth_train)
  # print(likelihood_ratio)
  # confusionMatrix <- CM_metrics(pred, truth)
  # print(confusionMatrix)

}

```
### Decision Tree
- Build decision tree for all variables

the performance on the test set is slightly better than on the training set,
```{r decision_tree_combined_1}
tree_train_data_1 <-  train_data %>% 
  select(all_of(c(target, combined_features)))

tree_test_data_1 <- test_data %>% 
  select(all_of(c(target, combined_features)))

tmodel <- rpart(player_type ~., data=tree_train_data_1, method = 'class')

trainPredictions <- predict(tmodel, newdata=tree_train_data_1, type = "class")
print(calcAUC(as.numeric(trainPredictions), tree_train_data_1[, target]))
plotAUC(as.numeric(trainPredictions), tree_train_data_1[, target])
trainAccuracy <- mean(trainPredictions == tree_train_data_1$player_type)
print(paste("Training Accuracy:", trainAccuracy))

testPredictions <- predict(tmodel, newdata=tree_test_data_1, type = "class")
print(calcAUC(as.numeric(testPredictions), tree_test_data_1[, target]))
testAccuracy <- mean(testPredictions == tree_test_data_1$player_type)
print(paste("Test Accuracy:", testAccuracy))

```
- Performance Measures
```{r}
#Special data processing for decision tree before calculating the performance measures
predict_train <- as.numeric(predict(tmodel, newdata=tree_train_data, type = "class"))
predict_test <- as.numeric(predict(tmodel, newdata=tree_test_data, type = "class"))
truth_train <- ifelse(tree_train_data$player_type == "Offensive", 1, 2)
truth_test <- ifelse(tree_test_data$player_type == "Offensive", 1, 2)

tmodel_performance <- main_performance(predict_train,predict_test,tree_train_data$player_type,tree_test_data$player_type, name="tmodel")

```

```{r confusion_matrix - metrics}
#convert the target value to match the prediction value
truth_train <- ifelse(tree_train_data$player_type == "Offensive", 1, 2)
truth_test <- ifelse(tree_test_data$player_type == "Offensive", 1, 2)

#print out the confusion matrix result for decision tree
tcm <- CM_metrics(predict_train, predict_test,truth_train, truth_test, "tmodel-Train", "tmodel-Test")
tcm
```

```{r decision_tree_combined_2}
tree_train_data_2 <-  train_data %>% 
  select(all_of(c(target, rfe_features)))

tree_test_data_2 <- test_data %>% 
  select(all_of(c(target, rfe_features)))

tmodel <- rpart(player_type ~., data=tree_train_data_2, method = 'class')

predict <- as.numeric(predict(tmodel, newdata=tree_train_data_2, type = "class"))

print(calcAUC(predict, tree_train_data_2[, target]))

predict <- as.numeric(predict(tmodel, newdata=tree_test_data_2, type = "class"))


pp <- prediction(predict, tree_test_data_2[, target])


print(calcAUC(predict, tree_test_data_2[, target]))

rpart.plot(tmodel)
```


```{r AUC}
#AUC score for the training data and test data
predict_train <- as.numeric(predict(tmodel, newdata=tree_train_data, type = "class"))
print(calcAUC(predict_train, train_data[, target]))

predict_test <- as.numeric(predict(tmodel, newdata=tree_test_data, type = "class"))
print(calcAUC(predict_test, test_data[, target]))

```

```{r confusion_matrix}
table_train <- table(train_data[, target], predict_train)
table_train
```




- Build decision tree for selected variables

### XGBoost

```{r xgboost_combine_1}
# convert categorical variables to factor
xgb_train_data_1 <- train_data %>%
  select(all_of(c(target, combined_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type,
    penalties_order = ifelse(penalties_order == 'Taker', 1, 0)
  )

xgb_test_data_1 <- test_data %>%
  select(all_of(c(target, combined_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type,
    penalties_order = ifelse(penalties_order == 'Taker', 1, 0)
  )

input_1 <- as.matrix(xgb_train_data_1[-c(1, 2)])

# 
# (NROUNDS <- which.min(evalframe$test_logloss_mean))
# 
# ggplot(evalframe, aes(x = iter, y = test_logloss_mean)) +
#   geom_line() +
#   geom_vline(xintercept = NROUNDS,
#              color = "darkred",
#              linetype = 2) +
#   ggtitle("Cross-validated log loss as a function of ensemble size")

xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    lambda = 1,
    alpha = 0.1,
    max_depth = 3,
    min_child_weight = 5,
    eta = 0.1
    )

xgbmodel_1 <- xgboost(
  data = input_1,
  label = xgb_train_data_1$type,
  params = xgb_params,
  nrounds = 100,
  early_stopping_rounds = 10,
  verbose = 0
)

cv_1 <- xgb.cv(
  data = input_1,
  label = xgb_train_data_1$type,
  params = xgb_params,
  nrounds = 100,
  nfold = 5,
  stratified = TRUE
)

evalframe_1 <- as.data.frame(cv_1$evaluation_log)

head(evalframe_1)

test_input_1 <- as.matrix(xgb_test_data_1[-c(1, 2)])


train_pred_1 <- predict(xgbmodel_1, input_1)
test_pred_1 <- predict(xgbmodel_1, test_input_1)

print(calcAUC(train_pred_1, xgb_train_data_1[, target]))
print(calcAUC(test_pred_1, xgb_test_data_1[, target]))
# 
# cv_results <- xgb.cv(
#   data = as.matrix(train_data[, predictors]),
#   label = train_data$target_variable,
#   params = xgb_params,
#   nrounds = 100,
#   nfold = 5,
#   stratified = TRUE,
#   verbose = 0
# )



```

```{r xgboost_combine_2}
# convert categorical variables to factor
xgb_train_data_2 <- train_data %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type)

xgb_test_data_2 <- test_data %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type)

input_2 <- as.matrix(xgb_train_data_2[-c(1, 2)])

xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    lambda = 1,
    alpha = 0.1,
    max_depth = 3,
    min_child_weight = 5,
    eta = 0.1
    )

xgbmodel_2 <- xgboost(
  data = input_2,
  label = xgb_train_data_2$type,
  params = xgb_params,
  nrounds = 100,
  verbose = FALSE
)

cv_2 <- xgb.cv(
  data = input_2,
  label = xgb_train_data_1$type,
  params = xgb_params,
  nrounds = 100,
  nfold = 5,
  stratified = TRUE
)

evalframe_2 <- as.data.frame(cv_2$evaluation_log)

head(evalframe_2)

test_input_2 <- as.matrix(xgb_test_data_2[-c(1, 2)])

train_pred_2 <- predict(xgbmodel_2, input_2)
test_pred_2 <- predict(xgbmodel_2, test_input_2)

print(calcAUC(train_pred_2, xgb_train_data_2[, target]))
print(calcAUC(test_pred_2, xgb_test_data_2[, target]))



```


## Models Evaluation



## Explaining Models using LIME

```{r LIME}
cases <- c(3,11,21,30)

# combined_1
sample_1 <- as.data.frame(xgb_test_data_1[cases, combined_features])
explainer_1 <- lime(xgb_train_data_1[, combined_features], model = xgbmodel_1, bin_continuous = FALSE)
true_label_1 <- xgb_test_data_1[cases, 2]
explanation_1 <- explain(sample_1, explainer_1, n_labels = 1, n_features = 6)
print(explanation_1)

plot_features(explanation_1)
plot_explanations(explanation_1)

# combined_2
sample_2 <- as.data.frame(xgb_test_data_2[cases, rfe_features])
explainer_2 <- lime(xgb_train_data_2[, rfe_features], model = xgbmodel_2, bin_continuous = FALSE)
true_label_2 <- xgb_test_data_2[cases, 2]
explanation_2 <- explain(sample_2, explainer_2, n_labels = 1, n_features = 6)
print(explanation_2)

plot_features(explanation_2)
plot_explanations(explanation_2)
```
## Performance Improve
improve the models performances by implementing such measures:
- increase the training data size
- cross validation

```{r merge_data}
# read new data
fpl_raw_data_1 <- read.csv("./FPL_Dataset_2022-2023.csv")
fpl_raw_data_2 <- read.csv("./FPL_Dataset_2023-2024.csv")
# merge
common_columns <- intersect(names(fpl_raw_data_1), names(fpl_raw_data_2))
fpl_raw_data <- merge(fpl_raw_data_1, fpl_raw_data_2, by = intersect(names(fpl_raw_data_1)), names(fpl_raw_data_2))

head(fpl_raw_data)
```

```{r preprocess_data}

# model_data <- fpl_raw_data %>% 
#   preprocess_init_transform() %>% 
#   preprocess_init_handle_NAs() %>%
#   preprocess_further_transform() %>%
#   preprocess_further_handle_NAs() %>%
#   preprocess_target_value()
# 
# # target value plot
# model_data %>% preprocess_target_barplot(model_data$player_type)
# model_data %>% preprocess_target_donut()
# 
# 
# # remove raw data
# rm(fpl_raw_data)
# 
# # str(model_data)
# # summary(model_data)
# 
# # knitr::kable()
```


## Decision Tree Model

## XGBoost Model

## Discussion

## Conclusion

# Cluster
