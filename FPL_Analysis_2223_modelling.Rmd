---
title: "FPL_Analysis_2223_modelling"
author: 
- "Tong LAN (24056082)"
- "Hanyu XUE (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

## Load libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(dendextend)
library(ROCR)
library(ROCit)
```

## Read Data

```{r read_data, message=FALSE}
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")
```

# Data Preprocessing

## Initial Transform

```{r transform, results='hide'}
model_data <- fpl_raw_data %>%
  # discard some columns, which are not useful for our analysis
  # for example, names, teams, points, news and variable related to cost
  select(
    -c(
      "id",
      "team",
      "name",
      "now_cost",
      "transfers_out",
      "value_form",
      "value_season",
      "cost_change_start",
      "news_added",
      "cost_change_start_fall",
      "ep_next",
      "event_points",
      "web_name",
      "status",
      "news",
      "chance_of_playing_next_round",
      "dreamteam_count",
      "chance_of_playing_this_round",
      "points_per_game",
      "total_points",
      "in_dreamteam",
      "form",
      "ep_this",
      "transfers_in",
      "selected_by_percent",
      "bps",
      "bonus"
    )
  ) %>%
  select(-ends_with("rank")) %>%
  select(-ends_with("rank_type"))

# str(model_data)
# summary(model_data)

# remove raw data
rm(fpl_raw_data)
```

## Handle Missing Values

```{r missing_values}
# check missing values
sum(is.na(model_data))
# check which columns have missing values
missing_values_sum <- colSums(is.na(model_data) > 0)
missing_values_sum[missing_values_sum > 0]
# convert missing values columns to categorical variables, True represent not missing and False represent missing
model_data <- model_data %>%
  mutate(
    corners_and_indirect_freekicks_order = ifelse(
      is.na(corners_and_indirect_freekicks_order),
      "Non-Taker",
      "Taker"
    ),
    penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
    direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
  )
# check missing values again
sum(is.na(model_data))
```
## Future Engineering

drop Position Column because it is highly correlated with the label column, like we just used it to create the label column.

Then, we should covert the label column to numerical values, 1 present offensive, 0 present defensive.

```{r transform_2, results='hide'}
model_data <- model_data %>%
  # drop highly correlated columns, if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
  select(
    -c(
      "clean_sheets",
      "expected_assists",
      "starts",
      "expected_goals_conceded",
      "saves",
      "expected_goal_involvements",
      "expected_goals",
      "goals_conceded"
    )
  ) %>%
  # transform other performance variable to per 90 values
  mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
  mutate(goals_scored_per_90 = goals_scored / minutes * 90,
         .after = goals_scored) %>%
  mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
  mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
  mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
  mutate(creativity_per_90 = creativity / minutes * 90, .after = creativity) %>%
  mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
  mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
         .after = yellow_cards) %>%
  # drop the origin columns
  select(
    -c(
      "assists",
      "goals_scored",
      "red_cards",
      "threat",
      "influence",
      "creativity",
      "own_goals",
      "yellow_cards"
    )
  ) %>%
  # drop columns highly correlated with the some other columns
  select(-c("penalties_missed",
            "penalties_saved",
            "ict_index"))

# str(model_data)
# summary(model_data)
```

### Features that were removed

### Features that were included

```{r missing_values_2}
# count missing values
missing_values <- apply(is.na(model_data), 2, sum)
which(missing_values > 0)
# if the actual value and expected value are both 0, then the actual value of 90 and expected value of 90 will result in NaN
# if the expected 90 values are NaN, it means that the player did not play a single minute
model_data <- model_data %>%
  mutate(
    assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
    goals_scored_per_90 = ifelse(is.na(goals_scored_per_90), 0, goals_scored_per_90),
    red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
    threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
    influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
    creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
    own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
    yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
  )

# look at the missing values again, they have been cleaned
apply(is.na(model_data), 2, sum)
```

## Target Value

add a target column

```{r target}
model_data <- model_data %>%
  # create target column, 1 represents offensive player, 0 represents defensive player
  mutate(
    player_type = ifelse(position == 'GKP' |
                           position == 'DEF', "Defensive", "Offensive"),
    .before = position
  ) %>%
  select(-position)

knitr::kable(model_data[1:5,], caption =  "First 5 rows and first 5 conlumns of data")
```

### plot target value
```{r target_value_barplot}
model_data %>%
  ggplot(aes(x = player_type, fill = player_type)) +
  geom_bar(alpha = 0.8, width = 0.8) +
  geom_label(stat = "count",
             aes(label = ..count..),
             show.legend = F) +
  theme_ipsum() +
  scale_fill_brewer(palette = "Set1")
```

```{r target_value_donut}
model_data %>%
  mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
  group_by(player_type) %>%
  summarise(count = n()) %>%
  mutate(
    percentage = count / sum(count),
    ymax = cumsum(percentage),
    ymin = c(0, head(ymax, n = -1)),
    labelPosition = (ymax + ymin) / 2,
    label = paste(
      ifelse(player_type == 0, 'Defensive' , 'Offensive'),
      "\n",
      round(percentage * 100, 2),
      "%",
      sep = ""
    )
  ) %>%
  ggplot(aes(
    ymax = ymax,
    ymin = ymin,
    xmax = 4,
    xmin = 3,
    fill = as.factor(player_type)
  )) +
  geom_rect() +
  coord_polar(theta = "y") +
  geom_label(x = 3.5,
             aes(y = labelPosition, label = label),
             size = 4) +
  scale_fill_brewer(palette = 4) +
  scale_color_brewer(palette = 3) +
  xlim(c(2, 4)) +
  theme_void() +
  theme(legend.position = "none")
```

## Split data into 2 sets

> When working with a small dataset, it is generally recommended to prioritize maximizing the amount of data available for training the model. Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance.

> In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# do a 90/10 split to form the training and test sets.
set.seed(500)
fortrain <- runif(nrow(model_data)) < 0.9
train_data <- model_data[fortrain,]
test_data <- model_data[-fortrain,]

# prepare the data for modelling
outCol <- names(train_data)[-1]
outcome <- 'player_type'
pos.label <- 'Offensive'

# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type'))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numericVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
```

# Classification

## Binary Classification Problem

offensive and defensive

## Single Variable

## Null Model and Single Variable Model Evaluation

#### LogLikelihood

```{r calculate_log_likelihood}
# define compute function - likelihood
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)), na.rm = T)
}
```

- Evaluate the performance by log likelihood

```{r null_log_likelihood}
# Compute the likelihood of the Null model on the test data
logNull <-
  logLikelihood(sum(test_data[, outcome] == pos.label) / nrow(test_data),
                test_data[, outcome] == pos.label)

cat("The log likelihood of the Null model is:", logNull)
## The log likelihood of the Null model is: -534.1338
```

## Single Variable Model Build

### AUC

- Create a predict function for categorical

```{r catogorical_predictions}
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

```{r call_categorical_prediction}
# call the predict function for the candidate columns
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], test_data[, v])
}
```

- Evaluate the categorical performance by AUC

```{r calculate_AUC}
# define compute function - AUC
calcAUC <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}
```

```{r call_categorical_AUC}
catResult <- tribble(~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  aucTrain <- calcAUC(train_data[, pi], train_data[, "player_type"])
  aucTest <- calcAUC(test_data[, pi], test_data[, "player_type"])
  result <- print(sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                          pi, aucTrain, aucTest))
  catResult <-
    add_row(catResult,
            feature = pi,
            pred = result,
            trainAUC = aucTrain,
            testAUC = aucTest)
}

# sort by train AUC desc, then test AUC desc
catResult <- arrange(catResult, desc(trainAUC), desc(testAUC))
catResult
```

- further explore the AUC values of the above categorical columns

```{r  density plot for AUC values (categorical)}
catAUC1 <- ggplot(test_data) + geom_density(aes(x=pred_penalties_order, color=as.factor(player_type)))
catAUC2 <- ggplot(test_data) + geom_density(aes(x=corners_and_indirect_freekicks_order, color=as.factor(player_type)))
catAUC3 <- ggplot(test_data) + geom_density(aes(x=pred_direct_freekicks_order, color=as.factor(player_type)))
grid.arrange(catAUC1, catAUC2,catAUC3, ncol=2)
```

The result shows 3 of the features have similar score by AUC. "penalties_order" is the highest AUC score.

### Numerical
- Create a predict function for numerical

```{r numerical_predictions}
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

- Evaluate the performance by AUC

```{r call_numerical_AUC}
# create a list to store the result
numResult <- tribble(~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in numericVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], test_data[, v])
  aucTrain <- calcAUC(train_data[, pi], train_data[, 'player_type'])
  
  if (aucTrain >= 0.4) {
    aucTest <- calcAUC(test_data[, pi], test_data[, 'player_type'])
    result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                      pi, aucTrain, aucTest)
    numResult <-
      add_row(numResult,
              feature = pi,
              pred = result,
              trainAUC = aucTrain,
              testAUC = aucTest)
  }
}

# sort by train AUC desc, then test AUC desc
numResult <- arrange(numResult, desc(trainAUC), desc(testAUC))
numResult
```

- further explore the AUC values of the above numerical columns
```{r  density plot for AUC values (numerical)}
#define a function - plotting
nAUCplot <- function(data, target, colName) {
  p <- ggplot(data) + geom_density(aes(x = data[, colName], color = as.factor(target)))
  return(p)
}

#define a function - get the column name
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}
#call the functions to plot the density plot
Nplotlist <- list()
Ncol <- str_split(extract_column(numResult, "feature"), ", ")[[1]]
for (col in Ncol) {
  plot <- nAUCplot(test_data, test_data$player_type, col)
  # grid.arrange(plot,ncol = 2)
  Nplotlist <- c(Nplotlist, plot)
}

grid.arrange(grobs= Nplotlist,ncol = 2)
```
- Compare the ROC
```{r the ROC for numerical variables}
#define a function - ROC curve
plot_roc <- function(predcol, outcol, colour_id=2, overlaid=F) {
ROCit_obj <- rocit(score=predcol, class=outcol==pos.label)
par(new=overlaid)
plot(ROCit_obj, col = c(colour_id, 1),
legend = FALSE, YIndex = FALSE, values = FALSE)
}

plot_roc(test_data$pred_expected_goal_involvements_per_90, test_data[,outcome]) 
plot_roc(test_data$pred_direct_freekicks_order, test_data[,outcome], colour_id=4, overlaid=T)
```



### likelihood ratio test

```{r calculate_categorical_likelihood}
# store the top performing categorical variables.
select_cat_result <- tribble( ~ feature,  ~ pred, ~ deviance)

minDrop <- 30  # may need to adjust this number
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, outcome] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    select_cat_result <-
      add_row(
        select_cat_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
  }
}

# sort by deviance desc
select_cat_result <- arrange(select_cat_result, desc(deviance))
select_cat_result
```

```{r calculate_numerical_likelihood}
# separate the expected and actual numerical columns
expectedVars <- numericVars[str_detect(numericVars, 'expected')]
actualVars <- c("assists_per_90", "goals_scored_per_90", "goals_conceded_per_90")
observeVars <- c(expectedVars, actualVars)

# store the top performing categorical variables.
sel_num_result <- tribble(~ feature, ~ pred, ~ deviance)
observe_selNumResult <- tribble(~ feature, ~ pred, ~ deviance)

minDrop <- 10  # may need to adjust this number
for (v in numericVars) {
  v
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, outcome] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    sel_num_result <-
      add_row(
        sel_num_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    if (v %in% observeVars) {
      observe_selNumResult <- add_row(
        observe_selNumResult,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    }
  }
}

# sort by deviance desc
sel_num_result <- arrange(sel_num_result, desc(deviance))
sel_num_result
observe_selNumResult <-
  arrange(observe_selNumResult, desc(deviance))
observe_selNumResult

# drop actual variable
sel_num_result <- sel_num_result %>% filter(!feature %in% actualVars)
sel_num_result
```

## Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

### Feature Selection Methods

Since our dataset contains both categorical and numerical features, we cannot use simple filter methods such as Pearson Correlation or Chi-Square Test to select features. Because they work only in categorical variables and numerical variables respectively. Instead, we will use the following methods to select features.

> A key part of building many variable models is selecting what variables to use. Each
variable we use represents a chance of explaining more of the outcome variation (a
chance of building a better model), but also represents a possible source of noise and
overfitting. To control this effect, we often preselect which subset of variables we’ll use
to fit.

> Practical Data Science With R

### Concatenation Top-performance Features

From the previous section, we have identified the following features that have good performance in predicting the target variable.

```{r feature_concatenation}
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}

combined_features <-
  paste(
    extract_column(select_cat_result, feature),
    extract_column(sel_num_result, feature),
    sep = ", "
  )

combined_features
```

#### Recursive Feature Elimination - Wrapped Methods

```{r RFE}

features <- train_data %>% 
  select(-c("player_type"))
target <- train_data$player_type

# Specify the number of desired features to select
num_features <- 3

# Create the RFE control object
ctrl <- rfeControl(
  functions = rfFuncs,  # Use decision tree as the learning algorithm
  method = "cv",  # Cross-validation method
  number = 5,  # Number of folds for cross-validation
  verbose = FALSE
)

# Perform feature selection using RFE
rfe_result <- rfe(features, target, sizes = num_features, rfeControl = ctrl)

# Get the selected features
selected_features <- rfe_result$optVariables
```


## Multiple Variables

### Decision Tree
- Build decision tree for all variables
```{r decision_tree}
fV <- paste(outcome,' ~',
paste(c(catVars, numericVars), collapse=' + '),
sep='')

tmodel <- rpart(fV, data=train_data)
rpart.plot(tmodel)
```
- Performance Measures
```{r performance_measures}
#AUC sc
fV <- paste(outcome,'>0 ~',
paste(c(catVars, numericVars), collapse=' + '),
sep='')
tmodel <- rpart(fV, data=train_data)

print(calcAUC(predict(tmodel, newdata=train_data), train_data[,outcome]))
```


- Build decision tree for selected variables


### Random Forest

### XGBoost
```{r}

```


## Models Evaluation

### Confusion Matrix
```{r}
# 设置新的阈值
threshold.value = 0.7  # 例如，将阈值设为0.7

# 根据阈值将预测的类别重新分配
predicted_classes <- ifelse(train_data$pred_expected_goal_involvements_per_90 >= threshold.value, 1, 0)

# 计算混淆矩阵
confusion_matrix <- confusionMatrix(predicted_classes, train_data$expected_goal_involvements_per_90)

# 输出混淆矩阵
print(confusion_matrix)


```


## Explaining Models using LIME

### Decision Tree Model

## XGBoost Model

## Discussion

## Conclusion

# Cluster
