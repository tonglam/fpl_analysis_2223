---
title: "FPL_Analysis_22-23 - Modelling"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

The dataset used in Project 2 is the same as Project 1, obtained from [*kaggle.com*](https://www.kaggle.com/), collected by *PAOLO MAZZA*. [1] As stated in Project 1, the dataset serves as a fundamental compilation of statistics derived from the official Fantasy Premier League website for the 2022-2023 season.

In Project 1, we have explored the dataset and discovered a significant correlation between the position and various performance variables, such as goals_scored, assists, clean_sheets, etc. As a result, our focus on Project 2 lies on the player's position, and we intend to build classification models to predict the labels associated with the players' positions. Additionally, we are employing clustering methods to group players based on their performance, with the aim of identifying similarities among them.

# Part 1 - Data Preprocessing

Before classifying and clustering, we need to apply some data transformation and cleaning to make the dataset more readable and easier to analysis. Firstly, based on our domain knowledge of football, we can drop some columns in the original dataset that are not useful for our analysis, aiming to reduce the dimensionality of our data. Then, we treat missing values by converting categorical variables into new binary categorical values and using a suitable approach to handle missing numerical values. Finally, we add our target label column for classification, examining the distribution of our target values.

## Load Libraries

Please load the necessary libraries for Project 2. If you have not installed these libraries, please uncomment the `install.packages()` code first.

```{r library, message=FALSE}
# install.packages("tidyverse")
# install.packages("knitr")
# install.packages("hrbrthemes")
# install.packages("RColorBrewer")
# install.packages("gridExtra")
# install.packages("caret")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("ROCR")
# install.packages("ROCit")
# install.packages("pander")
# install.packages("xgboost")
# install.packages("lime")
# install.packages("factoextra")
# install.packages("fpc")
# install.packages("grDevices")

library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ROCit)
library(pander)
library(xgboost)
library(lime)
library(factoextra)
library(fpc)
library(grDevices)
```

## Initial Transform

In the initial transform process, we utilize our domain knowledge to drop certain columns that are not relevant for the following analysis. This includes dropping columns with unique values and removing columns associated with "rank".

```{r preprocess_init_transform}
preprocess_init_transform <- function(data) {
  data %>%
    # drop some columns, which are not useful for our analysis
    select(
      -c(
        "id",
        "team",
        "name",
        "now_cost",
        "transfers_out",
        "value_form",
        "value_season",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "event_points",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "points_per_game",
        "total_points",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in",
        "selected_by_percent",
        "bps",
        "bonus"
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type"))
}
```

## Treating Missing Values

In the original dataset, there are three columns that contain missing values: "penalties_order", "corners_and_indirect_freekicks_order" and "direct_freekicks_order". It is evident that these columns represent categorical variables and the missing values exhibit a non-random pattern. Consequently, we can transform these columns into new binary categorical values, namely "Taker" and "Non-Taker".

```{r preprocess_init_handle_NAs}
preprocess_init_handle_NAs <- function(data) {
  # check missing values
  sum(is.na(data))
  # check which columns have missing values
  missing_values_sum <- colSums(is.na(data) > 0)
  missing_values_sum[missing_values_sum > 0]
  # convert missing values columns to categorical variables
  data <- data %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
  # check missing values again
  sum(is.na(data))
  return(data)
}
```

## Further Transform

In the next step, we will convert all the performance variables into per 90 values. This transformation allows us to standardize the performance variables to a consistent unit, making them more comparable. Once the per 90 values have been calculated, we can remove the original performance variables as they are clearly highly correlated with the per 90 values.

After the transformation, we will re-evaluate the presence of missing values. As the remaining missing values are all represented as NaN, we can replace them with 0 to handle the missing numerical values.

In the final part, we will provide a function that can convert binary categorical values into 1 or 0 for future use.

```{r preprocess_further_transform, results='hide'}
preprocess_further_transform <- function(data) {
  data %>%
    # drop highly correlated columns
    #(move to the content)if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
    select(
      -c(
        "clean_sheets",
        "expected_assists",
        "starts",
        "expected_goals_conceded",
        "saves",
        "expected_goal_involvements",
        "expected_goals"
      )
    ) %>%
    # transform other performance variable to per 90 values
    mutate(minutes_per_90 = minutes / 90, .after = minutes) %>%
    mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
    mutate(goals_per_90 = goals_scored / minutes * 90,
           .after = goals_scored) %>%
    mutate(goals_conceded_per_90 = goals_conceded / minutes * 90,
           .after = goals_conceded) %>%
    mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
    mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
    mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
    mutate(creativity_per_90 = creativity / minutes * 90,
           .after = creativity) %>%
    mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
    mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
           .after = yellow_cards) %>%
    # drop the origin columns
    select(
      -c(
        "minutes",
        "assists",
        "goals_scored",
        "goals_conceded",
        "red_cards",
        "threat",
        "influence",
        "creativity",
        "own_goals",
        "yellow_cards"
      )
    )
}
```

```{r preprocess_further_handle_NAs}
preprocess_further_handle_NAs <- function(data) {
  # count missing values
  missing_values <- apply(is.na(data), 2, sum)
  which(missing_values > 0)
  # fill missing values with 0
  data <- data %>%
    mutate(
      minutes_per_90 = ifelse(is.na(minutes_per_90), 0, minutes_per_90),
      assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
      goals_per_90 = ifelse(is.na(goals_per_90), 0, goals_per_90),
      goals_conceded_per_90 = ifelse(is.na(goals_conceded_per_90), 0, goals_conceded_per_90),
      red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
      threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
      influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
      creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
      own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
      yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
    )
  # look at the missing values again, they have been cleaned
  missing_values <- apply(is.na(data), 2, sum)
  missing_values
  return(data)
}
```

```{r convert categorical to numerical}
convert_categorial <- function(data) {
  if (any(grepl("penalties_order", colnames(data)))) {
    data <-
      data %>% mutate(penalties_order = ifelse(penalties_order == "Taker", 1, 0))
  }
  if (any(grepl("direct_freekicks_order", colnames(data)))) {
    data <-
      data %>% mutate(direct_freekicks_order = ifelse(direct_freekicks_order == "Taker", 1, 0))
  }
  if (any(grepl("corners_and_indirect_freekicks_order", colnames(data)))) {
    data <-
      data %>% mutate(
        corners_and_indirect_freekicks_order = ifelse(corners_and_indirect_freekicks_order == "Taker", 1, 0)
      )
  }
  return(data)
}
```

# Part 2 - Classification

## Binary Classification Problem

From the original dataset, we can see that there are 4 positions: "GKP", "DEF", "MID", and "FWD". We add a target value (label) called "player_type", where we classify "GKP" and "DEF" as defensive players, and "MID" and "FWD" as offensive players. This results in the target value for player_type being either 'Offensive' or 'Defensive'.

```{r binary target variable}
target <- 'player_type'
pos.label <- 'Offensive'
neg.label <- 'Defensive'
threhold <- 0.5
```

```{r preprocess_target_value}
preprocess_target_value <- function(data) {
  data %>%
    # create target column, 1 represents offensive player, 2 represents defensive player
    mutate(
      player_type = ifelse(position == 'GKP' |
                             position == 'DEF', "Defensive", "Offensive"),
      .before = position
    ) %>%
    select(-position)
}
```

We define plot functions to examine the distribution of the target value. We check if the distribution follows a Bernoulli distribution and if the labels are balanced classes, which is ideal for a binary classification problem.

```{r preprocess_target_barplot}
preprocess_target_barplot <- function(data, target) {
  data %>%
    ggplot(aes(x = target, fill = target)) +
    geom_bar(alpha = 0.8, width = 0.8) +
    geom_label(stat = "count",
               aes(label = after_stat(count)),
               show.legend = F) +
    theme_ipsum() +
    scale_fill_brewer(palette = "Set1")
}
```

```{r preprocess_target_donut}
preprocess_target_donut <- function(data) {
  data %>%
    mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
    group_by(player_type) %>%
    summarise(count = n()) %>%
    mutate(
      percentage = count / sum(count),
      ymax = cumsum(percentage),
      ymin = c(0, head(ymax, n = -1)),
      labelPosition = (ymax + ymin) / 2,
      label = paste(
        ifelse(player_type == 0, 'Defensive' , 'Offensive'),
        "\n",
        round(percentage * 100, 2),
        "%",
        sep = ""
      )
    ) %>%
    ggplot(aes(
      ymax = ymax,
      ymin = ymin,
      xmax = 4,
      xmin = 3,
      fill = as.factor(player_type)
    )) +
    geom_rect() +
    coord_polar(theta = "y") +
    geom_label(x = 3.5,
               aes(y = labelPosition, label = label),
               size = 4) +
    scale_fill_brewer(palette = 4) +
    scale_color_brewer(palette = 3) +
    xlim(c(2, 4)) +
    theme_void() +
    theme(legend.position = "none")
}
```

## Remain Variables

-   `player_type` - The target variable, which is the Offensive or Defensive
-   `clean_sheet_per_90` - The number of clean sheets per 90 minutes played
-   `expected_assists_per_90` - The number of expected assists per 90 minutes played
-   `expected_goals_per_90` - The number of expected goals per 90 minutes played
-   `goals_conceded_per_90` - The number of goals conceded per 90 minutes played
-   `minutes_per_90` - The number of minutes played per 90 minutes played
-   `own_goals_per_90` - The number of own goals per 90 minutes played
-   `red_cards_per_90` - The number of red cards per 90 minutes played
-   `saves_per_90` - The number of saves per 90 minutes played
-   `yellow_cards_per_90` - The number of yellow cards per 90 minutes played
-   `penalties_order` - Whether the player is the penalty taker
-   `direct_freekicks_order` - Whether the player is the direct free kick taker
-   `corners_and_indirect_freekicks_order` - Whether the player is the corner and indirect free kick taker
-   `goals_per_90` - The number of goals per 90 minutes played
-   `penalties_missed` - The number of penalties missed
-   `starts_per_90` - The number of starts per 90 minutes played
-   `expected_goals_conceded_per_90` - The number of expected goals conceded per 90 minutes played
-   `red_cards_per_90` - The number of red cards per 90 minutes played
-   `threat_per_90` - The number of threats per 90 minutes played
-   `influence_per_90` - The number of influence per 90 minutes played
-   `penalties_saved` - The number of penalties saved
-   `creativity_per_90` - The number of creativity per 90 minutes played
-   `expected_goals_involvements_per_90` - The number of expected goals involvements per 90 minutes played
-   `assists_per_90` - The number of assists per 90 minutes played
-   `ict_index` - The ICT index

## Read Data

After data transformation and cleaning, we start by reading the raw data of the 2022-23 season FPL. We then proceed to implement all the necessary data preprocessing steps and observe the distribution of the target values using a bar plot and a donut plot.

```{r read_data}
# read data and preprocess
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")
classification_data <- fpl_raw_data %>%
  preprocess_init_transform() %>%
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()
# target value plot
classification_data %>% preprocess_target_barplot(classification_data$player_type)
classification_data %>% preprocess_target_donut()
# remove raw data
rm(fpl_raw_data)
```

From these two plots, it is evident that the data is relatively balanced, as the disparity between offensive players and defensive players is not significant. Moving forward, we will perform sampling on the data.

## Split Data

It is recommended to prioritize maximizing the amount of data available for training the model, Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance. In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# split to training and testing set
split_ratio <- 0.9
set.seed(500)
fortrain <- runif(nrow(classification_data)) < split_ratio
train_data <- classification_data[fortrain,]
test_data <- classification_data[!fortrain,]
# prepare the data for modelling
outCol <- names(train_data)[-c(1, 2)]
# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type'))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
# view data
cat(paste("train data dim:"), dim(train_data), "\n")
kable(train_data[1:5, ], caption =  "First 5 rows of classification train data")

cat(paste("train data dim:"), dim(test_data), "\n")
kable(test_data[1:5, ], caption =  "First 5 rows of classification test data")
```

## Model Comparing Metrics

The Model Comparing Metrics used in this project are log-likelihood and reduction deviance. The log-likelihood is a statistical measure used to assess the goodness of fit of a statistical model, typically in the context of maximum likelihood estimation. It quantifies the probability of observed data given a specific model and its parameter values. [3] Reduction Deviance associated with likelihood ratio tests and helps assess the goodness of fit of a model. [3]

```{r model_comparing_metrics}
# extract column
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- colunm_name
  if (length(result) > 0) {
    result <- paste(colunm_name, collapse = ", ")
  }
  return(result)
}

# log likelihood
calc_log_likelihood <- function(ypred, ytrue, epsilon = 1e-6) {
  log_likelihood <-
    sum(ifelse(ytrue, log(ypred), log(1 - ypred - epsilon)), na.rm = T)
  return(log_likelihood)
}

calc_null_log_likelihood <- function(data, epsilon = 1e-6) {
  null_log_likelihood <-
    calc_log_likelihood(sum(data[[target]] == pos.label) / nrow(data),
                        data[[target]] == pos.label)
  return(null_log_likelihood)
}

# reduction of deviance
calc_drop_deviance <- function(ypred, ytrue, data, epsilon = 1e-6) {
  null_log_likelihood <- calc_null_log_likelihood(data)
  model_log_likelihood <- calc_log_likelihood(ypred, ytrue, epsilon)
  drop_deviance <- 2 * (model_log_likelihood - null_log_likelihood)
  return(drop_deviance)
}

null_model_comparison <- function(data, epsilon = 1e-6) {
  null_model <- data.frame(
    Model = 'Null Model',
    Log_Likelihood = calc_null_log_likelihood(data),
    Reduction_Deviance = 0
  )
  row.names(null_model) <- NULL
  return(null_model)
}

model_comparison <-
  function(ypred,
           ytrue,
           data,
           model = 'Model',
           epsilon = 1e-6) {
    log_likelihood <- calc_log_likelihood(ypred, ytrue, epsilon)
    drop_deviance <- calc_drop_deviance(ypred, ytrue, data, epsilon)
    compare_df <- data.frame(
      Model =  model,
      Log_Likelihood = log_likelihood,
      Reduction_Deviance = drop_deviance
    )
    row.names(compare_df) <- NULL
    return(compare_df)
  }
```

## Model Evaluation Metrics

We employ multiple metrics to assess the performance of a classification model. The metrics used in this project are accuracy, precision, recall, F1 score, and AUC. Accuracy is the ratio of correctly predicted observations to the total observations. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class. F1 score is the weighted average of Precision and Recall. AUC stands for Area Under the Curve. It is a single number summary of classifier performance.

```{r single_evaluation_metrics}
# AUC calculation
calc_auc <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}

# evaluation result
single_evaluation <- function(vars) {
  varResult <-
    tribble(
      ~ Type,
      ~ Pred_Variable,
      ~ Log_Likelihood,
      ~ Reduction_Deviance,
      ~ Train_AUC,
      ~ Test_AUC
    )
  for (v in vars) {
    type <- ''
    if (v %in% catVars) {
      type <- 'categorical'
    } else if (v %in% numVars) {
      type <- 'numerical'
    }
    pred_variable <- paste('pred_', v, sep = '')
    log_likelihood <-
      calc_log_likelihood(train_data[[pred_variable]], train_data[[target]] == pos.label)
    drop_deviance <-
      calc_drop_deviance(train_data[[pred_variable]], train_data[[target]] == pos.label, train_data)
    train_auc <-
      calc_auc(train_data[, pred_variable], train_data[[target]])
    test_auc <-
      calc_auc(test_data[, pred_variable], test_data[[target]])
    
    varResult <-
      add_row(
        varResult,
        Type = type,
        Pred_Variable = pred_variable,
        Log_Likelihood = log_likelihood,
        Reduction_Deviance = drop_deviance,
        Train_AUC = train_auc,
        Test_AUC = test_auc
      )
  }
  
  varResult <- arrange(varResult, type, desc(Log_Likelihood))
  
  # add null model values
  varResult <- add_row(
    varResult,
    Type = 'Null Model',
    Pred_Variable = 'Null Model',
    Log_Likelihood = calc_null_log_likelihood(train_data),
    Reduction_Deviance = 0,
    Train_AUC = 0.5,
    Test_AUC = 0.5,
    .before = 1
  )
  
  return (varResult)
}
```

Also, the area under the curve (AUC) plot and the receiver operating characteristic (ROC) plot could assist us in easily observing the performance of the classification model.

```{r single_evaluation_plot}
# AUC plot
plot_auc <- function(data, target, feature) {
  data %>%
    ggplot(aes(x = data[[feature]], color = as.factor(player_type))) +
    geom_density() +
    xlab(feature)
}

# ROC plot - single variable model
plot_roc_sa <- function(data,
                        features,
                        title) {
  colors = c("salmon",
             "lightblue",
             "navyblue",
             "seagreen",
             "orchid",
             "lightpink")
  for (i in 1:length(features)) {
    par(new = T)
    feature = features[i]
    pred_feature = paste0("pred_", feature)
    ROCit_obj <-
      rocit(score = data[[pred_feature]], class = data[, target] == pos.label)
    plot(
      ROCit_obj,
      col = c(colors[i], 1),
      legend = FALSE,
      YIndex = FALSE,
      values = FALSE
    )
    title(title)
  }
  legend(
    "bottomright",
    legend = features,
    col = colors,
    cex = 0.6,
    lty = 1
  )
}
```

We define functions for the confusion matrix and combine them with other metrics. Additionally, we create a table to store and display the relevant performance for each model.

```{r evaluation_metrics}
model_evaluation <-
  function(pred,
           truth,
           name = "model",
           threshold = 0.5) {
    # data preparation
    if (str_detect(name, "XGBoost")) {
      pred_class <- ifelse(pred > threshold, pos.label, neg.label)
      truth_class <-
        ifelse(truth > threshold, pos.label, neg.label)
    }
    factor_pred <- pred
    if (class(pred) != "factor") {
      if (str_detect(name, "XGBoost")) {
        factor_pred <- as.factor(pred_class)
      } else{
        factor_pred <- as.factor(pred)
      }
    }
    factor_truth <- truth
    if (class(truth) != "factor") {
      factor_truth <- as.factor(truth)
    }
    # metrics calculation
    cm <- confusionMatrix(factor_pred, factor_truth)
    accuracy <- cm$overall['Accuracy']
    precision <- cm$byClass['Pos Pred Value']
    recall <- cm$byClass['Sensitivity']
    f1 <- cm$byClass['F1']
    
    if (str_detect(name, "Decision Tree")) {
      pred <- as.numeric(pred)
    }
    AUC <- calc_auc(pred, truth)
    # store the result
    result <- data.frame(
      Model = name,
      Accuracy = accuracy,
      Precision = precision,
      Recall = recall,
      F1 = f1,
      AUC = AUC
    )
    row.names(result) <- NULL
    return (result)
  }

model_performance <-
  function(train_pred,
           test_pred,
           train_truth,
           test_truth,
           name = "model") {
    train_performance <-
      model_evaluation(train_pred, train_truth, paste(name, "- Train"))
    test_performance <-
      model_evaluation(test_pred, test_truth, paste(name, "- Test"))
    return(rbind(train_performance, test_performance))
  }

multiple_performance <- function(train_preds,
                                 test_preds,
                                 train_truths,
                                 test_truths,
                                 names) {
  result <- data.frame()
  for (i in 1:length(names)) {
    train_pred <- train_preds[[i]]
    test_pred <- test_preds[[i]]
    train_truth <- train_truths[[i]]
    test_truth <- test_truths[[i]]
    name <- names[[i]]
    performance <-
      model_performance(train_pred, test_pred, train_truth, test_truth, name)
    result <- rbind(result, performance)
  }
}
```

```{r evaluation_plot}
# ROC curve - multiple variable model
plot_model_roc <-
  function(train_pred,
           train_truth,
           test_pred,
           test_truth,
           title) {
    roc_train <-
      rocit(score = train_pred, class = train_truth == pos.label)
    roc_test <-
      rocit(score = test_pred, class = test_truth == pos.label)
    
    plot(
      roc_train,
      col = c("lightblue", "forestgreen"),
      lwd = 3,
      legend = FALSE,
      YIndex = FALSE,
      values = TRUE,
      asp = 1
    )
    lines(
      roc_test$TPR ~ roc_test$FPR,
      lwd = 3,
      col = c("salmon", "forestgreen"),
      asp = 1
    )
    legend(
      "bottomright",
      col = c("lightblue", "salmon", "forestgreen"),
      c("Model on Train", "Model on Test", "Null Model"),
      cex = 0.8,
      lwd = 2
    )
    title(title)
  }

# tree plot
plot_tree <- function(model) {
  rpart.plot(model)
}
```

## Null Model Performance

```{r null_model_performance}
log_null_model_comparison <- null_model_comparison(train_data)
kable(log_null_model_comparison, caption = "Null Model")

null_TP <- 0
null_TN <- sum(train_data[[target]] == neg.label)
null_FP <- 0
null_FN <- sum(train_data[[target]] == pos.label)

null_accuracy <- (null_TP + null_TN) / nrow(train_data)
null_precision <- null_TP / (null_TP + null_FP)
null_recall <- null_TP / (null_TP + null_FN)
null_Npos <- sum(train_data[[target]] == pos.label)
null_F1 <-
  2 * (null_precision * null_recall) / (null_precision + null_recall)
pred.Null <- null_Npos / nrow(train_data)
pred.Null <- rep(pred.Null, nrow(train_data))
null_AUC <- calc_auc(pred.Null, train_data[[target]])

null_performance <- tribble(
  ~ Model,
  ~ Accuracy,
  ~ Precision,
  ~ Recall,
  ~ F1,
  ~ AUC,
  "Null Model",
  null_accuracy,
  null_precision,
  null_recall,
  null_F1,
  null_AUC
)
kable(null_performance, caption = "Null Model Evaluation")
```

At the beginning of building our classification models, we should first exam the performance of Null Model, to give a baseline for evaluating our model performances. Upon examining the result table, we observe that the Null Model yields a log-likelihood of -486.8746, an accuracy of 0.5, and precision, recall, and F1 score of 0. The AUC score, which is identical to the accuracy, is also 0.5.

## Single Variable Models

In this section, we will build models using each individual variable from the training dataset and assess their performances. We will separate the variables into categorical and numerical variables, and exam each of them.

### Categorial Variables

First, we can build categorical single variable models and obtain the model predictions.

```{r categorical_predictions}
# categorical variables prediction
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos, ] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  return(pred)
}

# predict all the categorical variables
predCatVars <- c()
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  predCatVars <- c(predCatVars, pi)
  train_data[, pi] <-
    mkPredC(train_data[[target]], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[[target]], train_data[, v], test_data[, v])
}

# view categorical predictions
kable(train_data[1:5, which(names(train_data) %in% predCatVars)], caption =  "First 5 rows of categorical predictions")
```

### Numerical Variables

Next, we can build numerical single variable models and obtain the model predictions, too.

```{r numerical_predictions}
# numerical variables prediction
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# predict all the numerical variables
predNumVars <- c()
for (v in numVars) {
  pi <- paste('pred_', v, sep = '')
  predNumVars <- c(predNumVars, pi)
  train_data[, pi] <-
    mkPredN(train_data[[target]], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[[target]], train_data[, v], test_data[, v])
}

# view numerical predictions
kable(train_data[1:5, which(names(train_data) %in% predNumVars)], caption =  "First 5 rows of numerical predictions")
```

### Variables Evaluation

Once we have obtained the predictions from each single variable model, we can proceed to evaluate the performance of each model. By using the previously mentioned evaluation metrics, we can easily observe the performance of both categorical and numerical variables.

```{r categorical_evaluation}
# evaluate categorical variables
cat_evaluation <- single_evaluation(catVars)
kable(cat_evaluation, caption = "Categorical Variables Evaluation")
# evaluate numerical variables
num_evaluation <- single_evaluation(numVars)
kable(num_evaluation, caption = "Numerical Variables Evaluation")
```

As we can see, the most of features are better than the Null model. The AUC value of each feature has little difference between the training set and the test set which means the models are fairly accurate. The best categorical AUC for training and testing data are 0.55 and 0.61 from "pred_penalties_order". The best numerical AUC is 0.76/0.80 from "pred_expected_goal_involvements_per_90" which is good. The highest Deviance is 199.19 still from "pred_expected_goal_involvements_per_90". And the following numerical features still have good performance. But the "pred_red_cards_per_90" shows lowest performance which metrics are close to Null model.

### Top-performance Variables

In the next step, we can utilize the results from the previous evaluation to identify and select the variables that demonstrate the highest performance for further model building.

```{r top_performance_variables}
cat_min_drop <- 30 # can adjust this value
select_cat_result <- cat_evaluation[-1] %>%
  filter(Reduction_Deviance > cat_min_drop) %>%
  mutate(Variable = sub("pred_", "", Pred_Variable)) %>%
  select(Variable)
num_min_drop <- 15 # can adjust this value
select_num_result <- num_evaluation[-1] %>%
  filter(Reduction_Deviance > num_min_drop) %>%
  mutate(Variable = sub("pred_", "", Pred_Variable)) %>%
  select(Variable)
```

## Visualise Top-performance Variables

For ease of understanding, we will utilize a grid arranged AUC plot and a ROC curve plot to visually illustrate the performance of the top-performing variables.

```{r top-performance_variables_auc_plot}
# AUC
p1 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_cat_result[1,]))
p2 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_num_result[1,]))
p3 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_num_result[2,]))
p4 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_num_result[3,]))
p5 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_num_result[4,]))
p6 <-
  plot_auc(test_data,
           test_data$player_type,
           as.character(select_num_result[5,]))
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

# ROC
plot_roc_sa(
  test_data,
  features = c(
    as.character(select_cat_result[1,]),
    as.character(select_num_result[1,]),
    as.character(select_num_result[2,]),
    as.character(select_num_result[3,]),
    as.character(select_num_result[4,]),
    as.character(select_num_result[5,])
  ),
  title = "ROC for Top-Performance Single Variables"
)
```

Through the AUC density charts above, we can check the relationship between predicted probabilities and the variables. The higher the predicted probabilities, the more likely the player is offensive. From the ROC, we can see "threat_per_90"(pred) and "expected_goal_involvements_per_90"(pred) are very close to the top left corner. It means these two variables have good performance in predicting the target variable.

## Feature Selection

Feature selection is commonly performed to identify the most relevant features from a larger set of available features. This process is crucial for enhancing the performance of the classification model as it reduces dimensionality, eliminates irrelevant or redundant features, and improves interpretability.

Given that our dataset comprises both categorical and numerical features, simple filter methods such as Pearson Correlation or Chi-Square Test cannot be utilized for feature selection. These methods are limited to working only with categorical or numerical variables, respectively. Instead, we will employ the following methods to select features.

### Concatenation Top-performance Feature

Based on the findings from the previous section, we have identified a set of features that exhibit strong performance in predicting the target variable. These features will be our candidate features.

In our first feature combination method, we will concatenate the top-performing categorical and numerical features to create a new feature set. Concatenation is chosen as our initial combined technique due to its simplicity and straightforwardness.

```{r concatenation_feature}
concatenation_features <-
  c(select_cat_result$Variable, select_num_result$Variable)
concatenation_features
```

### Recursive Feature Elimination

#### Literature Review

Building Machine Learning (ML) based in small training sample is a common problem in many practical applications. In line with the discussion made by Chen and Jeong (2007), with small training and high dimensionality, feature selection plays a vital role in enhancing the performance of classification models while avoiding overfitting. Feature selection is to select a subset of features from a larger set of original variables, using different methods to accomplish the subsetting task, including filter method, wrapper method and intrinsic method.

Recursive Feature Elimination (RFE) is one of the common used wrapper method for feature selection, proving to be particularly valuable when applied to such small datasets. [3] [7] As highlighted by Jeon and Oh (2020), RFE selects the optimal feature subset by employing classification accuracy and systematically eliminates the least informative features, leading to a reduction in model accuracy. All features are initially ranked based on their respective weights, with those carrying the lowest weight being progressively removed. The classifier is subsequently re-trained using the remaining features iteratively, until the entire ranking of features is obtained.

Chen and Jeong (2008) implemented an enhanced Recursive Feature Elimination (RFE) and applied a Support Vector Machine (SVM)-RFE based method for feature selection. They compared the classic RFE method with the SVM-RFE method and found that the enhanced RFE method significantly improves over the RFE method. Both methods can improve the model performance on datasets with both small and high dimensionality. Jeon and Oh (2020) implemented a Hybrid-Recursive Feature Elimination (Hybrid-RFE) feature selection method. They found that with Hybrid-RFE and cross-validation, testing the sample data on four classification models (SVM, KNN, RF, NB) showed better performance of the models. Sachdeva et al. (2022) implemented four different classification models (SVM, LR, KNN, RF) with RFE in feature selection. Their study proved that RFE can significantly increase the accuracy of model performance.

RFE is also used in building models in different domains. For example, Yan and Zhang (2015) used it in analyzing gas sensor data, Senan et al. (2021) used it in diagnosing chronic kidney disease, and Mathew (2019) used it for breast cancer diagnosis. However, as mentioned by Darst et al. (2018), RFE may not be effective in datasets that contain many highly correlated variables. It could be an effective method for nongenetic omics datasets.

Since our dataset is small and has high dimensionality, and we have already dropped the highly correlated variables during the data preprocessing stage, it is evident from the literature review that Recursive Feature Elimination (RFE) is a suitable method for feature selection in our dataset. Therefore, our second feature combination technique will be RFE.

```{r RFE}
rfe_train_data <- train_data %>%
  select(all_of(c(target, catVars, numVars))) %>%
  mutate(player_type = as.factor(player_type))

result <-
  rfe(
    rfe_train_data[, -which(names(rfe_train_data) == target)],
    rfe_train_data[, target],
    sizes = c(1:4),
    rfeControl = rfeControl(functions = rfFuncs),
    method = "repeatedcv",
    verbose = FALSE
  )
optVariables <- result$optVariables
rfe_features <- optVariables[1:10]
rfe_features
```

## Multiple Variables Models

In this section, we will utilize the two combined feature sets that we created in the previous section to build our classification models. Specifically, we have chosen to use the Decision Tree model and the XGBoost model for this project. We will implement these two models using the respective feature sets.

After building the models, we will compare their performance using the model comparison metrics and model performance metrics mentioned earlier. In the final part, we will attempt to use Local Interpretable Model-Agnostic Explanations (LIME) to provide explanations for our XGBoost model.

### Decision Tree

A Decision Tree Classification Model is a supervised machine learning algorithm used for solving classification problems. It's a tree-like structure where an internal node represents a feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome or class label. The primary goal of a Decision Tree Classification Model is to create a tree that best predicts the class labels of the input data. [3]

#### Build Model

Use the two sets of selected features to build two Decision Tree models.

```{r decision_tree_model}
# Decision Tree 1 - Concatenation
dt_train_data1 <-
  train_data %>%
  select(all_of(c(target, concatenation_features)))
dt_test_data1 <-
  test_data %>%
  select(all_of(c(target, concatenation_features)))
dt_model1 <-
  rpart(player_type ~ ., data = dt_train_data1, method = 'class')

# Decision Tree 2 - RFE
dt_train_data2 <-
  train_data %>%
  select(all_of(c(target, rfe_features)))
dt_test_data2 <-
  test_data %>%
  select(all_of(c(target, rfe_features)))
dt_model2 <-
  rpart(player_type ~ ., data = dt_train_data2, method = 'class')
```

#### Model Prediction

Obtain predictions from the two Decision Tree models that were built earlier.

```{r decision_tree_prediction}
# Decision Tree 1 - Concatenation
dt_train_pred_class1 <-
  predict(dt_model1, newdata = dt_train_data1, type = "class")
dt_test_pred_class1 <-
  predict(dt_model1, newdata = dt_test_data1, type = "class")

# Decision Tree 2 - RFE
dt_train_pred_class2 <-
  predict(dt_model2, newdata = dt_train_data2, type = "class")
dt_test_pred_class2 <-
  predict(dt_model2, newdata = dt_test_data2, type = "class")
```

#### Model Comparision

Compare the performance of the null model with the two Decision Tree models.

```{r decision_tree_comparision}
dt_comparision1 <-
  model_comparison(
    as.numeric(dt_train_pred_class1),
    ifelse(dt_train_data1[[target]] == pos.label, 1, 2),
    dt_train_data1,
    "Decision Tree (Concatenation)"
  )

dt_comparision2 <-
  model_comparison(
    as.numeric(dt_train_pred_class2),
    ifelse(dt_train_data2[[target]] == pos.label, 1, 2),
    dt_train_data2,
    "Decision Tree (RFE)"
  )

dt_all_comparision <-
  rbind(log_null_model_comparison, dt_comparision1, dt_comparision2)
kable(dt_all_comparision, caption = "Decision Tree Model Comparision")
```

From the first table, we can see that the log-likelihood and reduction in deviance for both models are significantly higher than those of the Null model, indicating that the two models are a good fit.

#### Model Performance

Evaluate two Decision Tree models using performance metrics.

```{r tree_model_performance}
dt_performance1 <-
  model_performance(
    dt_train_pred_class1,
    dt_test_pred_class1,
    dt_train_data1$player_type,
    dt_test_data1$player_type,
    "Decision Tree (Concatenation)"
  )

dt_performance2 <-
  model_performance(
    dt_train_pred_class2,
    dt_test_pred_class2,
    dt_train_data2$player_type,
    dt_test_data2$player_type,
    "Decision Tree (RFE)"
  )

dt_model_performance <- rbind(dt_performance1, dt_performance2)
kable(dt_model_performance, caption = "Decision Tree Model Performance")
```

The second table indicates that there is a small difference between the scores of the two models on the training and test datasets, which is around 0.02. This suggests that both models are performing well and are not overfitting. Additionally, all the accuracy scores are above 0.76, indicating that both models can successfully classify and predict the player type accurately. The precision scores are also fairly high, above 0.68. The recall scores are the highest among all the metrics, indicating that these tree models successfully capture most of the real positive samples. The tree models achieve a good balance between accuracy and recall, as evidenced by the high F1 scores (each \> 0.76). The AUC scores are also good enough. Overall, these two tree models are capable of accurately predicting the player type.

#### Visualisation

Visualise two Decision Tree models performances and the decision tree.

```{r tree_model_performance_plot}
dt_pred_train_roc1 <- predict(dt_model1, newdata = dt_train_data1)
dt_pred_test_roc1 <- predict(dt_model1, newdata = dt_test_data1)

dt_roc_1 <- plot_model_roc(
  dt_pred_train_roc1[, 2],
  train_data$player_type,
  dt_pred_test_roc1[, 2],
  test_data$player_type,
  title = "ROC for Decision Tree (Concatenation)"
)

dt_pred_train_roc2 <- predict(dt_model2, newdata = dt_train_data2)
dt_pred_test_roc2 <- predict(dt_model2, newdata = dt_test_data2)

tree2_roc <- plot_model_roc(
  dt_pred_train_roc2[, 2],
  train_data$player_type,
  dt_pred_test_roc2[, 2],
  test_data$player_type,
  title = "ROC for Decision Tree (RFE)"
)

# visualise the decision tree
rpart.plot(dt_model1, main = "Decision Tree (Concatenation)")
rpart.plot(dt_model2, main = "Decision Tree (RFE)")
```

For both tree models, the ROC curves may not be very well, but the AUC values are still substantial. The Concatenation model has AUC values of 0.78 for training and 0.80 for testing, while the RFE model has AUC values of 0.77 for training and 0.80 for testing. These AUC values indicate that both models have good discriminatory power. Although the ROC curves may not be very close to the left, there is little difference between the curves of the training and test data for each model, demonstrating the high accuracy of the models on the training data. From the decision tree plot, we can observe that the feature "expected_goal_involvement_per_90" is the most significant predictor at the highest level.

### XGBoost

XGBoost Classification is a supervised machine learning technique that employs a gradient boosting framework for solving classification problems. It is a tree-based ensemble method, meaning that it combines the predictions of multiple decision trees to make accurate and robust predictions. [3]

#### Build Model

Use the two sets of selected features to build two XGBoost models.

```{r xgboost_model}
# hyperparameter
xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  lambda = 1,
  alpha = 0.1,
  max_depth = 3,
  min_child_weight = 5,
  eta = 0.1
)

# XGBoost - Concatenation
xgb_train_data1 <- train_data %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_test_data1 <- test_data %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_input1 <- as.matrix(xgb_train_data1[-c(1, 2)])
xgb_test_input1 <- as.matrix(xgb_test_data1[-c(1, 2)])

xgb_model1 <- xgboost(
  data = xgb_input1,
  label = xgb_train_data1$type,
  params = xgb_params,
  nrounds = 10,
  early_stopping_rounds = 10,
  verbose = 0
)

# XGBoost - RFE
xgb_train_data2 <- train_data %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_test_data2 <- test_data %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

xgb_input2 <- as.matrix(xgb_train_data2[-c(1, 2)])
xgb_test_input2 <- as.matrix(xgb_test_data2[-c(1, 2)])

xgb_model2 <- xgboost(
  data = xgb_input2,
  label = xgb_train_data2$type,
  params = xgb_params,
  nrounds = 10,
  early_stopping_rounds = 10,
  verbose = 0
)
```

#### Model Prediction

Obtain predictions from the two XGBoost models that were built earlier.

```{r xgboost_prediction}
# XGBoost - Concatenation
xgb_train_pred1 <- predict(xgb_model1, xgb_input1)
xgb_train_pred_class1 <-
  ifelse(xgb_train_pred1 > threhold, pos.label, neg.label)
xgb_test_pred1 <- predict(xgb_model1, xgb_test_input1)
xgb_test_pred_class1 <-
  ifelse(xgb_test_pred1 > threhold, pos.label, neg.label)

# XGBoost - RFE
xgb_train_pred2 <- predict(xgb_model2, xgb_input2)
xgb_train_pred_class2 <-
  ifelse(xgb_train_pred2 > threhold, pos.label, neg.label)
xgb_test_pred2 <- predict(xgb_model2, xgb_test_input2)
xgb_test_pred_class2 <-
  ifelse(xgb_test_pred2 > threhold, pos.label, neg.label)
```

#### Model Comparision

Compare the performance of the null model with the two XGBoost models.

```{r xgboost_comparision}
xgb_comparision1 <-
  model_comparison(
    ifelse(xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(xgb_train_data1[[target]]  == pos.label, 1, 2),
    xgb_train_data1,
    "XGBoost (Concatenation)"
  )

xgb_comparision2 <-
  model_comparison(
    ifelse(xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(xgb_train_data2[[target]] == pos.label, 1, 2),
    xgb_train_data2,
    "XGBoost (RFE)"
  )

xgb_all_comparision <-
  rbind(log_null_model_comparison,
        xgb_comparision1,
        xgb_comparision2)
kable(xgb_all_comparision, caption = "XGBoost Model Comparision")
```

For 2 XGBoost models, log-likelihood and reduction deviance are much better than NUll model(-486.8746). XGBoost 2 (RFE) is slightly better than XGBoost 1 (Concatenation).

#### Model Performance

Evaluate two XGBoost models using performance metrics.

```{r xgboost_model_performance}
xgb_performance1 <-
  model_performance(
    xgb_train_pred1,
    xgb_test_pred1,
    xgb_train_data1$player_type,
    xgb_test_data1$player_type,
    "XGBoost (Concatenation)"
  )

xgb_performance2 <-
  model_performance(
    xgb_train_pred2,
    xgb_test_pred2,
    xgb_train_data2$player_type,
    xgb_test_data2$player_type,
    "XGBoost (RFE)"
  )

xgb_preformance <- rbind(xgb_performance1, xgb_performance2)
kable(xgb_preformance, caption = "XGBoost Model Performance")
```

The above table shows the performance of the two XGBoost models. Among all metrics, the disparity between their scores is minimal. Both models perform the highest recall and AUC, which are remarkably similar above 0.87 and close to 1, the best recall score is 0.93. It suggests that the 2 models have excellent performance in this binary classification, but their adaptability and performance in other measures should be further assessed. The other remaining metrics (Accuracy, Precision, F1 score), all above 0.67, mostly about 0.76, which also shows that 2 models are effective.
Then, We will use ROC to further evaluate the model.

#### Visualisation

Visualise two XGBoost models' performances.

```{r xgboost_model_performance_plot}
xgb_pred_train_roc1 <- predict(xgb_model1, newdata = xgb_input1)
xgb_pred_test_roc1 <- predict(xgb_model1, newdata = xgb_test_input1)

plot_model_roc(
  xgb_pred_train_roc1,
  train_data$player_type,
  xgb_pred_test_roc1,
  test_data$player_type,
  title = "ROC for XGBoost (Concatenation)"
)

xgb_pred_train_roc2 <- predict(xgb_model2, newdata = xgb_input2)
xgb_pred_test_roc2 <- predict(xgb_model2, newdata = xgb_test_input2)

plot_model_roc(
  xgb_pred_train_roc2,
  train_data$player_type,
  xgb_pred_test_roc2,
  test_data$player_type,
  title = "ROC for XGBoost (RFE)"
)
```

From this plot, we can observe that both ROC curves are indeed very close to the Y axis and the top left corner. This indicates that the two models have a high true positive rate and a low false positive rate. The high AUC values (both above 0.87) further confirm that the two models have excellent fit performance.

### Explaining Models Using LIME

As the GPT 3.5 model helping explain. from the result of LIME, it is evident that attributes such as "expected_goal_involvements_per_90", "goals_per_90", and "start_per_90" hold significant importance in both models. This aligns with our previous analysis. However, it is worth noting that the prominence of "expected_goal_involvements_per_90" is notably greater in the second model compared to the first. This discrepancy arises due to the fact that the second model employs RFE for feature selection, while the first model utilizes all available features. Consequently, the second model exhibits higher accuracy in predicting the player type.

```{r LIME}
cases <- c(2, 14, 26, 38)

# combined 1 - concatenation feature
sample1 <-
  as.data.frame(xgb_test_data1[cases, concatenation_features])
explainer1 <-
  lime(xgb_train_data1[, concatenation_features],
       model = xgb_model1,
       bin_continuous = FALSE)
true_label1 <- xgb_test_data1[cases, 2]
explanation1 <-
  explain(sample1,
          explainer1,
          n_labels = 1,
          n_features = 6)

plot_features(explanation1)
plot_explanations(explanation1)

# combined2 - RFE
sample2 <- as.data.frame(xgb_test_data2[cases, rfe_features])
explainer2 <-
  lime(xgb_train_data2[, rfe_features],
       model = xgb_model2,
       bin_continuous = FALSE)
true_label2 <- xgb_test_data2[cases, 2]
explanation2 <-
  explain(sample2,
          explainer2,
          n_labels = 1,
          n_features = 6)

plot_features(explanation2)
plot_explanations(explanation2)
```

From the result of LIME we can see that features like "expected_goal_involvements_per_90", "goals_per_90", "start_per_90" etc. are the most important features in both models. This is consistent with our previous analysis. The difference is that the importance of "expected_goal_involvements_per_90" is much higher in the second model than in the first model. This is because the second model uses RFE to select features, and the first model uses all features. The second model is more accurate in predicting the player type.

## Improved Models

After the initial evaluation of the models, it is evident that both the Decision Tree models and the XGBoost models perform better in binary classification of player type compared to the Null Model and single variable models. However, there is still room for improvement. In this section, we will attempt to enhance the models by focusing on the following aspects:

-   Increasing the size of the training and testing datasets
-   Implementing k-fold cross-validation
-   Tuning hyperparameters

### Improving Measures

#### Increasing Data

To increase the size of our training and testing dataset, we will incorporate the data collected for the 2023-24 season, which was collected by the same person as the data for the 2022-23 season. [2] The new dataset will consist of 1503 observations. We will maintain the same 80/20 split for the train and test data, ensuring consistency in our evaluation process.

```{r preprocess_join_data}
# read new data
fpl_raw_data1 <- read.csv("./FPL_Dataset_2022-2023.csv")
fpl_raw_data2 <- read.csv("./FPL_Dataset_2023-2024.csv")

# merge
diff_columns <- setdiff(names(fpl_raw_data2), names(fpl_raw_data1))
fpl_raw_data2 <-
  fpl_raw_data2[, -which(names(fpl_raw_data2) %in% diff_columns)]
fpl_raw_data <- union(fpl_raw_data1, fpl_raw_data2)

join_classification_data <- fpl_raw_data %>%
  preprocess_init_transform() %>%
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()

# target value plot
join_classification_data %>% preprocess_target_barplot(join_classification_data$player_type)
join_classification_data %>% preprocess_target_donut()

# remove raw data
rm(fpl_raw_data1)
rm(fpl_raw_data2)

kable(join_classification_data[1:5, ], caption = "first 5 rows of join data")
```

From the target value plot of the new dataset, we can observe that the positive and negative labels are still fairly balanced. This indicates that the dataset continues to follow a binomial distribution, making it suitable for binary classification.

```{r split_join_data}
set.seed(1414)
# split data into train and test sets
index <-
  createDataPartition(join_classification_data$player_type,
                      p = split_ratio,
                      list = FALSE)
improved_train_data <-
  join_classification_data[index,] %>% select(all_of(c(target, rfe_features)))
improved_test_data <-
  join_classification_data[-index,] %>% select(all_of(c(target, rfe_features)))
```

#### Cross-validation

Cross-validation is a statistical technique in machine learning used to evaluate the performance of a predictive model by partitioning the dataset into multiple subsets (or folds). [3]

To improve the performance of our previous models, Decision Tree Models and XGBoost Models, we will implement k-fold cross-validation. This technique will help us to better assess the models' performance by evaluating them on multiple subsets of the data.

```{r cross_validation_models}
# decision tree
cv_decision_tree <-
  function(fold_train,
           fold_validation,
           dt_metrics) {
    # fold model
    fold_tree_model <-
      rpart(
        player_type ~ .,
        data = fold_train,
        method = 'class',
        control = rpart.control(maxdepth = 5)
      )
    fold_tree_model <- prune(fold_tree_model, cp = 0.01)
    
    # evaluation each fold
    fold_predictions <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    
    fold_train_prediction <-
      predict(fold_tree_model, newdata = fold_train, type = "class")
    fold_train_auc <-
      calc_auc(as.numeric(fold_train_prediction), fold_train[, target])
    
    fold_validation_prediction <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    fold_validation_auc <-
      calc_auc(as.numeric(fold_validation_prediction), fold_validation[, target])
    
    dt_metrics <-
      c(dt_metrics, fold_train_auc, fold_validation_auc)
    
    return(dt_metrics)
  }

final_decision_tree <-
  function(improved_train_data,
           improved_test_data,
           dt_metrics) {
    mean_train_auc <-
      mean(dt_metrics[seq(1, length(dt_metrics), 2)])
    mean_validation_auc <-
      mean(dt_metrics[seq(2, length(dt_metrics), 2)])
    sd_train_auc <-
      sd(dt_metrics[seq(1, length(dt_metrics), 2)])
    sd_validation_auc <-
      sd(dt_metrics[seq(2, length(dt_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final decision tree model
    final_tree_model <-
      rpart(
        player_type ~ .,
        data = improved_train_data,
        method = "class",
        control = rpart.control(maxdepth = 5)
      )
    final_tree_model <- prune(final_tree_model, cp = 0.01)
    
    return(final_tree_model)
  }

# XGBoost

# hyperparameter
improved_xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  lambda = 0.1,
  alpha = 0.1,
  max_depth = 3,
  min_child_weight = 5,
  eta = 0.1
)

cv_xgb <-
  function(fold_train,
           fold_validation,
           xgb_params,
           xgb_metrics) {
    # build model
    xgb_fold_train <- fold_train %>%
      mutate(type = as.numeric(player_type == pos.label),
             .after = player_type)
    xgb_fold_validation <- fold_validation %>%
      mutate(type = as.numeric(player_type == pos.label),
             .after = player_type)
    fold_train_input <- as.matrix(xgb_fold_train[-c(1, 2)])
    fold_validation_input <-
      as.matrix(xgb_fold_validation[-c(1, 2)])
    
    fold_xgbmodel <- xgboost(
      data = fold_train_input,
      label = xgb_fold_train$type,
      params = improved_xgb_params,
      nrounds = 10,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    # evaluation each fold
    fold_train_pred <- predict(fold_xgbmodel, fold_train_input)
    fold_train_auc <-
      calc_auc(fold_train_pred, xgb_fold_train[, target])
    fold_validation_pred <-
      predict(fold_xgbmodel, fold_validation_input)
    fold_validation_auc <-
      calc_auc(fold_validation_pred, fold_validation[, target])
    xgb_metrics <-
      c(xgb_metrics, fold_train_auc, fold_validation_auc)
    
    return(xgb_metrics)
  }

final_xgb <-
  function(improved_train_data,
           improved_test_data,
           xgb_params,
           xgb_metrics) {
    mean_train_auc <-
      mean(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    mean_validation_auc <-
      mean(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    sd_train_auc <-
      sd(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    sd_validation_auc <-
      sd(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final XGBoost model
    final_train_data <- improved_train_data %>%
      mutate(type = as.numeric(player_type == 'Offensive'),
             .after = player_type)
    final_test_data <- improved_test_data %>%
      mutate(type = as.numeric(player_type == 'Offensive'),
             .after = player_type)
    final_train_input <- as.matrix(final_train_data[-c(1, 2)])
    final_test_input <- as.matrix(final_test_data[-c(1, 2)])
    
    final_xgbmodel <- xgboost(
      data = final_train_input,
      label = final_train_data$type,
      params = improved_xgb_params,
      nrounds = 10,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    return(final_xgbmodel)
  }
```

### Build Improved Model

Use k-fold cross-validation.

```{r k-fold cv}
# 100-fold cross-validation
k <- 100
folds <-
  createFolds(
    improved_train_data$player_type,
    k = k,
    list = TRUE,
    returnTrain = TRUE
  )
```

#### Decision Tree

Use the two sets of selected features to build two Decision Tree models by implementing the improving measures.

```{r improved_decision_tree_model}
# Decision Tree - Concatenation
improved_dt_train_data1 <-
  join_classification_data[index, ] %>%
  select(all_of(c(target, concatenation_features)))
improved_dt_test_data1 <-
  join_classification_data[-index, ] %>%
  select(all_of(c(target, concatenation_features)))

dt_metrics1 <- c()
for (i in 1:k) {
  fold_train <- improved_dt_train_data1[folds[[i]],]
  fold_validation <- improved_dt_train_data1[-folds[[i]],]
  dt_metrics1 <-
    cv_decision_tree(fold_train, fold_validation, dt_metrics1)
}
improved_dt_model1 <-
  final_decision_tree(improved_dt_train_data1,
                      improved_dt_test_data1,
                      dt_metrics1)

# Decision Tree - RFE
improved_dt_train_data2 <-
  join_classification_data[index, ] %>%
  select(all_of(c(target, rfe_features)))
improved_dt_test_data2 <-
  join_classification_data[-index, ] %>%
  select(all_of(c(target, rfe_features)))

dt_metrics2 <- c()
for (i in 1:k) {
  fold_train <- improved_dt_train_data2[folds[[i]],]
  fold_validation <- improved_dt_train_data2[-folds[[i]],]
  dt_metrics2 <-
    cv_decision_tree(fold_train, fold_validation, dt_metrics2)
}
improved_dt_model2 <-
  final_decision_tree(improved_dt_train_data2,
                      improved_dt_test_data2,
                      dt_metrics2)
```

From the result, we can observe that the mean training AUC and mean validation AUC of the two Decision Tree Models, as well as the standard deviation of the training AUC and the standard deviation of the validation AUC. The small standard deviation here suggests that the model is not overfitting.

#### XGBoost

Use the two sets of selected features to build two XGBoost models by implementing the improving measures.

```{r improved_xgboost_model}
# XGBoost - Concatenation
improved_xgb_train_data1 <-
  join_classification_data[index, ] %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_test_data1 <-
  join_classification_data[-index, ] %>%
  convert_categorial() %>%
  select(all_of(c(target, concatenation_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_input1 <- as.matrix(improved_xgb_train_data1[-c(1, 2)])
improved_xgb_test_input1 <-
  as.matrix(improved_xgb_test_data1[-c(1, 2)])

xgb_metrics1 <- c()
for (i in 1:k) {
  fold_train <- improved_xgb_train_data1[folds[[i]],]
  fold_validation <- improved_xgb_train_data1[-folds[[i]],]
  xgb_metrics1 <-
    cv_xgb(fold_train, fold_validation, xgb_params, xgb_metrics1)
}
improved_xgb_model1 <-
  final_xgb(improved_xgb_train_data1,
            improved_xgb_test_data1,
            xgb_params,
            xgb_metrics1)

# XGBoost - RFE
improved_xgb_train_data2 <-
  join_classification_data[index, ] %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_test_data2 <-
  join_classification_data[-index, ] %>%
  convert_categorial() %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(type = ifelse(player_type == pos.label, 1, 0),
         .after = player_type)

improved_xgb_input2 <- as.matrix(improved_xgb_train_data2[-c(1, 2)])
improved_xgb_test_input2 <-
  as.matrix(improved_xgb_test_data2[-c(1, 2)])

xgb_metrics2 <- c()
for (i in 1:k) {
  fold_train <- improved_xgb_train_data2[folds[[i]],]
  fold_validation <- improved_xgb_train_data2[-folds[[i]],]
  xgb_metrics2 <-
    cv_xgb(fold_train, fold_validation, xgb_params, xgb_metrics2)
}
improved_xgb_model2 <-
  final_xgb(improved_xgb_train_data2,
            improved_xgb_test_data2,
            xgb_params,
            xgb_metrics2)
```

We can analyze the XGBoost modeling in the same manner as the Decision Tree models.

### Improved Model Prediction

Obtain predictions from the improved models we just built. At the same time, we should use the initial models to make predictions on the new data as well. This will help us to compare the performance of the initial models and the improved models.

#### Decision Tree

```{r improved_decision_tree_prediction}
# Decision Tree On New Data - Concatenation
new_dt_train_pred_class1 <-
  predict(dt_model1, newdata = improved_dt_train_data1, type = "class")
new_dt_test_pred_class1 <-
  predict(dt_model1, newdata = improved_dt_test_data1, type = "class")

# Decision Tree On New Data - RFE
new_dt_train_pred_class2 <-
  predict(dt_model2, newdata = improved_dt_train_data2, type = "class")
new_dt_test_pred_class2 <-
  predict(dt_model2, newdata = improved_dt_test_data2, type = "class")

# Improved Decision Tree - Concatenation
improved_dt_train_pred_class1 <-
  predict(improved_dt_model1, newdata = improved_dt_train_data1, type = "class")
improved_dt_test_pred_class1 <-
  predict(improved_dt_model1, newdata = improved_dt_test_data1, type = "class")

# Improved Decision Tree - RFE
improved_dt_train_pred_class2 <-
  predict(improved_dt_model2, newdata = improved_dt_train_data2, type = "class")
improved_dt_test_pred_class2 <-
  predict(improved_dt_model2, newdata = improved_dt_test_data2, type = "class")
```

#### XGBoost

```{r improved_xgboost_prediction}
# XGBoost - Concatenation
new_xgb_train_pred1 <- predict(xgb_model1, improved_xgb_input1)
new_xgb_train_pred_class1 <-
  ifelse(new_xgb_train_pred1 > threhold, pos.label, neg.label)
new_xgb_test_pred1 <- predict(xgb_model1, improved_xgb_test_input1)
new_xgb_test_pred_class1 <-
  ifelse(new_xgb_test_pred1 > threhold, pos.label, neg.label)

# XGBoost - RFE
new_xgb_train_pred2 <- predict(xgb_model2, improved_xgb_input2)
new_xgb_train_pred_class2 <-
  ifelse(new_xgb_train_pred2 > threhold, pos.label, neg.label)
new_xgb_test_pred2 <- predict(xgb_model2, improved_xgb_test_input2)
new_xgb_test_pred_class2 <-
  ifelse(new_xgb_test_pred2 > threhold, pos.label, neg.label)

# Improved XGBoost - Concatenation
improved_xgb_train_pred1 <-
  predict(improved_xgb_model1, improved_xgb_input1)
improved_xgb_train_pred_class1 <-
  ifelse(improved_xgb_train_pred1 > threhold, pos.label, neg.label)
improved_xgb_test_pred1 <-
  predict(improved_xgb_model1, improved_xgb_test_input1)
improved_xgb_test_pred_class1 <-
  ifelse(improved_xgb_test_pred1 > threhold, pos.label, neg.label)

# Improved XGBoost - RFE
improved_xgb_train_pred2 <-
  predict(improved_xgb_model2, improved_xgb_input2)
improved_xgb_train_pred_class2 <-
  ifelse(improved_xgb_train_pred2 > threhold, pos.label, neg.label)
improved_xgb_test_pred2 <-
  predict(improved_xgb_model2, improved_xgb_test_input2)
improved_xgb_test_pred_class2 <-
  ifelse(improved_xgb_test_pred2 > threhold, pos.label, neg.label)
```

### Improved Model Comparision

After obtaining the predictions from the improved models, we can compare the performance of the models.

#### Decision Tree

```{r improved_decision_tree_comparision}
new_dt_comparision1 <-
  model_comparison(
    as.numeric(new_dt_train_pred_class1),
    ifelse(improved_dt_train_data1[[target]] == pos.label, 1, 2),
    improved_dt_train_data1,
    "Decision Tree (Concatenation)"
  )

new_dt_comparision2 <-
  model_comparison(
    as.numeric(new_dt_train_pred_class2),
    ifelse(improved_dt_train_data2[[target]] == pos.label, 1, 2),
    improved_dt_train_data2,
    "Decision Tree (RFE)"
  )

improved_dt_comparision1 <-
  model_comparison(
    as.numeric(improved_dt_train_pred_class1),
    ifelse(improved_dt_train_data1[[target]] == pos.label, 1, 2),
    improved_dt_train_data1,
    "Improved Decision Tree (Concatenation)"
  )

improved_dt_comparision2 <-
  model_comparison(
    as.numeric(improved_dt_train_pred_class2),
    ifelse(improved_dt_train_data2[[target]] == pos.label, 1, 2),
    improved_dt_train_data2,
    "Improved Decision Tree (RFE)"
  )

improved_dt_all_comparision <-
  rbind(
    log_null_model_comparison,
    new_dt_comparision1,
    new_dt_comparision2,
    improved_dt_comparision1,
    improved_dt_comparision2
  )
kable(improved_dt_all_comparision, caption = "Improved Decision Tree Model Comparision")
```

All of our models show significant improvement compared to the null model. Among them, the improved Decision Tree model with concatenation demonstrates the best performance. However, the improvement achieved by the enhanced Decision Tree model is not substantial, possibly due to the fact that the feature selected by RFE is not optimal for the Decision Tree model.

#### XGBoost

```{r improved_xgboost_comparision}
new_xgb_comparision1 <-
  model_comparison(
    ifelse(new_xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data1[[target]]  == pos.label, 1, 2),
    improved_xgb_train_data1,
    "XGBoost (Concatenation)"
  )

new_xgb_comparision2 <-
  model_comparison(
    ifelse(new_xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data2[[target]] == pos.label, 1, 2),
    improved_xgb_train_data2,
    "XGBoost (RFE)"
  )

improved_xgb_comparision1 <-
  model_comparison(
    ifelse(improved_xgb_train_pred_class1 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data1[[target]]  == pos.label, 1, 2),
    improved_xgb_train_data1,
    "Improved XGBoost (Concatenation)"
  )

improved_xgb_comparision2 <-
  model_comparison(
    ifelse(improved_xgb_train_pred_class2 == pos.label, 1, 2),
    ifelse(improved_xgb_train_data2[[target]] == pos.label, 1, 2),
    improved_xgb_train_data2,
    "Improved XGBoost (RFE)"
  )

improved_xgb_all_comparision <-
  rbind(
    log_null_model_comparison,
    new_xgb_comparision1,
    new_xgb_comparision2,
    improved_xgb_comparision1,
    improved_xgb_comparision2
  )
kable(improved_xgb_all_comparision, caption = "Improved XGBoost Model Comparision")
```

Now we can move to the improved XGBoost models, as the result suggests, all our improved XGBoost models perform much better than the initial ones. It may suggest that our tuning hyperparams and k-fold cross validation are effective. Among them, the improved XGBoost model with RFE demonstrates the best performance.

### Improved Models Performance

Compare the performance of the null model, initial models, and improved models.

#### Decision Tree

```{r improved_decision_tree_model_performance}
new_dt_performance1 <-
  model_performance(
    new_dt_train_pred_class1,
    new_dt_test_pred_class1,
    improved_dt_train_data1$player_type,
    improved_dt_test_data1$player_type,
    "Decision Tree (Concatenation)"
  )

new_dt_performance2 <-
  model_performance(
    new_dt_train_pred_class2,
    new_dt_test_pred_class2,
    improved_dt_train_data2$player_type,
    improved_dt_test_data2$player_type,
    "Decision Tree (RFE)"
  )

improved_dt_performance1 <-
  model_performance(
    improved_dt_train_pred_class1,
    improved_dt_test_pred_class1,
    improved_dt_train_data1$player_type,
    improved_dt_test_data1$player_type,
    "Improved Decision Tree (Concatenation)"
  )

improved_dt_performance2 <-
  model_performance(
    improved_dt_train_pred_class2,
    improved_dt_test_pred_class2,
    improved_dt_train_data2$player_type,
    improved_dt_test_data2$player_type,
    "Improved Decision Tree (RFE)"
  )

improved_dt_model_performance <-
  rbind(
    new_dt_performance1,
    new_dt_performance2,
    improved_dt_performance1,
    improved_dt_performance2
  )
kable(improved_dt_model_performance, caption = "Improved Decision Tree Model Performance")
```

As we can observe, the "Decision Tree (RFE)" and "Improved Decision Tree (RFE)" exhibit higher recall and a more favorable trade-off between precision and recall. However, the "Decision Tree (Concatenation)" and "Improved Decision Tree (Concatenation)" models demonstrate good accuracy.

#### XGBoost

```{r improved_xgboost_model_performance}
new_xgb_performance1 <-
  model_performance(
    new_xgb_train_pred1,
    new_xgb_test_pred1,
    improved_xgb_train_data1$player_type,
    improved_xgb_test_data1$player_type,
    "XGBoost (Concatenation)"
  )

new_xgb_performance2 <-
  model_performance(
    new_xgb_train_pred2,
    new_xgb_test_pred2,
    improved_xgb_train_data2$player_type,
    improved_xgb_test_data2$player_type,
    "XGBoost (RFE)"
  )

improved_xgb_performance1 <-
  model_performance(
    improved_xgb_train_pred1,
    improved_xgb_test_pred1,
    improved_xgb_train_data1$player_type,
    improved_xgb_test_data1$player_type,
    "Improved XGBoost (Concatenation)"
  )

improved_xgb_performance2 <-
  model_performance(
    improved_xgb_train_pred2,
    improved_xgb_test_pred2,
    improved_xgb_train_data2$player_type,
    improved_xgb_test_data2$player_type,
    "Improved XGBoost (RFE)"
  )

improved_xgb_preformance <-
  rbind(
    new_xgb_performance1,
    new_xgb_performance2,
    improved_xgb_performance1,
    improved_xgb_performance2
  )
kable(improved_xgb_preformance, caption = "Improved XGBoost Model Performance")
```

From the above table, it is evident that XGBoost models, regardless of whether feature concatenation or RFE is utilized, consistently exhibit superior performance compared to the Decision Tree models on the testing dataset. Notably, the "Improved XGBoost (RFE)" model showcases the utmost performance among all the models when prioritizing accuracy.

### Improved Models Visualisation

Next, we will visualize the performance of the improved models.

#### Decision Tree

```{r improved_tree_model_performance_plot}
improved_dt_pred_train_roc1 <-
  predict(improved_dt_model1, newdata = improved_dt_train_data1)
improved_dt_pred_test_roc1 <-
  predict(improved_dt_model1, newdata = improved_dt_test_data1)

improved_dt_roc_1 <- plot_model_roc(
  improved_dt_pred_train_roc1[, 2],
  improved_train_data$player_type,
  improved_dt_pred_test_roc1[, 2],
  improved_test_data$player_type,
  title = "ROC for Improved Decision Tree (Concatenation)"
)

improved_dt_pred_train_roc2 <-
  predict(improved_dt_model2, newdata = improved_dt_train_data2)
improved_dt_pred_test_roc2 <-
  predict(improved_dt_model2, newdata = improved_dt_test_data2)

improved_tree2_roc <- plot_model_roc(
  improved_dt_pred_train_roc2[, 2],
  improved_train_data$player_type,
  improved_dt_pred_test_roc2[, 2],
  improved_test_data$player_type,
  title = "ROC for Improved Decision Tree (RFE)"
)

# visualise the decision tree
rpart.plot(improved_dt_model1, main = "Improved Decision Tree (Concatenation)")
rpart.plot(improved_dt_model2, main = "Improved Decision Tree (RFE)")
```

#### XGBoost

```{r improved_xgboost_model_performance_plot}
improved_xgb_pred_train_roc1 <-
  predict(improved_xgb_model1, newdata = improved_xgb_input1)
improved_xgb_pred_test_roc1 <-
  predict(improved_xgb_model1, newdata = improved_xgb_test_input1)

plot_model_roc(
  improved_xgb_pred_train_roc1,
  improved_train_data$player_type,
  improved_xgb_pred_test_roc1,
  improved_test_data$player_type,
  title = "ROC for Improved XGBoost (Concatenation)"
)

improved_xgb_pred_train_roc2 <-
  predict(improved_xgb_model2, newdata = improved_xgb_input2)
improved_xgb_pred_test_roc2 <-
  predict(improved_xgb_model2, newdata = improved_xgb_test_input2)

plot_model_roc(
  improved_xgb_pred_train_roc2,
  improved_train_data$player_type,
  improved_xgb_pred_test_roc2,
  improved_test_data$player_type,
  title = "ROC for Improved XGBoost (RFE)"
)
```

## Comparing All Classfication Models

For this project, we have built 8 classification models, list as below:

-   Initial Decision Tree with Concatenation
-   Initial Decision Tree with RFE
-   Initial XGBoost with Concatenation
-   Initial XGBoost with RFE
-   Improved Decision Tree with Concatenation
-   Improved Decision Tree with RFE
-   Improved XGBoost with Concatenation
-   Improved XGBoost with RFE

In this section, we are trying to compare all the models we build, and evaluate their performance.

```{r classification_models_comparision}
all_comparison <- rbind(
  dt_comparision1,
  dt_comparision2,
  xgb_comparision1,
  xgb_comparision2,
  improved_dt_comparision1,
  improved_dt_comparision2,
  improved_xgb_comparision1,
  improved_xgb_comparision2
)
kable(all_comparison, caption = "All Classification Models Comparision")

all_performance <-
  rbind(
    dt_performance1,
    dt_performance2,
    xgb_performance1,
    xgb_performance2,
    improved_dt_performance1,
    improved_dt_performance2,
    improved_xgb_performance1,
    improved_xgb_performance2
  )
kable(all_performance, caption = "All Classification Models Performance")
```

For the two classification models we utilized, the XGBoost models generally outperform the Decision Tree models in terms of classification metrics such as accuracy, precision, recall, F1 score, and AUC. This suggests that the XGBoost models are more suitable for this binary classification problem. After implementing our enhancement measures, the Improved Decision Tree (Concatenation) model exhibits the highest log-likelihood and reduction in deviance, indicating a good fit for the data. However, it falls behind in terms of accuracy and precision. Conversely, this model performs poorly in predicting positive target values. The "Decision Tree (RFE)" and "XGBoost (RFE)" models excel at predicting positive values, as they effectively capture true positive instances.

# Part 3 - Cluster

## Data Preprocessing

Before conducting our clustering task, it is necessary to preprocess the data. Firstly, we need to remove irrelevant and noisy features to ensure that they do not disturb the clustering results. Secondly, we should eliminate highly correlated columns to prevent redundancy. Thirdly, it is important to normalize the data to mitigate the impact of varying scales. Lastly, we should convert categorical features into dummy variables. In this project, we employ the One-Hot Encoding technique for this purpose.

### Data Transform and cleaning

```{r clustering_transform}
clustering_transform <- function(data) {
  data %>%
    # remove irrelevant and noisy features
    select(
      -c(
        "id",
        "team",
        "name",
        "transfers_out",
        "value_form",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in"
      )
    ) %>%
    # drop highly related columns which can introduce redundancy
    select(
      -c(
        "value_season",
        "points_per_game",
        "clean_sheets_per_90",
        "expected_goals_per_90",
        "expected_assists_per_90",
        "starts_per_90",
        "expected_goals_conceded_per_90",
        "saves_per_90",
        "expected_goal_involvements_per_90",
        "expected_goals_per_90",
        "goals_conceded_per_90",
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type")) %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
}
```

### One Hot Encoding

```{r clustering_OHE}
clustering_OHE <- function(data) {
  # find all categorical cols
  colname <- names(data)
  cat_col_char <-
    str_split(colname[sapply(data[, colname], class) %in% c('factor', 'character')], " ")
  cat_col <- c()
  for (i in 1:length(cat_col_char)) {
    value = cat_col_char[[i]]
    cat_col <- c(cat_col, value)
  }
  # one hot encoding
  encoded_data <-
    dummyVars(paste("~", paste(cat_col, collapse = "+")), data = data) %>%
    predict(newdata = data)
  return_data <-
    cbind(encoded_data, data[, -which(names(data) %in% cat_col)])
  return(return_data)
}
```

## kMeans Clustering

Now, we can import our data for clustering. We choose k-Means clustering because it is a simple and efficient algorithm that is suitable for our dataset.

### Read Data

```{r clustering_read_data}
# prepare data
clustering_data <- fpl_raw_data %>%
  clustering_transform() %>%
  clustering_OHE()
# scale
scale_clustering_data <- scale(clustering_data)
```

### Picking Best Number of Clusters

We first need to determine the optimal number of clusters. We use the elbow method to find the best number of clusters. From the plot below, we can see that the elbow point is at k = 3 or k=4. In our case, we will consider the optimal number of clusters is 4.

```{r clustering_methods}
fviz_nbclust(scale_clustering_data, kmeans, method = "wss")
```

### Clustering

```{r kmeans_clustering}
# clustering
kmClusters <-
  kmeans(
    scale_clustering_data,
    centers = 3,
    iter.max = 100,
    trace = F
  )
kmClusters$size
cat("Total of cluster sizes =", sum(kmClusters$size))
groups <- kmClusters$cluster
# visualize
clusters <- kmClusters$cluster
fviz_cluster(kmClusters, geom = "point", data = scale_clustering_data)

```

From the clustering result, it is evident that the clustering results in 4 distinct groups, which are not completely isolated and exhibit some degree of overlap.

## Analyse Clustering Result

In the final step, it is recommended to visualize the clustering results. Upon visualization, it becomes evident that the clusters are not well segregated. This observation implies that the clusters lack clear separation. The uneven distribution of the data can be attributed to this phenomenon. It is apparent that the data points are not distinctly separated into individual clusters. This is primarily due to the data not being well distributed.

To enhance our clustering results, it may be beneficial to consider dimensionality reduction techniques such as Principal Component Analysis (PCA) or t-SNE. Additionally, exploring different clustering algorithms could also be advantageous.

In terms of the clustering data, improving the similarity between data points can be achieved by introducing additional domain-related data. This can help establish stronger connections and relationships among the data points, ultimately leading to a more accurate and meaningful clustering result.

```{r CH_index}
sqr_euDist <- function(x, y) {
  sum((x - y) ** 2)
}

wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply(
    clustermat,
    1,
    FUN = function(row) {
      sqr_euDist(row, c0)
    }
  ))
}


wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

tss <- function(scaled_df) {
  wss(scaled_df)
}

CH_index <- function(scaled_df, kmax, method = "kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
    stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
    # kmeans
    for (k in 2:kmax) {
      clustering <- kmeans(scaled_df, k, nstart = 10, iter.max = 100)
      wss.value[k] <- clustering$tot.withinss
    }
  } else {
    # hclust
    d <- dist(scaled_df, method = "euclidean")
    pfit <- hclust(d, method = "ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k = k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax - 1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax,
             CH_index = B / W,
             WSS = wss.value)
}
```

```{r best_number_of_clusters}
kmClustering.ch <-
  kmeansruns(scale_clustering_data,
             krange = 1:10,
             criterion = "ch")
kmClustering.ch$bestk
kmClustering.asw <-
  kmeansruns(scale_clustering_data,
             krange = 1:10,
             criterion = "asw")
kmClustering.asw$bestk
# Compare the CH values for kmeans() and hclust().
print("CH index from kmeans for k=1 to 10:")
print(kmClustering.ch$crit)
print("CH index from hclust for k=1 to 10:")
hclusting <- CH_index(scale_clustering_data, 10, method = "hclust")
print(hclusting$CH_index)

kmCritframe <- data.frame(k = 1:10,
                          ch = kmClustering.ch$crit,
                          asw = kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x = k, y = ch)) +
  geom_point() + geom_line(colour = "salmon") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "CH index") + theme(text = element_text(size = 20))
fig2 <- ggplot(kmCritframe, aes(x = k, y = asw)) +
  geom_point() + geom_line(colour = "dodgerblue") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "ASW") + theme(text = element_text(size = 20))
grid.arrange(fig1, fig2, nrow = 1)
```

```{r convex_hull}
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,
          lapply(
            unique(groups),
            FUN = function(c) {
              f <- subset(proj2Ddf, cluster == c)
              f[chull(f),]
            }
          ))
}

fig <- c()
kvalues <- seq(2, 5)
nComp <- 2
princ <- prcomp(scale_clustering_data)
project2D <-
  as.data.frame(predict(princ, newdata = scale_clustering_data)[, 1:nComp])
for (k in kvalues) {
  groups <-
    kmeans(scale_clustering_data,
           k,
           nstart = 100,
           iter.max = 100)$cluster
  kmclust.project2D <- cbind(project2D,
                             cluster = as.factor(groups),
                             position = fpl_raw_data$position)
  kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
  assign(
    paste0("fig", k),
    ggplot(kmclust.project2D, aes(x = PC1, y = PC2)) +
      geom_point(aes(shape = cluster, color = cluster)) +
      geom_polygon(
        data = kmclust.hull,
        aes(group = cluster, fill = cluster),
        alpha = 0.4,
        linetype = 0
      ) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position = "none", text = element_text(size = 20))
  )
}
grid.arrange(fig2, fig3, fig4, fig5, nrow = 2)
```

# Conclusion

In this project, we have two main tasks: classification and clustering.

For Part 1, we are faced with a binary classification problem. We begin by building two classification models (Decision Tree and XGBoost) using different feature selection techniques. After evaluating the initial models, we implement several improvement measures to enhance our classification models. Finally, we compare the performance of our models and visualize the results. It is important to remember that classification involves a trade-off between precision and recall. Therefore, we should always consider the business needs and select the model that best aligns with those needs. Also, we conducted a literature review to determine why we chose Recursive Feature Elimination (RFE) as our second feature selection method.

For Part 2, we are tasked with performing a k-means clustering with four groups. We go through the clustering process and determine the optimal number of clusters. However, upon visualizing the results, we discover that the clustering may not perform well. In the end, we discuss the clustering results and explore ways to improve them.

# References

1.  ["Fantasy Premier League Dataset 2022-2023"](https://www.kaggle.com/datasets/meraxes10/fantasy-premier-league-dataset-2022-2023). *www.kaggle.com*

2.  ["Fantasy Premier League Dataset 2023-2024"](https://www.kaggle.com/datasets/meraxes10/fantasy-premier-league-dataset-2023-2024). *www.kaggle.com*

3.  OpenAI. (2022). *ChatGPT* (GPT 3.5) [Large language model]

4.  Chen, X. W., & Jeong, J. C. (2007, December). Enhanced recursive feature elimination. In*Sixth international conference on machine learning and applications (ICMLA 2007)*(pp. 429-435). IEEE.

5.  I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, "Gene selection for cancer classification using support vector machines," Machining Learning, vol. 46, no. 1-3, pp.389-422, 2002.

6.  Jeon, H., & Oh, S. (2020). Hybrid-recursive feature elimination for efficient feature selection.*Applied Sciences*,*10*(9), 3211.

7.  R. K. Sachdeva, P. Bathla, P. Rani, V. Kukreja and R. Ahuja, "A Systematic Method for Breast Cancer Classification using RFE Feature Selection," 2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE), Greater Noida, India, 2022, pp. 1673-1676, doi: 10.1109/ICACITE53722.2022.9823464.

8.  You, W., Yang, Z., & Ji, G. (2014). PLS-based recursive feature elimination for high-dimensional small sample.*Knowledge-Based Systems*,*55*, 15-28.

9.  Yan, K., & Zhang, D. (2015). Feature selection and analysis on correlated gas sensor data with recursive feature elimination.*Sensors and Actuators B: Chemical*,*212*, 353-363.

10. Senan, E. M., Al-Adhaileh, M. H., Alsaade, F. W., Aldhyani, T. H., Alqarni, A. A., Alsharif, N., ... & Alzahrani, M. Y. (2021). Diagnosis of chronic kidney disease using effective classification algorithms and recursive feature elimination techniques.*Journal of Healthcare Engineering*,*2021*.

11. Mathew, T. E. (2019). A logistic regression with recursive feature elimination model for breast cancer diagnosis.*International Journal on Emerging Technologies*,*10*(3), 55-63.

12. Darst, B.F., Malecki, K.C. & Engelman, C.D. Using recursive feature elimination in random forest to account for correlated variables in high dimensional data.*BMC Genet***19**(Suppl 1), 65 (2018). <https://doi.org/10.1186/s12863-018-0633-8>
