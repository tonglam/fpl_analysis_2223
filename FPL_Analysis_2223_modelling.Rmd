---
title: "FPL_Analysis_2223_modelling"
author: 
- "Tong LAN (24056082)"
- "Hanyu XUE (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

## Load libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ROCit)
library(pander)
library(xgboost)
```

## Read Data

```{r read_data, message=FALSE}
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")
```

# Data Preprocessing

## Initial Transform

```{r transform, results='hide'}
model_data <- fpl_raw_data %>%
  # discard some columns, which are not useful for our analysis
  # for example, names, teams, points, news and variable related to cost
  select(
    -c(
      "id",
      "team",
      "name",
      "now_cost",
      "transfers_out",
      "value_form",
      "value_season",
      "cost_change_start",
      "news_added",
      "cost_change_start_fall",
      "ep_next",
      "event_points",
      "web_name",
      "status",
      "news",
      "chance_of_playing_next_round",
      "dreamteam_count",
      "chance_of_playing_this_round",
      "points_per_game",
      "total_points",
      "in_dreamteam",
      "form",
      "ep_this",
      "transfers_in",
      "selected_by_percent",
      "bps",
      "bonus"
    )
  ) %>%
  select(-ends_with("rank")) %>%
  select(-ends_with("rank_type"))

# str(model_data)
# summary(model_data)

# remove raw data
rm(fpl_raw_data)
```

## Handle Missing Values

```{r missing_values}
# check missing values
sum(is.na(model_data))
# check which columns have missing values
missing_values_sum <- colSums(is.na(model_data) > 0)
missing_values_sum[missing_values_sum > 0]
# convert missing values columns to categorical variables, True represent not missing and False represent missing
model_data <- model_data %>%
  mutate(
    corners_and_indirect_freekicks_order = ifelse(
      is.na(corners_and_indirect_freekicks_order),
      "Non-Taker",
      "Taker"
    ),
    penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
    direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
  )
# check missing values again
sum(is.na(model_data))
```
## Future Engineering

drop Position Column because it is highly correlated with the label column, like we just used it to create the label column.

Then, we should covert the label column to numerical values, 1 present offensive, 0 present defensive.

```{r transform_2, results='hide'}
model_data <- model_data %>%
  # drop highly correlated columns, if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
  select(
    -c(
      "clean_sheets",
      "expected_assists",
      "starts",
      "expected_goals_conceded",
      "saves",
      "expected_goal_involvements",
      "expected_goals"
    )
  ) %>%
  # transform other performance variable to per 90 values
  mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
  mutate(goals_per_90 = goals_scored / minutes * 90,
         .after = goals_scored) %>%
  mutate(goals_conceded_per_90 = goals_conceded / minutes * 90,
         .after = goals_conceded) %>% 
  mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
  mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
  mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
  mutate(creativity_per_90 = creativity / minutes * 90, .after = creativity) %>%
  mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
  mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
         .after = yellow_cards) %>%
  # drop the origin columns
  select(
    -c(
      "assists",
      "goals_scored",
      "red_cards",
      "threat",
      "influence",
      "creativity",
      "own_goals",
      "yellow_cards"
    )
  ) %>%
  # drop columns highly correlated with the some other columns
  select(-c("penalties_missed",
            "penalties_saved",
            "ict_index"))

# str(model_data)
# summary(model_data)
```

### Features that were removed

### Features that were included

```{r missing_values_2}
# count missing values
missing_values <- apply(is.na(model_data), 2, sum)
which(missing_values > 0)
# if the actual value and expected value are both 0, then the actual value of 90 and expected value of 90 will result in NaN
# if the expected 90 values are NaN, it means that the player did not play a single minute
model_data <- model_data %>%
  mutate(
    assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
    goals_per_90 = ifelse(is.na(goals_per_90), 0, goals_per_90),
    goals_conceded_per_90 = ifelse(is.na(goals_conceded_per_90), 0, goals_conceded_per_90),
    red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
    threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
    influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
    creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
    own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
    yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
  )

# look at the missing values again, they have been cleaned
missing_values <- apply(is.na(model_data), 2, sum)
missing_values
```

## Target Value

add a target column

```{r target}
model_data <- model_data %>%
  # create target column, 1 represents offensive player, 0 represents defensive player
  mutate(
    player_type = ifelse(position == 'GKP' |
                           position == 'DEF', "Defensive", "Offensive"),
    .before = position
  ) %>%
  select(-position)

knitr::kable(model_data[1:5,], caption =  "First 5 rows and first 5 conlumns of data")
```

### plot target value
```{r target_value_barplot}
model_data %>%
  ggplot(aes(x = player_type, fill = player_type)) +
  geom_bar(alpha = 0.8, width = 0.8) +
  geom_label(stat = "count",
             aes(label = ..count..),
             show.legend = F) +
  theme_ipsum() +
  scale_fill_brewer(palette = "Set1")
```

```{r target_value_donut}
model_data %>%
  mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
  group_by(player_type) %>%
  summarise(count = n()) %>%
  mutate(
    percentage = count / sum(count),
    ymax = cumsum(percentage),
    ymin = c(0, head(ymax, n = -1)),
    labelPosition = (ymax + ymin) / 2,
    label = paste(
      ifelse(player_type == 0, 'Defensive' , 'Offensive'),
      "\n",
      round(percentage * 100, 2),
      "%",
      sep = ""
    )
  ) %>%
  ggplot(aes(
    ymax = ymax,
    ymin = ymin,
    xmax = 4,
    xmin = 3,
    fill = as.factor(player_type)
  )) +
  geom_rect() +
  coord_polar(theta = "y") +
  geom_label(x = 3.5,
             aes(y = labelPosition, label = label),
             size = 4) +
  scale_fill_brewer(palette = 4) +
  scale_color_brewer(palette = 3) +
  xlim(c(2, 4)) +
  theme_void() +
  theme(legend.position = "none")
```

## Split data into 2 sets

> When working with a small dataset, it is generally recommended to prioritize maximizing the amount of data available for training the model. Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance.

> In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# do a 90/10 split to form the training and test sets.
set.seed(500)
fortrain <- runif(nrow(model_data)) < 0.9
train_data <- model_data[fortrain,]
test_data <- model_data[!fortrain,]

# prepare the data for modelling
outCol <- names(train_data)[-1]
target <- 'player_type'
pos.label <- 'Offensive'

# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type'))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numericVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
```

# Classification

## Binary Classification Problem

offensive and defensive

## Single Variable

## Null Model and Single Variable Model Evaluation

#### LogLikelihood

```{r calculate_log_likelihood}
# define compute function - likelihood
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)), na.rm = T)
}
```

- Evaluate the performance by log likelihood

```{r null_log_likelihood}
# Compute the likelihood of the Null model on the test data
logNull <-
  logLikelihood(sum(test_data[, target] == pos.label) / nrow(test_data),
                test_data[, target] == pos.label)

cat("The log likelihood of the Null model is:", logNull)
## The log likelihood of the Null model is: -534.1338
```

## Single Variable Model Build

### Categorical
- Create a predict function for categorical

```{r categorical_predictions}
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

```{r call_categorical_prediction}
# call the predict function for the candidate columns
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], test_data[, v])
}
```

- Evaluate the categorical performance by AUC

```{r calculate_AUC}
# define compute function - AUC
calcAUC <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}

# define plot function - AUC
plotAUC <- function(data, target, feature) {
 data %>%
  ggplot(aes(x = data[[feature]], color = as.factor(target))) + 
  geom_density() +
  xlab(feature)
}
```


```{r call_categorical_AUC}
catResult <- tribble(~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  aucTrain <- calcAUC(train_data[, pi], train_data[, "player_type"])
  aucTest <- calcAUC(test_data[, pi], test_data[, "player_type"])
  result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                          pi, aucTrain, aucTest)
  catResult <-
    add_row(catResult,
            feature = pi,
            pred = result,
            trainAUC = aucTrain,
            testAUC = aucTest)
}

# sort by train AUC desc, then test AUC desc
catResult <- arrange(catResult, desc(trainAUC), desc(testAUC))
catResult
```

- further explore the AUC values of the above categorical columns

```{r  density plot for AUC values (categorical)}
catAUC1 <- plotAUC(test_data, test_data$player_type, "pred_penalties_order")
catAUC2 <- plotAUC(test_data, test_data$player_type, "pred_corners_and_indirect_freekicks_order")
catAUC3 <- plotAUC(test_data, test_data$player_type, "pred_direct_freekicks_order")
grid.arrange(catAUC1, catAUC2, catAUC3, ncol = 2)
```

The result shows 3 of the features have similar score by AUC. "penalties_order" is the highest AUC score.

### Numerical
- Create a predict function for numerical

```{r numerical_predictions}
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

- Evaluate the performance by AUC

```{r call_numerical_AUC}
# create a list to store the result
numResult <- tribble( ~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in numericVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], test_data[, v])
  aucTrain <- calcAUC(train_data[, pi], train_data[, 'player_type'])
  
  if (aucTrain >= 0.4) {
    aucTest <- calcAUC(test_data[, pi], test_data[, 'player_type'])
    result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                      pi, aucTrain, aucTest)
    numResult <-
      add_row(
        numResult,
        feature = pi,
        pred = result,
        trainAUC = aucTrain,
        testAUC = aucTest
      )
  }
}

# sort by train AUC desc, then test AUC desc
numResult <- arrange(numResult, desc(trainAUC), desc(testAUC))
numResult
```

- further explore the AUC values of the above numerical columns
```{r  density plot for AUC values (numerical)}
# define a function - get the column name
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}

# call the functions to plot the density plot
Nplotlist <- list()
Ncol <- str_split(extract_column(numResult, "feature"), ", ")[[1]]
for (col in Ncol[1:6]) {
  plot <- plotAUC(test_data, test_data$player_type, col)
  Nplotlist <- c(Nplotlist, list(plot))

}

grid.arrange(grobs = Nplotlist, ncol = 2)
```
- Compare the ROC
```{r the ROC for numerical variables}
# define a function - ROC curve
plot_roc <- function(predcol,
                     outcol,
                     colour_id = 2,
                     overlaid = F) {
  ROCit_obj <- rocit(score = predcol, class = outcol == pos.label)
  par(new = overlaid)
  plot(
    ROCit_obj,
    col = c(colour_id, 1),
    legend = FALSE,
    YIndex = FALSE,
    values = FALSE
  )
}

plot_roc(test_data$pred_expected_goal_involvements_per_90,
         test_data[, target])
plot_roc(
  test_data$pred_direct_freekicks_order,
  test_data[, target],
  colour_id = 4,
  overlaid = T
)
```



### likelihood ratio test

```{r calculate_categorical_likelihood}
# store the top performing categorical variables.
select_cat_result <- tribble(~ feature,  ~ pred, ~ deviance)

minDrop <- 10  # may need to adjust this number
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    select_cat_result <-
      add_row(
        select_cat_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
  }
}

# sort by deviance desc
select_cat_result <- arrange(select_cat_result, desc(deviance))
select_cat_result
```

```{r calculate_numerical_likelihood}
# separate the expected and actual numerical columns
expectedVars <- numericVars[str_detect(numericVars, 'expected')]
actualVars <- str_replace(expectedVars, "expected_", "")
observeVars <- c(expectedVars, actualVars)

# store the top performing categorical variables.
sel_num_result <- tribble( ~ feature, ~ pred, ~ deviance)
observe_selNumResult <- tribble( ~ feature, ~ pred, ~ deviance)

minDrop <- 10  # may need to adjust this number
for (v in numericVars) {
  v
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    sel_num_result <-
      add_row(
        sel_num_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    if (v %in% observeVars) {
      observe_selNumResult <- add_row(
        observe_selNumResult,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    }
  }
}

# sort by deviance desc
sel_num_result <- arrange(sel_num_result, desc(deviance))
sel_num_result$pred
observe_selNumResult <-
  arrange(observe_selNumResult, desc(deviance))
observe_selNumResult$pred

# drop actual variable
sel_num_result <-
  sel_num_result %>% filter(!feature %in% actualVars)
numericVars <- numericVars[!numericVars %in% actualVars]
```

## Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

### Feature Selection Methods

Since our dataset contains both categorical and numerical features, we cannot use simple filter methods such as Pearson Correlation or Chi-Square Test to select features. Because they work only in categorical variables and numerical variables respectively. Instead, we will use the following methods to select features.

> A key part of building many variable models is selecting what variables to use. Each
variable we use represents a chance of explaining more of the outcome variation (a
chance of building a better model), but also represents a possible source of noise and
overfitting. To control this effect, we often preselect which subset of variables we’ll use
to fit.

> Practical Data Science With R

### Concatenation Top-performance Features

From the previous section, we have identified the following features that have good performance in predicting the target variable.

```{r feature_concatenation}
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}

combined_features <-
  paste(
    extract_column(select_cat_result, feature),
    extract_column(sel_num_result, feature),
    sep = ", "
  )

combined_features
```

#### Recursive Feature Elimination - Wrapped Methods

```{r RFE}
# train_data <- train_data %>% mutate(player_type = as.factor(player_type))
rfe_train_data <- train_data %>% 
  select(all_of(c(target, catVars, numericVars))) %>% 
  mutate(player_type = as.factor(player_type))

result <- rfe(rfe_train_data[, -which(names(rfe_train_data) == target)],
              rfe_train_data[, target],
              sizes = c(1:4),
              rfeControl = rfeControl(functions = rfFuncs),
              method = "repeatedcv",
              verbose = FALSE)
rfe_features <- result$optVariables[1:6]
rfe_features
```


## Multiple Variables

### Decision Tree
- Build decision tree for all variables
```{r decision_tree}
tree_train_data <-  train_data %>% 
  select(all_of(c(target, catVars, numericVars)))

tree_test_data <- test_data %>% 
  select(all_of(c(target, catVars, numericVars)))

tmodel <- rpart(player_type ~., data=tree_train_data, method = 'class')

predict <- as.numeric(predict(tmodel, newdata=tree_train_data, type = "class"))

print(calcAUC(predict, train_data[, target]))

predict <- as.numeric(predict(tmodel, newdata=tree_test_data, type = "class"))


pp <- prediction(predict, tree_test_data[, target])


print(calcAUC(predict, tree_test_data[, target]))

tmodel <- rpart(fV, data=train_data,method = "class")
rpart.plot(tmodel)
```


- Performance Measures
```{r AUC}
#AUC score
fV <- paste(target,'>0 ~',paste(c(catVars, numericVars), collapse=' + '),sep='')
tmodel <- rpart(fV, data=train_data)

print(calcAUC(predict(tmodel, newdata=train_data), train_data[, target]))
```

```{r confusion_matrix}
#Create a Confusion Matrix function
performanceMeasures <- function(pred, truth, name = "model") {
   ctable <- table(truth = truth, pred = (pred > 0.5))
   accuracy <- sum(diag(ctable)) / sum(ctable)
   precision <- ctable[2, 2] / sum(ctable[, 2])
   recall <- ctable[2, 2] / sum(ctable[2, ])
   f1 <- 2 * precision * recall / (precision + recall)
   data.frame(model = name, precision = precision,
              recall = recall,
              f1 = f1, accuracy = accuracy)
}

pretty_perf_table <- function(model,train_data,test_data) {
  # setting up Pander Options
  panderOptions("plain.ascii", TRUE)
  panderOptions("keep.trailing.zeros", TRUE)
  panderOptions("table.style", "simple")
  perf_justify <- "lrrrr"
  # comparing performance on training vs. test
  pred_train <- predict(model, newdata=train_data)
  truth_train <- train_data[, target]
  pred_test <- predict(model, newdata=test_data)
  truth_test <- test_data[, target]
  trainperf_tree <- performanceMeasures(
      pred_train, truth_train, "tmodel, training")
  testperf_tree <- performanceMeasures(
      pred_test, truth_test, "tmodel, test")
  perftable <- rbind(trainperf_tree, testperf_tree)
  pandoc.table(perftable, justify = perf_justify)
}

pretty_perf_table(tmodel, train_data, test_data)
```


- Build decision tree for selected variables


### Random Forest

### XGBoost
```{r xgboost function} 
#Define a function for xgboost model
fit_iris_example = function(variable_matrix, labelvec) {
  cv <- xgb.cv(
    variable_matrix,
    label = labelvec,
    params = list(objective = "binary:logistic"),
    nfold = 5,
    nrounds = 100,
    print_every_n = 10,
    metrics = "logloss"
  )
  
  evalframe <- as.data.frame(cv$evaluation_log)
  NROUNDS <- which.min(evalframe$test_logloss_mean)
  
  model <- xgboost(
    data = variable_matrix,
    label = labelvec,
    params = list(objective = "binary:logistic"),
    nrounds = NROUNDS,
    verbose = FALSE
  )
  
  model
}
```

```{r}
#Build a xgboost model
train_ <- as.matrix(numericVars)
model <- fit_iris_example(input, train_data$player_type)
```


## Models Evaluation

### Confusion Matrix
```{r}
# 设置新的阈值
threshold.value = 0.7  # 例如，将阈值设为0.7

# 根据阈值将预测的类别重新分配
predicted_classes <- ifelse(train_data$pred_expected_goal_involvements_per_90 >= threshold.value, 1, 0)

# 计算混淆矩阵
confusion_matrix <- confusionMatrix(predicted_classes, train_data$expected_goal_involvements_per_90)

# 输出混淆矩阵
print(confusion_matrix)


```


## Explaining Models using LIME

### Decision Tree Model

## XGBoost Model

## Discussion

## Conclusion

# Cluster
