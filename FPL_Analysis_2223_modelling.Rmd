---
title: "FPL_Analysis_2223_modelling"
author: 
- "Tong LAN (24056082)"
- "Hanyu XUE (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

## Load libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(RColorBrewer)
library(gridExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ROCit)
library(pander)
library(xgboost)
library(lime)
library(factoextra)
library(fpc)
library(grDevices)
```
# Data Preprocessing

## Initial Transform

```{r preprocess_init_transform}
preprocess_init_transform <- function(data) {
  data %>%
    # drop some columns, which are not useful for our analysis
    # for example, names, teams, points, news and variable related to cost
    select(
      -c(
        "id",
        "team",
        "name",
        "now_cost",
        "transfers_out",
        "value_form",
        "value_season",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "event_points",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "points_per_game",
        "total_points",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in",
        "selected_by_percent",
        "bps",
        "bonus"
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type"))
}
```

## Handle Missing Values

```{r preprocess_init_handle_NAs}
preprocess_init_handle_NAs <- function(data) {
  # check missing values
  sum(is.na(data))
  # check which columns have missing values
  missing_values_sum <- colSums(is.na(data) > 0)
  missing_values_sum[missing_values_sum > 0]
  # convert missing values columns to categorical variables, True represent not missing and False represent missing
  data <- data %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
  # check missing values again
  sum(is.na(data))
  
  return(data)
}
```

## Further Transform

drop Position Column because it is highly correlated with the label column, like we just used it to create the label column.

Then, we should covert the label column to numerical values, 1 present offensive, 0 present defensive.

```{r preprocess_further_transform, results='hide'}
preprocess_further_transform <- function(data) {
  data %>%
    # drop highly correlated columns, if a variable has per 90 values, remain per 90 values instead of the origin ones because they are more useful for further analysis
    select(
      -c(
        "clean_sheets",
        "expected_assists",
        "starts",
        "expected_goals_conceded",
        "saves",
        "expected_goal_involvements",
        "expected_goals"
      )
    ) %>%
    # transform other performance variable to per 90 values
    mutate(minutes_per_90 = minutes / 90, .after = minutes) %>%
    mutate(assists_per_90 = assists / minutes * 90, .after = assists) %>%
    mutate(goals_per_90 = goals_scored / minutes * 90,
           .after = goals_scored) %>%
    mutate(goals_conceded_per_90 = goals_conceded / minutes * 90,
           .after = goals_conceded) %>%
    mutate(red_cards_per_90 = red_cards / minutes * 90, .after = red_cards) %>%
    mutate(threat_per_90 = threat / minutes * 90, .after = threat) %>%
    mutate(influence_per_90 = influence / minutes * 90, .after = influence) %>%
    mutate(creativity_per_90 = creativity / minutes * 90,
           .after = creativity) %>%
    mutate(own_goals_per_90 = own_goals / minutes * 90, .after = own_goals) %>%
    mutate(yellow_cards_per_90 = yellow_cards / minutes * 90,
           .after = yellow_cards) %>%
    # drop the origin columns
    select(
      -c(
        "minutes",
        "assists",
        "goals_scored",
        "goals_conceded",
        "red_cards",
        "threat",
        "influence",
        "creativity",
        "own_goals",
        "yellow_cards"
      )
    )
}
```

### Features that were removed

### Features that were included

```{r preprocess_further_handle_NAs}
preprocess_further_handle_NAs <- function(data) {
  # count missing values
  missing_values <- apply(is.na(data), 2, sum)
  which(missing_values > 0)
  # if the actual value and expected value are both 0, then the actual value of 90 and expected value of 90 will result in NaN
  # if the expected 90 values are NaN, it means that the player did not play a single minute
  data <- data %>%
    mutate(
      minutes_per_90 = ifelse(is.na(minutes_per_90), 0, minutes_per_90),
      assists_per_90 = ifelse(is.na(assists_per_90), 0, assists_per_90),
      goals_per_90 = ifelse(is.na(goals_per_90), 0, goals_per_90),
      goals_conceded_per_90 = ifelse(is.na(goals_conceded_per_90), 0, goals_conceded_per_90),
      red_cards_per_90 = ifelse(is.na(red_cards_per_90), 0, red_cards_per_90),
      threat_per_90 = ifelse(is.na(threat_per_90), 0, threat_per_90),
      influence_per_90 = ifelse(is.na(influence_per_90), 0, influence_per_90),
      creativity_per_90 = ifelse(is.na(creativity_per_90), 0, creativity_per_90),
      own_goals_per_90 = ifelse(is.na(own_goals_per_90), 0, own_goals_per_90),
      yellow_cards_per_90 = ifelse(is.na(yellow_cards_per_90), 0, yellow_cards_per_90)
    )
  
  # look at the missing values again, they have been cleaned
  missing_values <- apply(is.na(data), 2, sum)
  missing_values
  
  return(data)
}
```

## Add Target Value

add a target column

```{r preprocess_target_value}
preprocess_target_value <- function(data) {
  data %>%
    # create target column, 1 represents offensive player, 0 represents defensive player
    mutate(
      player_type = ifelse(position == 'GKP' |
                             position == 'DEF', "Defensive", "Offensive"),
      .before = position
    ) %>%
    mutate(player_type_value = ifelse(player_type == "Defensive", 2, 1),
           .after = player_type) %>%
    select(-position)
}
```

### plot target value

```{r preprocess_target_barplot}
preprocess_target_barplot <- function(data, target) {
  data %>%
    ggplot(aes(x = target, fill = target)) +
    geom_bar(alpha = 0.8, width = 0.8) +
    geom_label(stat = "count",
               aes(label = ..count..),
               show.legend = F) +
    theme_ipsum() +
    scale_fill_brewer(palette = "Set1")
}
```

```{r preprocess_target_donut}
preprocess_target_donut <- function(data) {
  data %>%
    mutate(player_type = ifelse(player_type == "Defensive", 0, 1)) %>%
    group_by(player_type) %>%
    summarise(count = n()) %>%
    mutate(
      percentage = count / sum(count),
      ymax = cumsum(percentage),
      ymin = c(0, head(ymax, n = -1)),
      labelPosition = (ymax + ymin) / 2,
      label = paste(ifelse(player_type == 0, 'Defensive' , 'Offensive'),
                    "\n",
                    round(percentage * 100, 2),
                    "%",
                    sep = "")
    ) %>%
    ggplot(aes(
      ymax = ymax,
      ymin = ymin,
      xmax = 4,
      xmin = 3,
      fill = as.factor(player_type)
    )) +
    geom_rect() +
    coord_polar(theta = "y") +
    geom_label(x = 3.5,
               aes(y = labelPosition, label = label),
               size = 4) +
    scale_fill_brewer(palette = 4) +
    scale_color_brewer(palette = 3) +
    xlim(c(2, 4)) +
    theme_void() +
    theme(legend.position = "none")
}
```


## Read Data

```{r read_data, message=FALSE}
fpl_raw_data <- read.csv("./FPL_Dataset_2022-2023.csv")

classification_data <- fpl_raw_data %>% 
  preprocess_init_transform() %>% 
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()

# target value plot
classification_data %>% preprocess_target_barplot(classification_data$player_type)
classification_data %>% preprocess_target_donut()


# remove raw data
rm(fpl_raw_data)

# str(classification_data)
# summary(classification_data)

# knitr::kable()
```


## Split data into 2 sets

> When working with a small dataset, it is generally recommended to prioritize maximizing the amount of data available for training the model. Splitting the small dataset into three separate sets (train, validation, and test) may further reduce the amount of data available for training, which can negatively impact model performance.

> In such cases, it is often more appropriate to perform a simple train-test split without a separate validation set. This allows you to allocate a larger portion of the dataset for training the model, while still retaining a subset for evaluation purposes.

```{r split_data}
# do a 90/10 split to form the training and test sets.
set.seed(500)
fortrain <- runif(nrow(classification_data)) < 0.9
train_data <- classification_data[fortrain, ]
test_data <- classification_data[!fortrain, ]

# prepare the data for modelling
outCol <- names(train_data)[-c(1, 2)]
target <- 'player_type'
target_value <- 'player_type_value'
pos.label <- 'Offensive'

# divide data into numerical and categorical
vars <- setdiff(outCol, c('player_type', "player_type_value"))
catVars <-
  vars[sapply(train_data[, vars], class) %in% c('factor', 'character')]
numericVars <-
  vars[sapply(train_data[, vars], class) %in% c('numeric', 'integer')]
```

# Classification

## Binary Classification Problem

offensive and defensive

## Single Variable

## Null Model and Single Variable Model Evaluation

#### LogLikelihood

```{r calculate_log_likelihood}
# define compute function - likelihood
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)), na.rm = T)
}
```

- Evaluate the performance by log likelihood

```{r null_log_likelihood}
# Compute the likelihood of the Null model on the test data
logNull <-
  logLikelihood(sum(test_data[, target] == pos.label) / nrow(test_data),
                test_data[, target] == pos.label)

cat("The log likelihood of the Null model is:", logNull)
```

## Single Variable Model Build

### Categorical
- Create a predict function for categorical

```{r categorical_predictions}
mkPredC <- function(outCol, varCol, appCol, pos = pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,] + 1.0e-3 * pPos) / (colSums(vTab) + 1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

```{r call_categorical_prediction}
# call the predict function for the candidate columns
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredC(train_data[, 'player_type'], train_data[, v], test_data[, v])
}
```

- Evaluate the categorical performance by AUC

```{r calculate_AUC}
# define compute function - AUC
calcAUC <- function(predcol, outcol, pos = pos.label) {
  perf <- performance(prediction(predcol, outcol == pos), 'auc')
  as.numeric(perf@y.values)
}

# define plot function - AUC
plotAUC <- function(data, target, feature) {
 data %>%
  ggplot(aes(x = data[[feature]], color = as.factor(target))) + 
  geom_density() +
  xlab(feature)
}
```


```{r call_categorical_AUC}
catResult <- tribble(~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  aucTrain <- calcAUC(train_data[, pi], train_data[, "player_type"])
  aucTest <- calcAUC(test_data[, pi], test_data[, "player_type"])
  result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                          pi, aucTrain, aucTest)
  catResult <-
    add_row(catResult,
            feature = pi,
            pred = result,
            trainAUC = aucTrain,
            testAUC = aucTest)
}

# sort by train AUC desc, then test AUC desc
catResult <- arrange(catResult, desc(trainAUC), desc(testAUC))
catResult
```

- further explore the AUC values of the above categorical columns

```{r  density plot for AUC values (categorical)}
catAUC1 <- plotAUC(test_data, test_data$player_type, "pred_penalties_order")
catAUC2 <- plotAUC(test_data, test_data$player_type, "pred_corners_and_indirect_freekicks_order")
catAUC3 <- plotAUC(test_data, test_data$player_type, "pred_direct_freekicks_order")
grid.arrange(catAUC1, catAUC2, catAUC3, ncol = 2)
```

The result shows 3 of the features have similar score by AUC. "penalties_order" is the highest AUC score.

### Numerical
- Create a predict function for numerical

```{r numerical_predictions}
mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(quantile(varCol, probs = seq(0, 1, 0.1), na.rm = T))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

- Evaluate the performance by AUC

```{r call_numerical_AUC}
# create a list to store the result
numResult <- tribble( ~ feature, ~ pred, ~ trainAUC, ~ testAUC)

for (v in numericVars) {
  pi <- paste('pred_', v, sep = '')
  train_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], train_data[, v])
  test_data[, pi] <-
    mkPredN(train_data[, 'player_type'], train_data[, v], test_data[, v])
  aucTrain <- calcAUC(train_data[, pi], train_data[, 'player_type'])
  
  if (aucTrain >= 0.4) {
    aucTest <- calcAUC(test_data[, pi], test_data[, 'player_type'])
    result <- sprintf("%s: trainAUC: %4.3f; TestAUC: %4.3f",
                      pi, aucTrain, aucTest)
    numResult <-
      add_row(
        numResult,
        feature = pi,
        pred = result,
        trainAUC = aucTrain,
        testAUC = aucTest
      )
  }
}

# sort by train AUC desc, then test AUC desc
numResult <- arrange(numResult, desc(trainAUC), desc(testAUC))
numResult['pred']
```

- further explore the AUC values of the above numerical columns
```{r  density plot for AUC values (numerical)}
# define a function - get the column name
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- paste(colunm_name, collapse = ", ")
  return(result)
}

# call the functions to plot the density plot
Nplotlist <- list()
Ncol <- str_split(extract_column(numResult, "feature"), ", ")[[1]]
for (col in Ncol[1:6]) {
  plot <- plotAUC(test_data, test_data$player_type, col)
  Nplotlist <- c(Nplotlist, list(plot))

}

grid.arrange(grobs = Nplotlist, ncol = 2)
```

### likelihood ratio test

```{r calculate_categorical_likelihood}
# store the top performing categorical variables.
select_cat_result <- tribble(~ feature,  ~ pred, ~ deviance)

minDrop <- 10
for (v in catVars) {
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    select_cat_result <-
      add_row(
        select_cat_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
  }
}

# sort by deviance desc
select_cat_result <- arrange(select_cat_result, desc(deviance))
select_cat_result
```

```{r calculate_numerical_likelihood}
# separate the expected and actual numerical columns
expectedVars <- numericVars[str_detect(numericVars, 'expected')]
actualVars <- str_replace(expectedVars, "expected_", "")
observeVars <- c(expectedVars, actualVars)

# store the top performing categorical variables.
sel_num_result <- tribble( ~ feature, ~ pred, ~ deviance)
observe_selNumResult <- tribble( ~ feature, ~ pred, ~ deviance)

minDrop <- 3  
for (v in numericVars) {
  v
  pi <- paste('pred_', v, sep = '')
  devDrop <-
    2 * (logLikelihood(test_data[, pi], test_data[, target] == pos.label) - logNull)
  if (devDrop >= minDrop) {
    result <- sprintf("%6s, deviance reduction: %g", v, devDrop)
    sel_num_result <-
      add_row(
        sel_num_result,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    if (v %in% observeVars) {
      observe_selNumResult <- add_row(
        observe_selNumResult,
        feature = v,
        pred = result,
        deviance = devDrop
      )
    }
  }
}

# sort by deviance desc
sel_num_result <- arrange(sel_num_result, desc(deviance))
observe_selNumResult <-
  arrange(observe_selNumResult, desc(deviance))

# drop actual variable
sel_num_result <-
  sel_num_result %>% filter(!feature %in% actualVars)
sel_num_result$pred
numericVars <- numericVars[!numericVars %in% actualVars]
```

## Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

### Feature Selection Methods

Since our dataset contains both categorical and numerical features, we cannot use simple filter methods such as Pearson Correlation or Chi-Square Test to select features. Because they work only in categorical variables and numerical variables respectively. Instead, we will use the following methods to select features.

> A key part of building many variable models is selecting what variables to use. Each
variable we use represents a chance of explaining more of the outcome variation (a
chance of building a better model), but also represents a possible source of noise and
overfitting. To control this effect, we often preselect which subset of variables weâ€™ll use
to fit.

> Practical Data Science With R

### Concatenation Top-performance Features

From the previous section, we have identified the following features that have good performance in predicting the target variable.

```{r feature_concatenation}
extract_column <- function(tribble, column) {
  colunm_name <- pull(tribble, {
    {
      column
    }
  })
  result <- colunm_name
  if (length(result) > 0) {
    result <- paste(colunm_name, collapse = ", ")
  }
  return(result)
}

combined_features <-
  c(extract_column(select_cat_result, feature),
    strsplit(extract_column(sel_num_result, feature), ", ")[[1]])

combined_features
```

#### Recursive Feature Elimination - Wrapped Methods

```{r RFE}
rfe_train_data <- train_data %>% 
  select(all_of(c(target, catVars, numericVars))) %>% 
  mutate(player_type = as.factor(player_type))

result <- rfe(rfe_train_data[, -which(names(rfe_train_data) == target)],
              rfe_train_data[, target],
              sizes = c(1:4),
              rfeControl = rfeControl(functions = rfFuncs),
              method = "repeatedcv",
              verbose = FALSE)
rfe_features <- result$optVariables[1:12]
rfe_features
```


## Multiple Variables

## Methods for evaluating models
According to the multiple usage, we define a function so that we can do multiple performance calculations. This function includes several main methods, such as confusion matrix, AUC, likelihood. It will be used in the following sections and increase the efficiency of the code. At the same time, it is convenient to compare the results of each model.

```{r main performance_measures function}
#single performance result function
single_performance <- function(pred, truth, name = "model") {
  # data preparation
  if (str_detect(name, "xgb")) {
    pred_class <- ifelse(pred > 0.5, "Offensive", "Defensive")
    truth_class <- ifelse(truth == 1, "Offensive", "Defensive")
  }
  factor_pred <- pred
  if (class(pred) != "factor") {
    if (str_detect(name, "xgb")) {
      factor_pred <- as.factor(pred_class)
    } else{
      factor_pred <- as.factor(pred)
    }
  }
  factor_truth <- truth
  if (class(truth) != "factor") {
      factor_truth <- as.factor(truth)
  }
  #metrics calculation
  cm <- confusionMatrix(factor_pred, factor_truth)
  accuracy <- cm$overall['Accuracy']
  precision <- cm$byClass['Pos Pred Value']
  recall <- cm$byClass['Sensitivity']
  f1 <- cm$byClass['F1']
  
  if(str_detect(name, "tree")){
    pred <- as.numeric(pred)
  }
  AUC <- calcAUC(pred, truth)
  type_truth <- ifelse(truth == "Offensive", 1, 2)
  likelihood <- logLikelihood(type_truth, pred)
  #create a data frame to store the result
  data.frame(
    Model = name,
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    F1 = f1,
    AUC = AUC,
    Loglikelihood = likelihood
  )
}

#combined all the result for training data and test data
main_performance <-
  function(pred1, pred2, truth1, truth2, name1, name2) {
    pred1_performance <- single_performance(pred1, truth1, name1)
    pred2_performance <- single_performance(pred2, truth2, name2)
    result <- rbind(pred1_performance, pred2_performance)
  }

```


### Decision Tree

the performance on the test set is slightly better than on the training set,

- Build models
```{r decision_tree models}
#a tree model with the combined features by Concatenation - tmodel1
tree_train_data_1 <-  train_data %>%
  select(all_of(c(target, combined_features)))
tree_test_data_1 <- test_data %>%
  select(all_of(c(target, combined_features)))
tmodel1 <-
  rpart(player_type ~ ., data = tree_train_data_1, method = 'class')

#a tree model with the combined features by RFE - tmodel2
tree_train_data_2 <-  train_data %>%
  select(all_of(c(target, rfe_features)))
tree_test_data_2 <- test_data %>%
  select(all_of(c(target, rfe_features)))
tmodel2 <-
  rpart(player_type ~ ., data = tree_train_data_2, method = 'class')
```

- Performance Measures

```{r calculate prediction}
# tmodel1 prediction
trainPredictions1 <- predict(tmodel1, newdata=tree_train_data_1, type = "class")
# auc_tmodel1_train <- calcAUC(as.numeric(trainPredictions1), tree_train_data_1[, target])
testPredictions1 <- predict(tmodel1, newdata=tree_test_data_1, type = "class")
# auc_tmodel1_test <- calcAUC(as.numeric(testPredictions1), tree_test_data_1[, target])
# 

# tmodel2 prediction
trainPredictions2 <- predict(tmodel2, newdata=tree_train_data_2, type = "class")
# auc_tmodel2_train <- calcAUC(as.numeric(trainPredictions2), tree_train_data_2[, target])
testPredictions2 <- predict(tmodel2, newdata=tree_test_data_2, type = "class")
# auc_tmodel2_test2 <- calcAUC(as.numeric(testPredictions2), tree_test_data_2[, target])  

```

```{r}
table_train_tmodel1<- table(train_data[, target], trainPredictions1)
table_train_tmodel1
```


```{r tmodel main performance_measures function }
#calculate the performance measures for tmodel1 and tmodel2
t1 <-
  main_performance(
    trainPredictions1,
    testPredictions1,
    tree_train_data_1$player_type,
    tree_test_data_1$player_type,
    "tree_model1-Train",
    "tree_model1-Test"
  )

t2 <-
  main_performance(
    trainPredictions2,
    testPredictions2,
    tree_train_data_2$player_type,
    tree_test_data_2$player_type,
    "tree_tmodel2-Train",
    "tree_tmodel2-Test"
  )

tmodel_mainperformance <- rbind(t1, t2)
tmodel_mainperformance
```



```{r confusion_matrix}
# table_train <- table(train_data[, target], predict_train)
# table_train
```

```{r}
# define a ROC curve function
plot_roc <- function(predcol1, outcol1, predcol2, outcol2, title) {
  roc_1 <- rocit(score = predcol1, class = outcol1 == 'Defensive')
  roc_2 <- rocit(score = predcol2, class = outcol2 == 'Defensive')

  plot(
    roc_1,
    col = c( "lightblue", "forestgreen"),
    lwd = 3,
    legend = FALSE,
    YIndex = FALSE,
    values = TRUE,
    asp = 1
  )
  lines(
    roc_2$TPR ~ roc_2$FPR,
    lwd = 3,
    col = c("salmon", "forestgreen"),
    asp = 1
  )
  legend(
    "bottomright",
    col = c("lightblue", "salmon", "forestgreen"),
    c("Test Data", "Training Data", "Null Model"),
    lwd = 2
  )
  title(title)
}

pred_train_roc1 <- predict(tmodel1, newdata = tree_train_data_1)
pred_test_roc1 <- predict(tmodel1, newdata = tree_test_data_1)

plot_roc(
  pred_train_roc1[, 1],
  train_data$player_type,
  pred_test_roc1[, 1],
  test_data$player_type,
  title = "ROC for tmodel1 (features by Concatenation)"
)

```


```{r ROC curve - tmodel2}
pred_train_roc2 <- predict(tmodel2, newdata = tree_train_data_2)
pred_test_roc2 <- predict(tmodel2, newdata = tree_test_data_2)

plot_roc(
  pred_train_roc2[, 1],
  train_data$player_type,
  pred_test_roc2[, 1],
  test_data$player_type,
  title = "ROC for tmodel2 (features by RFE)"
)
```


```{r Visualising the decision tree}
rpart.plot(tmodel1)
rpart.plot(tmodel2)
```

- Conclusion


### XGBoost
-Build XGBoost models for the two sets of features
```{r xgboost_combine_1}
# convert categorical variables to factor
xgb_train_data_1 <- train_data %>%
  select(all_of(c(target, combined_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type,
    penalties_order = ifelse(penalties_order == 'Taker', 1, 0)
  )

xgb_test_data_1 <- test_data %>%
  select(all_of(c(target, combined_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type,
    penalties_order = ifelse(penalties_order == 'Taker', 1, 0)
  )

input_1 <- as.matrix(xgb_train_data_1[-c(1, 2)])

xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    lambda = 1,
    alpha = 0.1,
    max_depth = 3,
    min_child_weight = 5,
    eta = 0.1
    )

xgbmodel_1 <- xgboost(
  data = input_1,
  label = xgb_train_data_1$type,
  params = xgb_params,
  nrounds = 10,
  early_stopping_rounds = 10,
  verbose = 0
)

test_input_1 <- as.matrix(xgb_test_data_1[-c(1, 2)])


train_pred_xgb1 <- predict(xgbmodel_1, input_1)
train_pred_xgb1_class <- ifelse(train_pred_xgb1 > 0.5, "Offensive", "Defensive")

test_pred_xgb1 <- predict(xgbmodel_1, test_input_1)
test_pred_xgb1_class <- ifelse(test_pred_xgb1 > 0.5, "Offensive", "Defensive")


print(calcAUC(train_pred_xgb1, xgb_train_data_1[, target]))
print(calcAUC(test_pred_xgb1, xgb_test_data_1[, target]))
# 
# cv_results <- xgb.cv(
#   data = as.matrix(train_data[, predictors]),
#   label = train_data$target_variable,
#   params = xgb_params,
#   nrounds = 100,
#   nfold = 5,
#   stratified = TRUE,
#   verbose = 0
# )



```


```{r xgboost_combine_2}
# convert categorical variables to factor
xgb_train_data_2 <- train_data %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type)

xgb_test_data_2 <- test_data %>%
  select(all_of(c(target, rfe_features))) %>%
  mutate(
    type = as.numeric(player_type == 'Offensive'),
    .after = player_type)

input_2 <- as.matrix(xgb_train_data_2[-c(1, 2)])

xgb_params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    lambda = 1,
    alpha = 0.1,
    max_depth = 3,
    min_child_weight = 5,
    eta = 0.1
    )

xgbmodel_2 <- xgboost(
  data = input_2,
  label = xgb_train_data_2$type,
  params = xgb_params,
  nrounds = 10,
  verbose = FALSE
)

test_input_2 <- as.matrix(xgb_test_data_2[-c(1, 2)])

train_pred_xgb2 <- predict(xgbmodel_2, input_2)
test_pred_xgb2 <- predict(xgbmodel_2, test_input_2)

print(calcAUC(train_pred_xgb2, xgb_train_data_2[, target]))
print(calcAUC(test_pred_xgb2, xgb_test_data_2[, target]))



```

- Performance for the above 2 models
```{r confusion_matrix table}
#singlexgb<- single_performance(train_pred_xgb1, xgb_train_data_1$player_type,"xgb-Train")
xgb1_perf <- main_performance(train_pred_xgb1, test_pred_xgb1, xgb_train_data_1$player_type, xgb_test_data_1$player_type, "xgb1-Train","xgb1-Test")
xgb2_perf <- main_performance(train_pred_xgb2, test_pred_xgb2, xgb_train_data_2$player_type, xgb_test_data_2$player_type, "xgb2-Train","xgb2-Test")

xgb_pref <- rbind(xgb1_perf, xgb2_perf)
xgb_pref
```


```{r ROC - xgb1}
pred_train_roc_xgb1 <- predict(xgbmodel_1, newdata = input_1)
pred_test_roc_xgb1 <- predict(xgbmodel_1, newdata = test_input_1)

plot_roc(pred_train_roc_xgb1,
         train_data$player_type,
         pred_test_roc_xgb1,
         test_data$player_type,
         title = "ROC for XGBoost1 (features by Concatenation)")
```


```{r ROC - xgb2}
pred_train_roc_xgb2 <- predict(xgbmodel_2, newdata = input_2)
pred_test_roc_xgb2 <- predict(xgbmodel_2, newdata = test_input_2)

plot_roc(pred_train_roc_xgb2,
         train_data$player_type,
         pred_test_roc_xgb2,
         test_data$player_type,
         title = "ROC for XGBoost2 (features by RFE)")
```


## Models Evaluation



## Explaining Models using LIME

```{r LIME}
cases <- c(3,11,21,30)

# combined_1
sample_1 <- as.data.frame(xgb_test_data_1[cases, combined_features])
explainer_1 <- lime(xgb_train_data_1[, combined_features], model = xgbmodel_1, bin_continuous = FALSE)
true_label_1 <- xgb_test_data_1[cases, 2]
explanation_1 <- explain(sample_1, explainer_1, n_labels = 1, n_features = 6)
print(explanation_1)

plot_features(explanation_1)
plot_explanations(explanation_1)

# combined_2
sample_2 <- as.data.frame(xgb_test_data_2[cases, rfe_features])
explainer_2 <- lime(xgb_train_data_2[, rfe_features], model = xgbmodel_2, bin_continuous = FALSE)
true_label_2 <- xgb_test_data_2[cases, 2]
explanation_2 <- explain(sample_2, explainer_2, n_labels = 1, n_features = 6)
print(explanation_2)

plot_features(explanation_2)
plot_explanations(explanation_2)
```

## Performance Improve
improve the models performances by implementing such measures:
- increase the training data size
- cross validation

```{r join_data}
# read new data
fpl_raw_data_1 <- read.csv("./FPL_Dataset_2022-2023.csv")
fpl_raw_data_2 <- read.csv("./FPL_Dataset_2023-2024.csv")

# merge
diff_columns <- setdiff(names(fpl_raw_data_2), names(fpl_raw_data_1))
fpl_raw_data_2 <- fpl_raw_data_2[, -which(names(fpl_raw_data_2) %in% diff_columns)]
fpl_raw_data <- union(fpl_raw_data_1, fpl_raw_data_2)

join_classification_data <- fpl_raw_data %>% 
  preprocess_init_transform() %>% 
  preprocess_init_handle_NAs() %>%
  preprocess_further_transform() %>%
  preprocess_further_handle_NAs() %>%
  preprocess_target_value()

# target value plot
join_classification_data %>% preprocess_target_barplot(join_classification_data$player_type)
join_classification_data %>% preprocess_target_donut()

# remove raw data
rm(fpl_raw_data_1)
rm(fpl_raw_data_2)

head(join_classification_data)
```
## Cross Validation

```{r cross_validation}
# decision tree
cv_decision_tree <-
  function(fold_train,
           fold_validation,
           decision_tree_metrics) {
    # build model
    fold_tree_model <-
      rpart(player_type ~ ., data = fold_train, method = 'class')
    # evaluation each fold
    predictions <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    
    fold_train_prediction <-
      predict(fold_tree_model, newdata = fold_train, type = "class")
    fold_train_auc <-
      calcAUC(as.numeric(fold_train_prediction), fold_train[, target])
    
    fold_validation_prediction <-
      predict(fold_tree_model, newdata = fold_validation, type = "class")
    fold_validation_auc <-
      calcAUC(as.numeric(fold_validation_prediction), fold_validation[, target])
    
    decision_tree_metrics <-
      c(decision_tree_metrics, fold_train_auc, fold_validation_auc)
    
    return(decision_tree_metrics)
  }

final_decision_tree <-
  function(join_train,
           join_test,
           decision_tree_metrics) {
    mean_train_auc <-
      mean(decision_tree_metrics[seq(1, length(decision_tree_metrics), 2)])
    mean_validation_auc <-
      mean(decision_tree_metrics[seq(2, length(decision_tree_metrics), 2)])
    sd_train_auc <-
      sd(decision_tree_metrics[seq(1, length(decision_tree_metrics), 2)])
    sd_validation_auc <-
      sd(decision_tree_metrics[seq(2, length(decision_tree_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final decision tree model
    final_tree_model <- rpart(player_type ~ ., data = join_train)
    
    # evaluate the final model
    final_train_predictions <-
      predict(final_tree_model, newdata = join_train, type = "class")
    final_train_auc <-
      calcAUC(as.numeric(final_train_predictions), join_train[, target])
    final_test_predictions <-
      predict(final_tree_model, newdata = join_test, type = "class")
    final_test_auc <-
      calcAUC(as.numeric(final_test_predictions), join_test[, target])
    
    cat("Final Decision Tree Model Evaluation on Training Set:\n")
    cat("AUC:", final_train_auc, "\n")
    cat("Final Decision Tree Model Evaluation on Testing Set:\n")
    cat("AUC:", final_test_auc, "\n")
  }

# XGBoost
cv_xgb <-
  function(fold_train,
           fold_validation,
           xgb_params,
           xgb_metrics) {
    # build model
    xgb_fold_train <- fold_train %>%
      mutate(type = as.numeric(player_type == 'Offensive'), .after = player_type)
    xgb_fold_validation <- fold_validation %>%
      mutate(type = as.numeric(player_type == 'Offensive'), .after = player_type)
    fold_train_input <- as.matrix(xgb_fold_train[-c(1, 2)])
    fold_validation_input <- as.matrix(xgb_fold_validation[-c(1, 2)])
    
    fold_xgbmodel <- xgboost(
      data = fold_train_input,
      label = xgb_fold_train$type,
      params = xgb_params,
      nrounds = 100,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    # evaluation each fold
    fold_train_pred <- predict(fold_xgbmodel, fold_train_input)
    fold_train_auc <- calcAUC(fold_train_pred, xgb_fold_train[, target])
    fold_validation_pred <- predict(fold_xgbmodel, fold_validation_input)
    fold_validation_auc <-
      calcAUC(fold_validation_pred, fold_validation[, target])
     xgb_metrics <-
      c(xgb_metrics, fold_train_auc, fold_validation_auc)

    return(xgb_metrics)
  }

final_xgb <-
  function(join_train,
           join_test,
           xgb_params,
           xgb_metrics) {
    mean_train_auc <-
      mean(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    mean_validation_auc <-
      mean(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    sd_train_auc <-
      sd(xgb_metrics[seq(1, length(xgb_metrics), 2)])
    sd_validation_auc <-
      sd(xgb_metrics[seq(2, length(xgb_metrics), 2)])
    
    cat("Cross-Validation Results:\n")
    cat("Mean Train AUC:", mean_train_auc, "\n")
    cat("Mean Validation AUC:", mean_validation_auc, "\n")
    cat("Standard Deviation of Train AUC:", sd_train_auc, "\n")
    cat("Standard Deviation of Validation AUC:",
        sd_validation_auc,
        "\n")
    
    # train the final XGBoost model
    final_train <- join_train %>%
      mutate(type = as.numeric(player_type == 'Offensive'), .after = player_type)
    final_test <- join_test %>%
      mutate(type = as.numeric(player_type == 'Offensive'), .after = player_type)
    final_train_input <- as.matrix(final_train[-c(1, 2)])
    final_test_input <- as.matrix(final_test[-c(1, 2)])
    
    final_xgbmodel <- xgboost(
      data = final_train_input,
      label = final_train$type,
      params = xgb_params,
      nrounds = 100,
      early_stopping_rounds = 10,
      verbose = 0
    )
    
    # evaluate the final model
    final_train_predictions <-
      predict(final_xgbmodel, newdata = final_train_input, type = "class")
    final_train_auc <-
      calcAUC(as.numeric(final_train_predictions), join_train[, target])
    final_test_predictions <-
      predict(final_xgbmodel, newdata = final_test_input, type = "class")
    final_test_auc <-
      calcAUC(as.numeric(final_test_predictions), join_test[, target])
    
    cat("Final XGBoost Model Evaluation on Training Set:\n")
    cat("AUC:", final_train_auc, "\n")
    cat("Final XGBoost Model Evaluation on Testing Set:\n")
    cat("AUC:", final_test_auc, "\n")
  }

set.seed(1414)

# split data into train and test sets
index <-
  createDataPartition(join_classification_data$player_type, p = 0.8, list = FALSE)
join_train <- join_classification_data[index,] %>% select(all_of(c(target, rfe_features)))
join_train <- join_train[!rowSums(sapply(join_train, is.infinite)), ]
join_test <- join_classification_data[-index,] %>% select(all_of(c(target, rfe_features)))
join_test <- join_test[!rowSums(sapply(join_test, is.infinite)), ]

# 100-fold cross-validation
k <- 100
folds <-
  createFolds(
    join_train$player_type,
    k = k,
    list = TRUE,
    returnTrain = TRUE
  )

xgb_params <- list(
      objective = "binary:logistic",
      eval_metric = "auc",
      lambda = 1,
      alpha = 0.1,
      max_depth = 3,
      min_child_weight = 5,
      eta = 0.1
    )

decision_tree_metrics <- c()
xgb_metrics <- c()
for (i in 1:k) {
  fold_train <- join_train[folds[[i]], ]
  fold_validation <- join_train[-folds[[i]], ]
  # Decision Tree Model
  decision_tree_metrics <-
    cv_decision_tree(fold_train, fold_validation, decision_tree_metrics)
  # XGBoost Model
  xgb_metrics <-
    cv_xgb(fold_train, fold_validation, xgb_params, xgb_metrics)
  
}

final_decision_tree(join_train, join_test, decision_tree_metrics)
final_xgb(join_train, join_test, xgb_params, xgb_metrics)
```

## Discussion

# Cluster

## Clustering problem
clustering different types of players based on their performance in the game

## Data Preprocessing

### Data Transform and cleaning
```{r clustering_transform}
clustering_transform <- function(data) {
  data %>%
    # remove irrelevant and noisy features
    select(
      -c(
        "id",
        "team",
        "name",
        "transfers_out",
        "value_form",
        "cost_change_start",
        "news_added",
        "cost_change_start_fall",
        "ep_next",
        "web_name",
        "status",
        "news",
        "chance_of_playing_next_round",
        "dreamteam_count",
        "chance_of_playing_this_round",
        "in_dreamteam",
        "form",
        "ep_this",
        "transfers_in"
      )
    ) %>%
    # drop highly related columns which can introduce redundancy
    select(
      -c(
        "value_season",
        "points_per_game",
        "clean_sheets_per_90",
        "expected_goals_per_90",
        "expected_assists_per_90",
        "starts_per_90",
        "expected_goals_conceded_per_90",
        "saves_per_90",
        "expected_goal_involvements_per_90",
        "expected_goals_per_90",
        "goals_conceded_per_90",
      )
    ) %>%
    select(-ends_with("rank")) %>%
    select(-ends_with("rank_type")) %>%
    mutate(
      corners_and_indirect_freekicks_order = ifelse(
        is.na(corners_and_indirect_freekicks_order),
        "Non-Taker",
        "Taker"
      ),
      penalties_order = ifelse(is.na(penalties_order), "Non-Taker", "Taker"),
      direct_freekicks_order = ifelse(is.na(direct_freekicks_order), "Non-Taker", "Taker")
    )
}
```


### One Hot Encoding
```{r clustering_OHE}
clustering_OHE <- function(data) {
  # find all categorical cols
  colname <- names(data)
  cat_col_char <- str_split(colname[sapply(data[, colname], class) %in% c('factor', 'character')], " ")
  cat_col <- c("position", "direct_freekicks_order", "corners_and_indirect_freekicks_order", "penalties_order")
  # for (i in 1:length(cat_col_char)) {
  #     value = cat_col_char[[i]]
  #     print(1)
  #     print(value)
  #     cat_col <- c(cat_col, value)
  # }
  # print(cat_col)
  # one hot encoding
  encoded_data <- dummyVars(paste("~", paste(cat_col, collapse = "+")), data = data) %>%
    predict(newdata = data)
  
  return_data <- cbind(encoded_data, data[, -which(names(data) %in% cat_col)])
  
  return(return_data)
}
```


## KMeans Clustering

Applied K-Means Clustering in R
https://www.youtube.com/watch?v=NKQpVU1LTm8

```{r kmeans_clustering}
# prepare data
clustering_data <- fpl_raw_data %>%
  clustering_transform() %>%
  clustering_OHE()
# scale
scale_clustering_data <- scale(clustering_data)
# distance
clustering_data_dist <- dist(scale_clustering_data, upper = T)
# calculate the optimal number of clusters
fviz_nbclust(scale_clustering_data, kmeans, method = "wss")
# clustering
kmClusters <- kmeans(scale_clustering_data, centers = 4, iter.max=100, trace = T)
kmClusters$centers
kmClusters$size
groups <- kmClusters$cluster
# visualize
clusters <- kmClusters$cluster
fviz_cluster(list(data = scale_clustering_data, cluster = clusters))
table(groups, fpl_raw_data$position)
```
## Evaluation Clustering Result

```{r CH_index}
sqr_euDist <- function(x, y) {
  sum((x - y) ** 2)
}

wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply(
    clustermat,
    1,
    FUN = function(row) {
      sqr_euDist(row, c0)
    }
  ))
}


wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

tss <- function(scaled_df) {
  wss(scaled_df)
}

CH_index <- function(scaled_df, kmax, method = "kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
    stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
    # kmeans
    for (k in 2:kmax) {
      clustering <- kmeans(scaled_df, k, nstart = 10, iter.max = 100)
      wss.value[k] <- clustering$tot.withinss
    }
  } else {
    # hclust
    d <- dist(scaled_df, method = "euclidean")
    pfit <- hclust(d, method = "ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k = k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax - 1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax,
             CH_index = B / W,
             WSS = wss.value)
}
```


```{r clustering_evaluation}
kmClustering.ch <- kmeansruns(scale_clustering_data, krange=1:10, criterion="ch")
kmClustering.ch$bestk

kmClustering.asw <- kmeansruns(scale_clustering_data, krange=1:10, criterion="asw")
kmClustering.asw$bestk


hclusting <- CH_index(scale_clustering_data, 10, method="hclust")
print(hclusting$CH_index)
```

```{r clustering_plot}
kmCritframe <- data.frame(k = 1:10,
                          ch = kmClustering.ch$crit,
                          asw = kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x = k, y = ch)) +
  geom_point() + geom_line(colour = "red") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "CH index") + theme(text = element_text(size = 20))
fig2 <- ggplot(kmCritframe, aes(x = k, y = asw)) +
  geom_point() + geom_line(colour = "blue") +
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  labs(y = "ASW") + theme(text = element_text(size = 20))
grid.arrange(fig1, fig2, nrow = 1)
```
```{r convex_hull}
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,
          lapply(
            unique(groups),
            FUN = function(c) {
              f <- subset(proj2Ddf, cluster == c)
              f[chull(f),]
            }
          ))
}
```


```{r clustering_plot_2}
fig <- c()
kvalues <- seq(2, 5)
nComp <- 2
princ <- prcomp(scale_clustering_data)
project2D <- as.data.frame(predict(princ, newdata=scale_clustering_data)[,1:nComp])
for (k in kvalues) {
  groups <- kmeans(scale_clustering_data, k, nstart = 100, iter.max = 100)$cluster
  kmclust.project2D <- cbind(project2D,
                             cluster = as.factor(groups),
                             position = fpl_raw_data$position)
  kmclust.hull <- find_convex_hull(kmclust.project2D, groups)
  assign(
    paste0("fig", k),
    ggplot(kmclust.project2D, aes(x = PC1, y = PC2)) +
      geom_point(aes(shape = cluster, color = cluster)) +
      geom_polygon(
        data = kmclust.hull,
        aes(group = cluster, fill = cluster),
        alpha = 0.4,
        linetype = 0
      ) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position = "none", text = element_text(size = 20))
  )
}
grid.arrange(fig2, fig3, fig4, fig5, nrow=2)
```

## Discussion

# Conclusion
