---
title: "FPL_Analysis_2223_modelling"
author: 
- "Tong (24056082)"
- "Hanyu Xue (24070974)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

## Load libraries

```{r library, message=FALSE}
library(tidyverse)
library(knitr)
library(hrbrthemes)
library(gridExtra)
```

## Read Data

```{r read_data}
path <- "./FPL_Dataset_2022-2023.csv"
fpl_raw_data <- read.csv(path)
```

# Data Prepare

## Target Value

add a target column

```{r transform_1}
# get level function
get_cost_level <- function(cost_bin) {
  level <- levels(cut_interval((fpl_raw_data$now_cost) / 10, 3))
  case_when(
    cost_bin == level[1] ~ "Low",
    cost_bin == level[2] ~ "Middle",
    cost_bin == level[3] ~ "Premium",
    .default = NA
  )
}

# transform data
fpl_data <- fpl_raw_data[c(
  "web_name",
  "team",
  "position",
  "total_points",
  "points_per_game",
  "now_cost",
  "minutes",
  "goals_scored",
  "expected_goals",
  "assists",
  "expected_assists",
  "goals_conceded",
  "expected_goals_conceded",
  "clean_sheets",
  "penalties_order"
)] %>%
  rename(all_of(
    c(
      Player = "web_name",
      Club = "team",
      Position = "position",
      Points = "total_points",
      Points90 = "points_per_game",
      Cost = "now_cost",
      Minutes = "minutes",
      Goals = "goals_scored",
      xG = "expected_goals",
      Assists = "assists",
      xA = "expected_assists",
      GC = "goals_conceded",
      xGC = "expected_goals_conceded",
      CS = "clean_sheets",
      PK_Taker = "penalties_order"
    )
  )) %>%
  # cost is 10 times larger because it is easier to store an integer in the database
  # now we can return it to its actual value by dividing it by 10
  mutate(Cost = Cost / 10) %>%
  # separate the players into three groups based on their values
  mutate(Cost_bin = cut_interval(Cost, 3), .after = Cost) %>%
  # categorize player level by their values
  mutate(Level = get_cost_level(Cost_bin), .after = Cost_bin) %>% 
  # create label feature
  mutate(
    Label = case_when(
      Position == "FWD" ~ "Offensive",
      Position == "MID" ~ "Offensive",
      Position == "DEF" ~ "Deffensive",
      Position == "GKP" ~ "Deffensive"
    ), .before = Player
)

# show the table(first five rows)
kable(fpl_data[1:5, ], caption =  "First 5 rows of fpl data")
```

# Feature Selection

Feature selection is typically carried out to identify the most relevant features from a larger set of available features. This process helps improve the performance of the classification model by reducing dimensionality, eliminating irrelevant or redundant features, and enhancing interpretability.

# Classification

## Binary Classification Problem

offensive and defensive

## Split data into 2 sets
```{r split data}
#do a 90/10 split to form the training and test sets.
set.seed(500)
fortrain <- runif(nrow(fpl_data)) < 0.9
train <- fpl_data[fortrain,]
test <- fpl_data[!fortrain,]
#
```


## Single Variable
- Single variable predictions model: feature
```{r Single variable predictions}
mkPredC <- function(outCol,varCol,appCol, pos=pos.label) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab / sum(naTab))[pos]
  vTab <- table(as.factor(outCol),varCol)
  pPosWv <- (vTab[pos,]+1.0e-3 * pPos) / (colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

## Multiple Variables

### Decision Tree

### Random Forest

### XGBoost

## Module Evaluation

# Cluster
